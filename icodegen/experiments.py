# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_experiments.ipynb (unless otherwise specified).

__all__ = ['get_mean_probs', 'get_mean_cross_entropy']

# Cell
import matplotlib.pyplot as plt
plt.style.use('ggplot')

import numpy as np
import pandas as pd
import tensorflow as tf

from ds4se.mgmnt.prep.i import jsonl_list_to_dataframe, get_dfs
from .model import *
from pathlib import Path
from transformers import GPT2Tokenizer, TFGPT2LMHeadModel

# Cell
def get_mean_probs(df, model, n = None):
    """
    Get the mean probability of each token that the model
    should predict for an entire pandas dataframe.

    :param df: the pandas dataframe containing each method to have the model predict on
    :param model: the model used to generate the predictions
    :param n: the number of methods to evaluate. If none, the entire dataframe will be used
    :returns: returns a numpy array of the mean probability for each token in the model's vocab
    """
    if n is None: n = len(df)

    # setup container lists for the number of occurrences and sum of probabilities for each token
    counts = [0] * len(model.tokenizer)
    sum_probs = [0.] * len(model.tokenizer)
    # loop through each method
    for mthd in df.code.values[:n]:
        # token the method and generate the probabilities for the model's predictions
        inputs = model.tokenize(mthd)
        probs = model.get_probs(inputs)[0].numpy()
        # loop through each token and its probability and update the container lists
        for idx, p in zip(inputs['input_ids'][0], probs):
            counts[idx] += 1
            sum_probs[idx] += p[idx]

    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token
    counts = np.array(counts)
    sum_probs = np.array(sum_probs)
    return sum_probs / counts

# Cell
def get_mean_cross_entropy(df, model, n = None):
    """
    Get the mean cross entropy for a model on an entire pandas dataframe

    :param df: the pandas dataframe containing each method to have the model predict on
    :param model: the model used to generate the predictions
    :param n: the number of methods to evaluate. If none, the entire dataframe will be used
    :returns: returns the mean cross entropy of the models predictions compared to true labels
    """
    if n is None: n = len(df)

    cross_entropy_losses = []
    bce = tf.keras.losses.BinaryCrossentropy(
        reduction = tf.keras.losses.Reduction.NONE
    )
    for mthd in df.code.values[:n]:
        # token the method and generate one hot encoding for the true labels
        inputs = model.tokenize(mthd)
        labels = np.zeros((len(inputs['input_ids']), len(model.tokenizer)))
        labels[np.arange(len(inputs['input_ids'])), inputs['input_ids'].numpy()] = 1

        # get the probabilities for each token from the model
        probs = model.get_probs(inputs)[0].numpy()

        # calculate the cross entropy between the labels and probabilities
        cross_entropy = bce(labels, probs).numpy()
        cross_entropy_losses.append(cross_entropy)

    # flatten list of cross entropies and take the mean
    cross_entropy_losses = np.concatenate(cross_entropy_losses)
    return np.mean(cross_entropy_losses)