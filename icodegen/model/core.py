# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_model.core.ipynb (unless otherwise specified).

__all__ = ['Model', 'TransformerModel', 'GRUModel', 'logger', 'Model', 'TransformerModel', 'RNNModel', 'VANILLA_CONFIG',
           'GRU_CONFIG_1', 'GRU_CONFIG_2', 'GRU_CONFIG_3', 'GRU_CONFIG_4', 'GRU_CONFIG_5', 'train']

# Cell
import tensorflow as tf

from abc import ABC, abstractmethod

# Cell
class Model(ABC):
    def __init__(self, tokenizer, model):
        self.tokenizer = tokenizer
        self.model = model

    @abstractmethod
    def tokenize(self, method):
        pass

    @abstractmethod
    def get_probs(self, inputs):
        pass

# Cell

# Tensorflow Huggingface Transformer
class TransformerModel(Model):
    def tokenize(self, method):
        return self.tokenizer(method, return_tensors="tf")

    def get_probs(self, inputs):
        outputs = self.model(inputs)
        logits = outputs[0]
        probs = tf.nn.softmax(logits)

        return probs

# Tensorflow GRU Model
class GRUModel(Model):
    def get_probs(self, method):
        pass

# Cell
import json
import logging

import pandas as pd
import tensorflow as tf

from abc import ABC, abstractmethod
from ..data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer
from pathlib import Path
from tokenizers import Tokenizer

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Cell
def _loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(
        labels, logits, from_logits=True
    )

# Cell
class Model(ABC):
    def __init__(self, tokenizer, model):
        self.tokenizer = tokenizer
        self.model = model

    @abstractmethod
    def from_path(path):
        pass

    @abstractmethod
    def get_probs(self, inputs):
        pass

    @abstractmethod
    def save(self, path):
        pass

    @abstractmethod
    def tokenize(self, method):
        pass

    @abstractmethod
    def train(self, ds, epochs):
        pass

# Cell
class TransformerModel(Model):
    def from_path(path):
        pass

    def generate(self, n):
        pass

    def get_probs(self, inputs):
        outputs = self.model(inputs)
        logits = outputs[0]
        probs = tf.nn.softmax(logits)

        return probs

    def save(self, path):
        pass

    def tokenize(self, method):
        output = {}
        # encod method and then convert to format that hf models expect
        encoding = self.tokenizer.encode("<sos>" + method)
        output["input_ids"] = tf.expand_dims(
            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0
        )
        output["attention_mask"] = tf.expand_dims(
            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0
        )

        return output

    #         return self.tokenizer(method, return_tensors="tf")

    def train(self, ds, epochs):
        pass

# Cell
class RNNModel(Model):
    _RNN_TYPE = {
        "rnn": tf.keras.layers.SimpleRNN,
        "gru": tf.keras.layers.GRU,
        "lstm": tf.keras.layers.LSTM,
    }

    def __init__(
        self,
        rnn_type,
        n_layers,
        vocab_size,
        embedding_dim,
        rnn_units,
        batch_size,
        out_path,
        tokenizer,
    ):
        self.rnn_type = rnn_type
        self.n_layers = n_layers
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.rnn_units = rnn_units

        self.config_name = f"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}"
        self.out_path = Path(out_path) / self.config_name
        self.out_path.mkdir(exist_ok=True)
        tensorboard_path = self.out_path / "tensorboard_logs"
        tensorboard_path.mkdir(exist_ok=True)
        self.callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                filepath=self.out_path / "ckpt_{epoch}", save_weights_only=True
            ),
            tf.keras.callbacks.TensorBoard(
                log_dir=str(tensorboard_path),
                histogram_freq=0,  # How often to log histogram visualizations
                embeddings_freq=0,  # How often to log embedding visualizations
                update_freq="epoch",
            ),  # How often to write logs (default: once per epoch)
            tf.keras.callbacks.EarlyStopping(
                # Stop training when `val_loss` is no longer improving
                monitor="val_loss",
                # "no longer improving" being defined as "no better than 1e-2 less"
                min_delta=1e-2,
                # "no longer improving" being further defined as "for at least 5 epochs"
                patience=5,
                verbose=1,
            ),
        ]

        layer = RNNModel._RNN_TYPE[rnn_type]
        rnn_layers = [
            layer(
                rnn_units,
                return_sequences=True,
                recurrent_initializer="glorot_uniform",
                # following BigCode != Big Vocab Paper
                dropout=0.5,
            )
            for _ in range(n_layers)
        ]
        model = tf.keras.Sequential(
            [
                tf.keras.layers.Embedding(
                    input_dim=vocab_size,
                    output_dim=embedding_dim,
                    mask_zero=True,  # Zero cannot be used in the vocabulary
                ),
            ]
            + rnn_layers
            + [
                tf.keras.layers.Dense(vocab_size),
            ]
        )

        super().__init__(tokenizer, model)

    @staticmethod
    def from_path(path):
        path = Path(path)

        tokenizer = Tokenizer.from_file(str(path / "tokenizer.json"))
        with open(path / "model_config.json", "r") as f:
            model_config = json.load(f)

        model = RNNModel(
            model_config["rnn_type"],
            model_config["n_layers"],
            model_config["vocab_size"],
            model_config["embedding_dim"],
            model_config["rnn_units"],
            1,
            path,
            tokenizer,
        )
        model.model = tf.keras.models.load_model(
            str(path), custom_objects={"_loss": _loss}
        )

        return model

    def get_probs(self, inputs):
        #         ids = self.tokenizer.encode("<sos>" + method).ids
        #         input_eval = tf.expand_dims(ids, 0)

        logits = self.model(inputs["input_ids"])
        probs = tf.nn.softmax(logits)  # [0].numpy()

        return probs

    def generate(self, n, temperature=1.0):
        # Converting our start string to numbers (vectorizing)
        text_generated = [self.tokenizer.encode("<sos>").ids[0]]
        input_eval = tf.expand_dims(text_generated, 0)

        # Here batch size == 1
        self.model.reset_states()
        for i in range(n):
            predictions = self.model(input_eval)
            # remove the batch dimension
            predictions = tf.squeeze(predictions, 0)

            # using a categorical distribution to predict the character
            # returned by the model
            predictions = predictions / temperature
            predicted_id = tf.random.categorical(predictions, num_samples=1)[
                -1, 0
            ].numpy()

            text_generated.append(predicted_id)
            # Pass the predicted character as the next input to the model
            # along with the previous hidden state
            input_eval = tf.expand_dims(text_generated, 0)

        return self.tokenizer.decode(text_generated, skip_special_tokens=False)

    def save(self):
        self.tokenizer.save(str(self.out_path / "tokenizer.json"), pretty=True)
        self.model.save(str(self.out_path))
        model_config = {
            "rnn_type": self.rnn_type,
            "n_layers": self.n_layers,
            "vocab_size": self.vocab_size,
            "embedding_dim": self.embedding_dim,
            "rnn_units": self.rnn_units,
        }
        with open(self.out_path / "model_config.json", "w") as f:
            json.dump(model_config, f)

    def tokenize(self, method):
        #         ids = self.tokenizer.encode("<sos>" + method).ids
        #         inputs = tf.expand_dims(ids, 0)
        output = {}
        # encode method and then convert to format that hf models expect
        encoding = self.tokenizer.encode("<sos>" + method)
        output["input_ids"] = tf.expand_dims(
            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0
        )
        output["attention_mask"] = tf.expand_dims(
            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0
        )
        return output  # self.tokenizer(method, return_tensors="tf")

    # TODO add tensorboard call back for easy visualization
    def train(self, ds_trn, ds_val, epochs):
        self.model.compile(optimizer="adam", loss=_loss)
        history = self.model.fit(
            ds_trn, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val
        )

        return history

# Cell

# Experiment 0.0.0
VANILLA_CONFIG = {
    "rnn_type": "rnn",
    "n_layers": 1,
    "embedding_dim": 256,
    "rnn_units": 1_024,
}

# Experiment 1.0.0
GRU_CONFIG_1 = {
    "rnn_type": "gru",
    "n_layers": 1,
    "embedding_dim": 256,
    "rnn_units": 1_024,
}

# Experiment 1.1.0
GRU_CONFIG_2 = {
    "rnn_type": "gru",
    "n_layers": 2,
    "embedding_dim": 256,
    "rnn_units": 1_024,
}

# Experiment 1.1.1
GRU_CONFIG_3 = {
    "rnn_type": "gru",
    "n_layers": 3,
    "embedding_dim": 256,
    "rnn_units": 1_024,
}

# Experiment 1.2.0
GRU_CONFIG_4 = {
    "rnn_type": "gru",
    "n_layers": 1,
    "embedding_dim": 256,
    "rnn_units": 512,
}

# Experiment 1.2.1
GRU_CONFIG_5 = {
    "rnn_type": "gru",
    "n_layers": 1,
    "embedding_dim": 256,
    "rnn_units": 2_048,
}

# Cell
def train(
    data_path, out_path, epochs=64, max_length=300, batch_size=64, configs=[], n=None
):
    """Function for training models related to the library."""
    out_path.mkdir(exist_ok=True)

    # Load in the datasets

    # Train BPE tokenizer
    # Check if the path where the tokenizer is to be saved is not empty
    # if it is not empty then just load the tokenizer there.
    if (out_path / "tokenizer.json").exists():
        logging.info(f"Loading tokenizer from {str(out_path / 'tokenizer.json')}.")
        tokenizer = Tokenizer.from_file(str(out_path / "tokenizer.json"))
    else:
        logging.info(
            f"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}."
        )
        df_bpe = pd.read_json(
            data_path / "codesearchnet_java" / "bpe.jsonl", orient="records", lines=True
        )[:n]
        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)
        tokenizer.save(str(out_path / "tokenizer.json"), pretty=True)
        del df_bpe

    # Tokenize the dataset and convert it to tfds.
    tfds_trn_path = (
        data_path / "codesearchnet_java" / f"tfds_trn_{max_length}len_{batch_size}bs"
    )
    if tfds_trn_path.exists():
        ds_trn = tf.data.experimental.load(
            str(tfds_trn_path),
            (
                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),
                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),
            ),
        )
    else:
        df_trn = pd.read_json(
            data_path / "codesearchnet_java" / "train.jsonl",
            orient="records",
            lines=True,
        )[:n]
        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)
        tfds_trn_path.mkdir(exist_ok=True)
        tf.data.experimental.save(ds_trn, str(tfds_trn_path))
        del df_trn

    tfds_val_path = (
        data_path / "codesearchnet_java" / f"tfds_val_{max_length}len_{batch_size}bs"
    )
    if tfds_val_path.exists():
        ds_val = tf.data.experimental.load(
            str(tfds_val_path),
            (
                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),
                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),
            ),
        )
    else:
        df_val = pd.read_json(
            data_path / "codesearchnet_java" / "valid.jsonl",
            orient="records",
            lines=True,
        )[:n]
        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)
        tfds_val_path.mkdir(exist_ok=True)
        tf.data.experimental.save(ds_val, str(tfds_val_path))
        del df_val

    logging.info("Starting the training of all RNN based models.")
    # Train RNN based models
    for config in configs:
        rnn_model = RNNModel(
            config["rnn_type"],
            config["n_layers"],
            tokenizer.get_vocab_size(),
            config["embedding_dim"],
            config["rnn_units"],
            batch_size,
            str(out_path),
            tokenizer,
        )
        rnn_model.train(ds_trn, ds_val, epochs)
        rnn_model.save()

    logging.info("Starting the training of all Transformer based models.")
    # Train Transformer models
    pass