{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> This model contains all the necessary functionality for managing data. @Nathan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import icodegen\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from subprocess import CalledProcessError, check_output\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भारत test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             code\n",
       "0  this is a test\n",
       "1       भारत test"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\"this is a test\", \"भारत test\"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _isASCII(mthd: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given method contains only ASCII characters. From https://stackoverflow.com/a/27084708/5768407.\n",
    "\n",
    "    :param mthd: the method to verify contains only ASCII characters\n",
    "    :returns: returns a boolean representing whether or not the given method contains only ASCII characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mthd.encode(encoding=\"utf-8\").decode(\"ascii\")\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def remove_non_ascii(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all methods that contain non-ascii characters from a given pandas dataframe, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a new dataframe without methods that contain non-ascii characters\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df = df[df.code.apply(_isASCII)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ASCII_DF = pd.DataFrame([\"this is a test\"], columns=[\"code\"])\n",
    "df_non_ascii = remove_non_ascii(df_fake)\n",
    "\n",
    "assert (NON_ASCII_DF == df_non_ascii).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public void setPipelines(java.util.Collection&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code\n",
       "0  public void setPipelines(java.util.Collection<..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(\n",
    "    [\n",
    "        \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "        if (pipelines == null) {\n",
    "            this.pipelines = null;\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(pipelines);\n",
    "    }\n",
    "    \"\"\"\n",
    "    ],\n",
    "    columns=[\"code\"],\n",
    ")\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _beautify(mthd: str) -> str:\n",
    "    \"\"\"\n",
    "    Beautifies a given method using uncrustify with the sun.cfg style, i.e., Oracle's style.\n",
    "\n",
    "    :param mthd: the method to beautify\n",
    "    :returns: returns a beautified version of the given method\n",
    "    \"\"\"\n",
    "    # get path of icodegen\n",
    "    icodegen_path = Path(icodegen.__path__[0])\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp.java\", \"w\") as f:\n",
    "        f.write(mthd)\n",
    "\n",
    "    try:\n",
    "        beaut_mthd = check_output(\n",
    "            [\n",
    "                icodegen_path / \"uncrustify\",\n",
    "                \"-c\",\n",
    "                icodegen_path / \"sun.cfg\",\n",
    "                \"-f\",\n",
    "                tmp_path / \"tmp.java\",\n",
    "            ]\n",
    "        ).decode(\"utf-8\")\n",
    "    except CalledProcessError as e:\n",
    "        # Exception thrown when the method is malformed, i.e, it is missing a curly brace\n",
    "        beaut_mthd = e.output.decode(\"utf-8\")\n",
    "\n",
    "    return beaut_mthd\n",
    "\n",
    "\n",
    "def beautify_code(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beautify the methods in a pandas dataframe using uncrustify with the sun.cfg style, i.e., Oracle's style, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the methods beautified\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(_beautify)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUT_MTHD = \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "    if (pipelines == null) {\n",
    "\tthis.pipelines = null;\n",
    "\treturn;\n",
    "    }\n",
    "    this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(\n",
    "\tpipelines);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "df_beaut = beautify_code(df_fake)\n",
    "\n",
    "assert BEAUT_MTHD == df_beaut.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\"<n>\": \"\\n\"}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<assert>\": \"assert\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<break>\": \"break\",\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<class>\": \"class\",\n",
    "    \"<const>\": \"const\",\n",
    "    \"<continue>\": \"continue\",\n",
    "    \"<default>\": \"default\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<else>\": \"else\",\n",
    "    \"<enum>\": \"enum\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<goto>\": \"goto\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<native>\": \"native\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<return>\": \"return\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<strictfp>\": \"strictfp\",\n",
    "    \"<super>\": \"super\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<while>\": \"while\",\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\",\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens,\n",
    "    **java_operator_tokens,\n",
    "    **java_structural_tokens,\n",
    "    **java_extra_tokens,\n",
    "    **extra_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt;&gt;&gt; &gt; + public ++ \\n\\n \\t \\t \\t\\t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  code\n",
       "0  >>> > + public ++ \\n\\n \\t \\t \\t\\t  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\">>> > + public ++ \\n\\n \\t \\t \\t\\t  \"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _replace_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for replacing all special tokens in a given method. This will replace longer special tokens first in order to not mistakenly breakup a special token that is part of a longer sequence. Adapted from https://stackoverflow.com/a/6117124/5768407 and https://stackoverflow.com/a/11753945/5768407\n",
    "\n",
    "    :param mthd: the method to have its special tokens replaced\n",
    "    :param spec_toks: a dictionary containing the special tokens to replace and the new tokens to replace them with\n",
    "    :returns: returns the method with its special tokens replaced\n",
    "    \"\"\"\n",
    "    # construct escaped versions of keys for running through regex\n",
    "    spec_toks = dict(\n",
    "        (re.escape(v), k)\n",
    "        for k, v in sorted(\n",
    "            java_special_tokens.items(), key=lambda x: len(x[1]), reverse=True\n",
    "        )\n",
    "    )\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "\n",
    "    return mthd\n",
    "\n",
    "\n",
    "def replace_special_tokens(\n",
    "    df: pd.DataFrame, spec_toks: Dict[str, str], n: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all the special tokens in a pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to replace special tokens in\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the special tokens replaced\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _replace_toks(mthd, spec_toks))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACED_MTHD = \"<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  \"\n",
    "df_replaced = replace_special_tokens(df_fake, java_special_tokens)\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fake_data = \"<triple_greater> <greater> <+> <public> <++> <n><n>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ce58281abb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvocab_sz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10_000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmin_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m ) -> Tokenizer:\n\u001b[1;32m     11\u001b[0m     \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "def train_tokenizer(\n",
    "    df: pd.DataFrame,\n",
    "    spec_toks: Dict[str, str],\n",
    "    max_length: int,\n",
    "    n: Optional[int] = None,\n",
    "    vocab_sz: Optional[int] = 10_000,\n",
    "    min_freq: Optional[int] = 2,\n",
    "    output: Optional[Path] = None,\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Train a ByteLevel BPE tokenizer on a given pandas dataframe. Code adapted from https://github.com/huggingface/tokenizers/tree/master/bindings/python.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the tokenizer train on\n",
    "    :param spec_toks: dict of special tokens to add to the tokenizers so they do not get split\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :param vocab_sz: the maximum vocabulary size of the trained tokenizer. Defaulted was selected from: Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\n",
    "    :param min_freq: the minimum frequency a token has to occur to be considered\n",
    "    :returns: returns a trained ByteLevel BPE tokenizer\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp_tokenize.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(df.code.values[:n]))\n",
    "\n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "    # train tokenizer with data in tmp file\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_sz,\n",
    "        min_frequency=min_freq,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\"] + list(spec_toks.keys()),\n",
    "    )\n",
    "    tokenizer.train(trainer, [str(tmp_path / \"tmp_tokenize.txt\")])\n",
    "    tokenizer.enable_padding(length=max_length)\n",
    "    tokenizer.enable_truncation(max_length)\n",
    "\n",
    "    # save tokenizer if output path given\n",
    "    if output is not None:\n",
    "        tokenizer.save(output, pretty=True)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_SPEC = [\n",
    "    \"<triple_greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<+>\",\n",
    "    \"Ġ\",\n",
    "    \"<public>\",\n",
    "    \"Ġ\",\n",
    "    \"<++>\",\n",
    "    \"Ġ\",\n",
    "    \"<n>\",\n",
    "    \"<n>\",\n",
    "]\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens)\n",
    "encoded = tokenizer.encode(fake_data)\n",
    "\n",
    "assert TOKENIZED_SPEC == encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _split_input_target(mthd):\n",
    "    input_text = mthd[:-1]\n",
    "    target_text = mthd[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def convert_df_to_tfds(\n",
    "    df: pd.DataFrame, tokenizer: Tokenizer, max_length: int, batch_size: int\n",
    "):\n",
    "    # TODO: optimize using tfds map function with tokenizer.encode_batch thingy\n",
    "    tokenized_mthds = [\n",
    "        [tokenizer.encode(\"<sos>\").ids[0]] + tokenizer.encode(mthd).ids\n",
    "        for mthd in df.code.values\n",
    "    ]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tokenized_mthds)\n",
    "    ds = ds.map(_split_input_target).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add tests for above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
