{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> This model contains all the necessary functionality for managing data. @Nathan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import gdown\n",
    "import icodegen\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import CalledProcessError, check_output\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from typing import Dict, Optional\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "URLs = {\n",
    "    \"bigclonebenchmark_lg\": \"https://drive.google.com/uc?id=1-4LPiiKGR5Zmg-TLqZEkRbRIdg7UlJQb\",\n",
    "    \"bigclonebenchmark_sm\": \"https://drive.google.com/uc?id=1FCq0lSs4oqc3jpSoucsHlRqjmbVwdRQ9\",\n",
    "    \"bug_fix_pairs\": \"https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\",\n",
    "    \"codesearchnet_java\": \"https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _download_data(out_path):\n",
    "    \"\"\"\n",
    "    Function for downloading all the data to reproduce our study.\n",
    "    \"\"\"\n",
    "    # Download bigclonebenchmark_lg and bigclonebenchmark_sm\n",
    "    logging.info(\"Downloading BigCloneBenchmark datasets.\")\n",
    "    bigclone_path = out_path / \"bigclonebenchmark\"\n",
    "    bigclone_path.mkdir(parents=True, exist_ok=True)\n",
    "    gdown.download(\n",
    "        URLs[\"bigclonebenchmark_lg\"], str(bigclone_path / \"bigclonebenchmark_lg.csv\")\n",
    "    )\n",
    "    gdown.download(\n",
    "        URLs[\"bigclonebenchmark_sm\"], str(bigclone_path / \"bigclonebenchmark_sm.csv\")\n",
    "    )\n",
    "\n",
    "    # Download Bug Fix Pairs\n",
    "    logging.info(\"Downloading and extracting Bug Fix Pairs dataset.\")\n",
    "    bugfix_path = out_path / \"bug_fix\"\n",
    "    bugfix_path.mkdir(parents=True, exist_ok=True)\n",
    "    gdown.cached_download(\n",
    "        URLs[\"bug_fix_pairs\"],\n",
    "        str(bugfix_path / \"bug_fix_pairs.zip\"),\n",
    "        postprocess=gdown.extractall,\n",
    "    )\n",
    "    with zipfile.ZipFile(\n",
    "        str(bugfix_path / \"datasets\" / \"50-100\" / \"source_code.zip\"), \"r\"\n",
    "    ) as zip_ref:\n",
    "        zip_ref.extractall(bugfix_path)\n",
    "\n",
    "    # from https://stackoverflow.com/a/14260592/5768407 by users\n",
    "    # yoavram (https://stackoverflow.com/users/1063612/yoavram) and\n",
    "    # kamran kausar (https://stackoverflow.com/users/3486460/kamran-kausar)\n",
    "    logging.info(\"Downloading and extracting CodeSearchNet Challenge dataset.\")\n",
    "    codesearchnet_path = out_path / \"codesearchnet\"\n",
    "    codesearchnet_path.mkdir(parents=True, exist_ok=True)\n",
    "    r = requests.get(URLs[\"codesearchnet_java\"])\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(codesearchnet_path / \"codesearchnet_java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading BigCloneBenchmark datasets.\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-4LPiiKGR5Zmg-TLqZEkRbRIdg7UlJQb\n",
      "To: /tmp/data/bigclonebenchmark/bigclonebenchmark_lg.csv\n",
      "231MB [00:10, 21.2MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FCq0lSs4oqc3jpSoucsHlRqjmbVwdRQ9\n",
      "To: /tmp/data/bigclonebenchmark/bigclonebenchmark_sm.csv\n",
      "100%|██████████| 921k/921k [00:00<00:00, 10.6MB/s]\n",
      "INFO:root:Downloading and extracting Bug Fix Pairs dataset.\n",
      "Cached Downloading: /tmp/data/bug_fix/bug_fix_pairs.zip\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\n",
      "To: /home/jovyan/.cache/gdown/tmp6flebmtf/dl\n",
      "113MB [00:05, 20.1MB/s] \n",
      "INFO:filelock:Lock 140684953912704 acquired on /home/jovyan/.cache/gdown/_dl_lock\n",
      "INFO:filelock:Lock 140684953912704 released on /home/jovyan/.cache/gdown/_dl_lock\n",
      "INFO:root:Downloading and extracting CodeSearchNet Challenge dataset.\n"
     ]
    }
   ],
   "source": [
    "# Commenting out to prevent CI/CD from wasting bandwith downloading\n",
    "# data_path = Path(\"/tmp/data\")\n",
    "# _download_data(data_path)\n",
    "\n",
    "# bigclone_path = data_path / \"bigclonebenchmark\"\n",
    "# assert Path(bigclone_path / \"bigclonebenchmark_lg.csv\").exists()\n",
    "# assert Path(bigclone_path / \"bigclonebenchmark_sm.csv\").exists()\n",
    "\n",
    "# bugfix_path = data_path / \"bug_fix\"\n",
    "# assert Path(bugfix_path / \"bug_fix_pairs.zip\").exists()\n",
    "# assert Path(bugfix_path / \"50-100/buggy\").exists()\n",
    "# assert Path(bugfix_path / \"50-100/fixed\").exists()\n",
    "\n",
    "# codesearchnet_path = data_path / \"codesearchnet\"\n",
    "# assert Path(codesearchnet_path / \"codesearchnet_java\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भारत test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             code\n",
       "0  this is a test\n",
       "1       भारत test"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\"this is a test\", \"भारत test\"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _isASCII(mthd: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given method contains only ASCII characters. From https://stackoverflow.com/a/27084708/5768407.\n",
    "\n",
    "    :param mthd: the method to verify contains only ASCII characters\n",
    "    :returns: returns a boolean representing whether or not the given method contains only ASCII characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mthd.encode(encoding=\"utf-8\").decode(\"ascii\")\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def remove_non_ascii(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all methods that contain non-ascii characters from a given pandas dataframe, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a new dataframe without methods that contain non-ascii characters\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df = df[df.code.apply(_isASCII)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "NON_ASCII_DF = pd.DataFrame([\"this is a test\"], columns=[\"code\"])\n",
    "df_non_ascii = remove_non_ascii(df_fake)\n",
    "\n",
    "assert (NON_ASCII_DF == df_non_ascii).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public void setPipelines(java.util.Collection&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code\n",
       "0  public void setPipelines(java.util.Collection<..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(\n",
    "    [\n",
    "        \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "        if (pipelines == null) {\n",
    "            this.pipelines = null;\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(pipelines);\n",
    "    }\n",
    "    \"\"\"\n",
    "    ],\n",
    "    columns=[\"code\"],\n",
    ")\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _beautify(mthd: str) -> str:\n",
    "    \"\"\"\n",
    "    Beautifies a given method using uncrustify with the sun.cfg style, i.e., Oracle's style.\n",
    "\n",
    "    :param mthd: the method to beautify\n",
    "    :returns: returns a beautified version of the given method\n",
    "    \"\"\"\n",
    "    # get path of icodegen\n",
    "    icodegen_path = Path(icodegen.__path__[0])\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp.java\", \"w\") as f:\n",
    "        f.write(mthd)\n",
    "\n",
    "    try:\n",
    "        beaut_mthd = check_output(\n",
    "            [\n",
    "                icodegen_path / \"uncrustify\",\n",
    "                \"-c\",\n",
    "                icodegen_path / \"sun.cfg\",\n",
    "                \"-f\",\n",
    "                tmp_path / \"tmp.java\",\n",
    "            ]\n",
    "        ).decode(\"utf-8\")\n",
    "    except CalledProcessError as e:\n",
    "        # Exception thrown when the method is malformed, i.e, it is missing a curly brace\n",
    "        beaut_mthd = e.output.decode(\"utf-8\")\n",
    "\n",
    "    return beaut_mthd\n",
    "\n",
    "\n",
    "def beautify_code(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beautify the methods in a pandas dataframe using uncrustify with the sun.cfg style, i.e., Oracle's style, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the methods beautified\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(_beautify)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUT_MTHD = \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "    if (pipelines == null) {\n",
    "\tthis.pipelines = null;\n",
    "\treturn;\n",
    "    }\n",
    "    this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(\n",
    "\tpipelines);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "df_beaut = beautify_code(df_fake)\n",
    "\n",
    "assert BEAUT_MTHD == df_beaut.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\"<n>\": \"\\n\"}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<assert>\": \"assert\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<break>\": \"break\",\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<class>\": \"class\",\n",
    "    \"<const>\": \"const\",\n",
    "    \"<continue>\": \"continue\",\n",
    "    \"<default>\": \"default\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<else>\": \"else\",\n",
    "    \"<enum>\": \"enum\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<goto>\": \"goto\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<native>\": \"native\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<return>\": \"return\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<strictfp>\": \"strictfp\",\n",
    "    \"<super>\": \"super\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<while>\": \"while\",\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\",\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens,\n",
    "    **java_operator_tokens,\n",
    "    **java_structural_tokens,\n",
    "    **java_extra_tokens,\n",
    "    **extra_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt;&gt;&gt; &gt; + public ++ \\n\\n \\t \\t \\t\\t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  code\n",
       "0  >>> > + public ++ \\n\\n \\t \\t \\t\\t  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\">>> > + public ++ \\n\\n \\t \\t \\t\\t  \"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _replace_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for replacing all special tokens in a given method. This will replace longer special tokens first in order to not mistakenly breakup a special token that is part of a longer sequence. Adapted from https://stackoverflow.com/a/6117124/5768407 and https://stackoverflow.com/a/11753945/5768407\n",
    "\n",
    "    :param mthd: the method to have its special tokens replaced\n",
    "    :param spec_toks: a dictionary containing the special tokens to replace and the new tokens to replace them with\n",
    "    :returns: returns the method with its special tokens replaced\n",
    "    \"\"\"\n",
    "    # construct escaped versions of keys for running through regex\n",
    "    spec_toks = dict(\n",
    "        (re.escape(v), k)\n",
    "        for k, v in sorted(\n",
    "            java_special_tokens.items(), key=lambda x: len(x[1]), reverse=True\n",
    "        )\n",
    "    )\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "\n",
    "    return mthd\n",
    "\n",
    "\n",
    "def replace_special_tokens(\n",
    "    df: pd.DataFrame, spec_toks: Dict[str, str], n: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all the special tokens in a pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to replace special tokens in\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the special tokens replaced\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _replace_toks(mthd, spec_toks))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACED_MTHD = \"<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  \"\n",
    "df_replaced = replace_special_tokens(df_fake, java_special_tokens)\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fake_data = \"<triple_greater> <greater> <+> <public> <++> <n><n>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_tokenizer(\n",
    "    df: pd.DataFrame,\n",
    "    spec_toks: Dict[str, str],\n",
    "    max_length: int,\n",
    "    n: Optional[int] = None,\n",
    "    vocab_sz: Optional[int] = 10_000,\n",
    "    min_freq: Optional[int] = 2,\n",
    "    output: Optional[Path] = None,\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Train a ByteLevel BPE tokenizer on a given pandas dataframe. Code adapted from https://github.com/huggingface/tokenizers/tree/master/bindings/python.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the tokenizer train on\n",
    "    :param spec_toks: dict of special tokens to add to the tokenizers so they do not get split\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :param vocab_sz: the maximum vocabulary size of the trained tokenizer. Defaulted was selected from: Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\n",
    "    :param min_freq: the minimum frequency a token has to occur to be considered\n",
    "    :returns: returns a trained ByteLevel BPE tokenizer\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp_tokenize.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(df.code.values[:n]))\n",
    "\n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "    # train tokenizer with data in tmp file\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_sz,\n",
    "        min_frequency=min_freq,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\"] + list(spec_toks.keys()),\n",
    "    )\n",
    "    tokenizer.train(trainer, [str(tmp_path / \"tmp_tokenize.txt\")])\n",
    "    tokenizer.enable_padding(length=max_length, pad_token=\"<pad>\")\n",
    "    tokenizer.enable_truncation(max_length)\n",
    "\n",
    "    # save tokenizer if output path given\n",
    "    if output is not None:\n",
    "        tokenizer.save(output, pretty=True)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_SPEC = [\n",
    "    \"<triple_greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<+>\",\n",
    "    \"Ġ\",\n",
    "    \"<public>\",\n",
    "    \"Ġ\",\n",
    "    \"<++>\",\n",
    "    \"Ġ\",\n",
    "    \"<n>\",\n",
    "    \"<n>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "]\n",
    "max_length = 16\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens, max_length)\n",
    "encoded = tokenizer.encode(fake_data)\n",
    "\n",
    "assert TOKENIZED_SPEC == encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _split_input_target(mthd):\n",
    "    input_text = mthd[:-1]\n",
    "    target_text = mthd[1:]\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def convert_df_to_tfds(\n",
    "    df: pd.DataFrame, tokenizer: Tokenizer, max_length: int, batch_size: int\n",
    "):\n",
    "    tokenized_mthds = []\n",
    "    for i in range(0, len(df.code.values), batch_size):\n",
    "        batch = df.code.values[i : i + batch_size]\n",
    "        batch = [f\"<sos>{x}\" for x in batch]\n",
    "        for x in tokenizer.encode_batch(batch):\n",
    "            tokenized_mthds.append(x.ids)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tokenized_mthds)\n",
    "    ds = ds.map(_split_input_target).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataset = convert_df_to_tfds(df_fake, tokenizer, max_length, batch_size)\n",
    "\n",
    "for _, shape in dataset:\n",
    "    assert (batch_size, max_length - 1) == shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _process_bug_fix(path):\n",
    "    buggy_paths = sorted((path / \"50-100\").glob(\"buggy/*.java\"))\n",
    "    fixed_paths = sorted((path / \"50-100\").glob(\"fixed/*.java\"))\n",
    "    bugs = []\n",
    "    fixes = []\n",
    "    for bug_p, fix_p in zip(buggy_paths, fixed_paths):\n",
    "        with open(bug_p, \"r\") as f:\n",
    "            bugs.append(f.read())\n",
    "\n",
    "        with open(fix_p, \"r\") as f:\n",
    "            fixes.append(f.read())\n",
    "\n",
    "    df_buggy = pd.DataFrame(bugs, columns=[\"code\"])\n",
    "    df_buggy = remove_non_ascii(df_buggy)\n",
    "    df_buggy = replace_special_tokens(df_buggy, java_special_tokens)\n",
    "\n",
    "    df_fixed = pd.DataFrame(fixes, columns=[\"code\"])\n",
    "    df_fixed = remove_non_ascii(df_fixed)\n",
    "    df_fixed = replace_special_tokens(df_fixed, java_special_tokens)\n",
    "\n",
    "    # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "    df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "    df_fixed.to_json(path / \"fixed.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def _jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def _process_codesearchnet(path):\n",
    "    \"\"\"\n",
    "    Grabs the different data splits and converts them into dataframes.\n",
    "    Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path / \"java\" / \"final\" / \"jsonl\" / split).glob(\"**/*.gz\"))\n",
    "        df = _jsonl_list_to_dataframe(files, [\"code\"])\n",
    "        df = remove_non_ascii(df)\n",
    "        df = replace_special_tokens(df, java_special_tokens)\n",
    "        # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "        if split == \"train\":\n",
    "            # 10% selected to match the Big Code != Big Vocab paper.\n",
    "            df_trn, df_bpe = train_test_split(df, test_size=0.1)\n",
    "            df_trn.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)\n",
    "            df_bpe.to_json(path / \"bpe.jsonl\", orient=\"records\", lines=True)\n",
    "        else:\n",
    "            df.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def process_data(path):\n",
    "    \"\"\"Function for processing data related to the library.\"\"\"\n",
    "    # Process Bug Fix Pairs data\n",
    "    bugfix_path = path / \"bug_fix\"\n",
    "    _process_bug_fix(bugfix_path)\n",
    "\n",
    "    # Process CodeSearchNet Challenge data\n",
    "    codesearchnet_path = path / \"codesearchnet\"\n",
    "    _process_codesearchnet(codesearchnet_path / \"codesearchnet_java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out since depends on downloaded data\n",
    "# process_data(data_path)\n",
    "\n",
    "# assert Path(bugfix_path / \"buggy.jsonl\").exists()\n",
    "# assert Path(bugfix_path / \"fixed.jsonl\").exists()\n",
    "\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"train.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"bpe.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"valid.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"test.jsonl\"\n",
    "# ).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUGGY_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# FIXED_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.info(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# df = pd.read_json(bugfix_path / \"bug_fix_pairs.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# assert BUGGY_MTHD == df.buggy.values[0] and FIXED_MTHD == df.fixed.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
