{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.9.3\n"
     ]
    }
   ],
   "source": [
    "#! pip install 'tokenizers==0.9.3'\n",
    "#! pip install 'transformers==3.5.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariate Analysis and Feature Engineering \n",
    ">\n",
    ">@danaderp 11.17.20 .\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools \n",
    "from operator import or_\n",
    "from collections import Counter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#import sentencepiece as spm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import bootstrap_plot\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024\n",
    "#df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "dvc_path = Path('../dvc-icodegen/')\n",
    "def params():\n",
    "    return {\n",
    "        'tokenizer':dvc_path / 'models/bpe/tokenizer-java-v1.json',\n",
    "        'tb_01':dvc_path / 'nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/bug_fix_error_taxonomy.jsonl',\n",
    "        'tb_02':dvc_path / 'nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/bug_fix_cross_entropy.jsonl',\n",
    "        'output': dvc_path / 'nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../dvc-icodegen/nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/xxx.jsonl')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['output'] / 'xxx.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 20:36:02,673 : INFO : ../dvc-icodegen/models/bpe/tokenizer-java-v1.json\n"
     ]
    }
   ],
   "source": [
    "#tst\n",
    "logging.info( params['tokenizer'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_tax = pd.read_json(\n",
    "            params['tb_01'], orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 20:36:53,865 : INFO : Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2021-06-24 20:36:53,866 : INFO : Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-06-24 20:36:53,866 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "df_cross = pd.read_json(\n",
    "        params['tb_02'], orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006220</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.008038</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368741</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.337473</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927699</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  y_cross_entropy  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006220   \n",
       "1  <private> <void> success<(>io.netty.channel.Ch...         6.008038   \n",
       "2  <private> <void> handleConnectRequest<(>com.as...         6.368741   \n",
       "3  <private> <void> handleConnectRequest<(>com.as...         6.337473   \n",
       "4  <@>java.lang.Override<n><protected> <void> onS...         3.927699   \n",
       "\n",
       "   x_treatment  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3         True  \n",
       "4        False  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Covariate Analysis: Token Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload tokenizers here\n",
    "#Use Tokenizer recent version\n",
    "#Load tokenizer\n",
    "#Update files (for one time) the main json files to add the column of tokenizers\n",
    "#Count the tokens per method\n",
    "#Visualize the most popular tokens (and less popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeHF():\n",
    "    def __init__( self ):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet (plain text)\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        #Load tokenizer\n",
    "        self.tokenizer = Tokenizer.from_file( str( params['tokenizer'] ) )\n",
    "        pass\n",
    "    \n",
    "    #@staticmethod\n",
    "    def encodingHF(self, method ):\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "             tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "             tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "    #@staticmethod\n",
    "    def decodingHF( self, idss ):\n",
    "        return self.tokenizer.decode( idss , skip_special_tokens=False )\n",
    "        #return self.tokenizer.convert_ids_to_tokens(idss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = df_cross['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty.mysql.OK<(><)><;><n>    ok.sequenceId <=> 2<;><n>    ok.setStatusFlag<(>Flags.SERVER_STATUS_AUTOCOMMIT<)><;><n>    out.writeBytes<(>ok.toPacket<(><)><)><;><n>    channel.writeAndFlush<(>out<)><;><n><}>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = class_tokenize.encodingHF( method = 'this is a text' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 360,\n",
       " 492,\n",
       " 388,\n",
       " 1634,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'this',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġtext',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 21:54:08,114 : INFO : Encoding(num_tokens=112, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "2021-06-24 21:54:08,115 : INFO : [1, 35, 189, 50, 3905, 83, 1702, 105, 3342, 487, 105, 2867, 105, 1821, 2647, 84, 189, 79, 91, 195, 2841, 105, 4047, 2853, 105, 3342, 487, 105, 168, 2387, 1366, 105, 136, 180, 2298, 3943, 5731, 1072, 105, 1926, 105, 1025, 83, 93, 6150, 2399, 189, 38, 189, 23, 168, 464, 180, 2298, 3943, 5731, 1072, 93, 84, 85, 91, 195, 3644, 105, 3342, 487, 105, 2057, 105, 881, 3453, 1062, 189, 53, 2647, 105, 212, 2586, 83, 84, 105, 2057, 83, 84, 85, 91, 195, 2841, 105, 4047, 2853, 105, 3342, 487, 105, 168, 2387, 1366, 105, 4236, 4959, 189, 53, 189, 33, 2841, 105, 4047, 2853, 105, 3342, 487]\n",
      "2021-06-24 21:54:08,115 : INFO : 112\n",
      "2021-06-24 21:54:08,116 : INFO : ['<sos>', '<private>', 'Ġ', '<void>', 'Ġsuccess', '<(>', 'io', '.', 'net', 'ty', '.', 'channel', '.', 'Channel', 'Ġchannel', '<)>', 'Ġ', '<{>', '<n>', 'ĠĠĠ', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty', '.', 'm', 'ys', 'ql', '.', 'M', 'y', 'SQL', 'Hand', 'shake', 'Handler', '.', 'logger', '.', 'debug', '<(>', '\"', 'success', 'Ġinfo', 'Ġ', '<return>', 'Ġ', '<for>', 'm', 'ĠM', 'y', 'SQL', 'Hand', 'shake', 'Handler', '\"', '<)>', '<;>', '<n>', 'ĠĠĠ', 'Ġio', '.', 'net', 'ty', '.', 'buffer', '.', 'Byte', 'Buf', 'Ġout', 'Ġ', '<=>', 'Ġchannel', '.', 'al', 'loc', '<(>', '<)>', '.', 'buffer', '<(>', '<)>', '<;>', '<n>', 'ĠĠĠ', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty', '.', 'm', 'ys', 'ql', '.', 'OK', 'Ġok', 'Ġ', '<=>', 'Ġ', '<new>', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty']\n"
     ]
    }
   ],
   "source": [
    "class_tokenize = TokenizeHF() \n",
    "input_ids = class_tokenize.encodingHF( method = code[0] )\n",
    "#input_ids = input_ids['input_ids']\n",
    "logging.info(input_ids)\n",
    "logging.info(input_ids.ids)\n",
    "logging.info(len(input_ids.ids))\n",
    "logging.info(input_ids.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos><private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_tokenize.decodingHF( input_ids.ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_size( method, class_tkz ):\n",
    "    '''Return the size of the tokens for a give method based on id'''\n",
    "    input_ids = class_tkz.encodingHF( method = method )\n",
    "    return len(input_ids.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = df_cross['code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = class_tokenize.encodingHF( method = tt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty.mysql.OK<(><)><;><n>    ok.sequenceId <=> 2<;><n>    ok.setStatusFlag<(>Flags.SERVER_STATUS_AUTOCOMMIT<)><;><n>    out.writeBytes<(>ok.toPacket<(><)><)><;><n>    channel.writeAndFlush<(>out<)><;><n><}>',\n",
       "       '<private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.info<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty.mysql.OK<(><)><;><n>    ok.sequenceId <=> 2<;><n>    ok.setStatusFlag<(>Flags.SERVER_STATUS_AUTOCOMMIT<)><;><n>    out.writeBytes<(>ok.toPacket<(><)><)><;><n>    channel.writeAndFlush<(>out<)><;><n><}>',\n",
       "       '<private> <void> handleConnectRequest<(>com.assistant.connection.ConnectionManager.ConnectRequest request<)> <{><n>    com.assistant.utils.Log.d<(>com.assistant.connection.ConnectionManager.TAG, <(>\"handleConnectRequest, request<:>\" <+> request<)><)><;><n>    <if> <(>mStopped<)> <{><n>        com.assistant.utils.Log.d<(>com.assistant.connection.ConnectionManager.TAG, \"handleConnectRequest, mStopped is <true>\"<)><;><n>        removeConnectRequest<(>request.connId<)><;><n>        not<if>yConnectionCreationResult<(><null>, request.listener, <true>, Connection.CONNECTION_REASON_CODE_CONNECT_REQUEST_CANCELED<)><;><n>        <return> <;><n>    <}><n>    connectToInternal<(>request.ipAddress, request.port, request.listener, request<)><;><n><}>',\n",
       "       '<private> <void> handleConnectRequest<(>com.assistant.connection.ConnectionManager.ConnectRequest request<)> <{><n>    com.assistant.utils.Log.d<(>com.assistant.connection.ConnectionManager.TAG, <(>\"handleConnectRequest, request<:>\" <+> request<)><)><;><n>    <if> <(>mStopped<)> <{><n>        com.assistant.utils.Log.d<(>com.assistant.connection.ConnectionManager.TAG, \"handleConnectRequest, mStopped is <true>\"<)><;><n>        removeConnectRequest<(>request.ipAddress<)><;><n>        not<if>yConnectionCreationResult<(><null>, request.listener, <true>, Connection.CONNECTION_REASON_CODE_CONNECT_REQUEST_CANCELED<)><;><n>        <return> <;><n>    <}><n>    connectToInternal<(>request.ipAddress, request.port, request.listener, request<)><;><n><}>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_count_subwords = [method_size( class_tkz = class_tokenize, method = mtd )  for mtd in df_cross['code'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_count_subwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attaching new column\n",
    "df_error_tax['z_count_subwords'] = z_count_subwords\n",
    "df_cross['z_count_subwords'] = z_count_subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_tax.to_json(params['output'] / 'bug_fix_error_taxonomy_z_v1.jsonl', orient=\"records\", lines=True)\n",
    "df_cross.to_json(params['output'] / 'bug_fix_cross_entropy_z_v1.jsonl', orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>x_treatment</th>\n",
       "      <th>z_count_subwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006220</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.008038</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368741</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.337473</td>\n",
       "      <td>True</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927699</td>\n",
       "      <td>False</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  y_cross_entropy  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006220   \n",
       "1  <private> <void> success<(>io.netty.channel.Ch...         6.008038   \n",
       "2  <private> <void> handleConnectRequest<(>com.as...         6.368741   \n",
       "3  <private> <void> handleConnectRequest<(>com.as...         6.337473   \n",
       "4  <@>java.lang.Override<n><protected> <void> onS...         3.927699   \n",
       "\n",
       "   x_treatment  z_count_subwords  \n",
       "0        False               112  \n",
       "1         True               112  \n",
       "2        False               112  \n",
       "3         True               112  \n",
       "4        False               112  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>z_count_subwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>129444.000000</td>\n",
       "      <td>129444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.142502</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.510858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.066005</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.038853</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.982966</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.085069</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.283474</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_cross_entropy  z_count_subwords\n",
       "count    129444.000000          129444.0\n",
       "mean          6.142502             112.0\n",
       "std           1.510858               0.0\n",
       "min           2.066005             112.0\n",
       "25%           5.038853             112.0\n",
       "50%           5.982966             112.0\n",
       "75%           7.085069             112.0\n",
       "max          12.283474             112.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Covariate Analysis: SE Metrics\n",
    "> This module provides a tool for computing metrics (from static analysis) for python source code using Using <a href=\"https://github.com/mauricioaniche/ck\">CK Package</a>\n",
    "\n",
    "CK is a java package (jar) which is going to be executed from terminal. It requires the code which is going to be analyzed to be located at <i>physical</i> files. For that reason, the dataset is going to be used to produce some <i>.java</i> files.\n",
    "\n",
    "Each record, corresponds to a individual class. When working with method-level snippets, \"articial\" classes are created for performing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_dataset_to_files(df_series, destination_path):\n",
    "    \"\"\"\n",
    "    Function to generate .java files.\n",
    "    \n",
    "    Params:\n",
    "    # df_series: Pandas Series (DataFrame column) with the source code records.\n",
    "    # destination_path: (str) Absolute path to be used as directory for the generated files.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Collection of paths for the corresponding java files.\n",
    "    \n",
    "    \"\"\"\n",
    "    java_template = 'public class <class_name>{\\n    <code_snippet>\\n}'\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        logging.info('Creating directory.')\n",
    "        os.mkdir(destination_path)\n",
    "    \n",
    "    logging.info(\"Generating physical .java files.\")\n",
    "    \n",
    "    file_paths = []\n",
    "    for idx, value in df_series.iteritems():\n",
    "        class_name = f'ClassRecord{idx}'\n",
    "        code = java_template.replace('<class_name>', class_name)\n",
    "        code = code.replace('<code_snippet>', value)\n",
    "        file_path = f'{destination_path}/{class_name}.java'\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(code)\n",
    "            file_paths.append(file_path)\n",
    "            \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def jarWrapper(*args):\n",
    "    process = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE)\n",
    "    ret = []\n",
    "    while process.poll() is None:\n",
    "        line = process.stdout.readline()\n",
    "        if line != '' and line.endswith(b'\\n'):\n",
    "            ret.append(line[:-1])\n",
    "    stdout, stderr = process.communicate()\n",
    "    \n",
    "    ret += stdout.split(b'\\n')\n",
    "    if stderr != '':\n",
    "        ret += stderr.split(b'\\n')\n",
    "        \n",
    "    if '' in ret:\n",
    "        ret.remove('')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class JavaAnalyzer():\n",
    "    \"\"\"\n",
    "    Class get metrics f\n",
    "    \"\"\"\n",
    "    def __init__(self, ck_jar_path):\n",
    "        self.ck_jar_path = ck_jar_path\n",
    "    \n",
    "    def compute_metrics(self, df_series, files_destination_path):\n",
    "        \"\"\"\n",
    "        Computes metrics for a pandas series of java source code snippets\n",
    "        \n",
    "        Params\n",
    "        # df_series: Pandas series (df column) containing java source snippets\n",
    "        # files_destination_path: Path indicating where the physical .java files are going to be created (for metrics computation)\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        Pandas Dataframe containing metrics\n",
    "        \n",
    "        \"\"\"\n",
    "        file_paths = write_dataset_to_files(df_series, files_destination_path)\n",
    "        self.__call_ck_package(files_destination_path)\n",
    "        metrics_df = self.__get_metrics_df()\n",
    "        self.__remove_csv_files()\n",
    "        self.__remove_tmp_java_files(file_paths)\n",
    "        \n",
    "        return metrics_df\n",
    "        \n",
    "    def __call_ck_package(self, files_path):\n",
    "        \"\"\"\n",
    "        Performs call to external .jar package.\n",
    "        \"\"\"\n",
    "        args = [self.ck_jar_path, files_path, 'false', '0', 'True']\n",
    "        result = jarWrapper(*args)\n",
    "        logging.info(f'CK package produced this output:\\n{result}')\n",
    "        \n",
    "    def __get_metrics_df(self):\n",
    "        \"\"\"\n",
    "        Reads report files (csv) generated by the CK package.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        Pandas Dataframe containing appropriate metrics\n",
    "        \"\"\"\n",
    "        class_metrics_df = pd.read_csv('class.csv')\n",
    "        # method_metrics_df = pd.read_csv('method.csv')\n",
    "\n",
    "        # merged_df = pd.merge(left = class_metrics_df, right = method_metrics_df, left_on='file', right_on='file')\n",
    "\n",
    "        appropriate_columns = ['file','class', 'wmc', 'totalMethodsQty', 'staticMethodsQty', 'publicMethodsQty', 'privateMethodsQty',\n",
    "                          'protectedMethodsQty', 'defaultMethodsQty', 'abstractMethodsQty', 'finalMethodsQty','synchronizedMethodsQty',\n",
    "                          'totalFieldsQty', 'staticFieldsQty', 'publicFieldsQty', 'privateFieldsQty', 'protectedFieldsQty',\n",
    "                          'defaultFieldsQty', 'visibleFieldsQty', 'finalFieldsQty', 'synchronizedFieldsQty',\n",
    "                          'nosi', 'loc', 'returnQty', 'loopQty', 'comparisonsQty', 'tryCatchQty', 'parenthesizedExpsQty',\n",
    "                          'stringLiteralsQty', 'numbersQty', 'assignmentsQty', 'mathOperationsQty', 'variablesQty', 'maxNestedBlocksQty',\n",
    "                          'anonymousClassesQty', 'innerClassesQty', 'lambdasQty', 'uniqueWordsQty', 'modifiers']\n",
    "\n",
    "        class_metrics_df = class_metrics_df[appropriate_columns]\n",
    "\n",
    "        return class_metrics_df\n",
    "    \n",
    "    def __remove_csv_files(self):\n",
    "        \"\"\"\n",
    "        Removes files generated by CK package.\n",
    "        \"\"\"\n",
    "        if os.path.exists('class.csv'):\n",
    "            os.remove('class.csv')\n",
    "        if os.path.exists('method.csv'):\n",
    "            os.remove('method.csv')\n",
    "        if os.path.exists('field.csv'):\n",
    "            os.remove('field.csv')\n",
    "            \n",
    "    def __remove_tmp_java_files(self, paths):\n",
    "        \"\"\"\n",
    "        Removes the temporary generated java files.\n",
    "        \"\"\"\n",
    "        for file_path in paths:\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters for testing\n",
    "\n",
    "def get_default_params():\n",
    "    return {\n",
    "    'ck_jar_path': 'ck_metrics_tool/ck-metrics.jar',\n",
    "    'search_net_ds_path': '/tf/main/dvc-ds4se/code/searchnet/clean_java.csv',\n",
    "    'sampling_size': 100,\n",
    "    'physical_files_path': '/tf/main/nbs/test_data/test_metrics'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Java Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "      <th>bpe32_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>467203.000000</td>\n",
       "      <td>467203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.079653</td>\n",
       "      <td>146.274557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>189.121245</td>\n",
       "      <td>303.804009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27192.000000</td>\n",
       "      <td>52975.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code_len      bpe32_len\n",
       "count  467203.000000  467203.000000\n",
       "mean      113.079653     146.274557\n",
       "std       189.121245     303.804009\n",
       "min        20.000000      20.000000\n",
       "25%        42.000000      50.000000\n",
       "50%        67.000000      81.000000\n",
       "75%       122.000000     150.000000\n",
       "max     27192.000000   52975.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Java Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "      <th>bpe32_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>384868.000000</td>\n",
       "      <td>384868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.720826</td>\n",
       "      <td>147.151002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>194.007951</td>\n",
       "      <td>313.904001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27192.000000</td>\n",
       "      <td>52975.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code_len      bpe32_len\n",
       "count  384868.000000  384868.000000\n",
       "mean      113.720826     147.151002\n",
       "std       194.007951     313.904001\n",
       "min        20.000000      20.000000\n",
       "25%        42.000000      50.000000\n",
       "50%        67.000000      81.000000\n",
       "75%       122.000000     151.000000\n",
       "max     27192.000000   52975.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:31:54,629 : INFO : [            code_len      bpe32_len\n",
      "count  384868.000000  384868.000000\n",
      "mean      113.720826     147.151002\n",
      "std       194.007951     313.904001\n",
      "min        20.000000      20.000000\n",
      "25%        42.000000      50.000000\n",
      "50%        67.000000      81.000000\n",
      "75%       122.000000     151.000000\n",
      "max     27192.000000   52975.000000,            code_len     bpe32_len\n",
      "count  14605.000000  14605.000000\n",
      "mean      94.331736    120.097843\n",
      "std      115.802231    171.079255\n",
      "min       21.000000     21.000000\n",
      "25%       39.000000     46.000000\n",
      "50%       59.000000     71.000000\n",
      "75%      104.000000    127.000000\n",
      "max     3099.000000   5747.000000,            code_len     bpe32_len\n",
      "count  25011.000000  25011.000000\n",
      "mean     114.274599    148.204710\n",
      "std      166.432695    245.938732\n",
      "min       21.000000     22.000000\n",
      "25%       43.000000     52.000000\n",
      "50%       69.000000     84.000000\n",
      "75%      125.000000    155.000000\n",
      "max     5685.000000  10015.000000,            code_len     bpe32_len\n",
      "count  42719.000000  42719.000000\n",
      "mean     113.013156    146.197781\n",
      "std      176.473185    275.722996\n",
      "min       20.000000     21.000000\n",
      "25%       42.000000     50.000000\n",
      "50%       67.000000     82.000000\n",
      "75%      123.000000    151.000000\n",
      "max     8404.000000  15552.000000]\n"
     ]
    }
   ],
   "source": [
    "logging.info([ p.describe() for p in list_all_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:29:30,408 : INFO : [45.9606, 37.065, 47.4432, 44.477999999999994]\n"
     ]
    }
   ],
   "source": [
    "logging.info([ stats.median_absolute_deviation(p['code_len'].values) for p in list_all_partitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent Characters\n",
    "train_tokens = df_train.code_tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab_tokens = [ eval(method)  for method in train_tokens ] #Evaluating given tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter_tokens = [ Counter(method) for method in train_vocab_tokens ] #Counting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = functools.reduce(lambda a,b : a+b, train_counter_tokens ) ## [Warning! Time Consuming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc0adc03a92e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_counter' is not defined"
     ]
    }
   ],
   "source": [
    "train_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persisting the counter object\n",
    "with open(params['eda']+'['+ str(datetime.datetime.now()) +']-codesearchnet_token_counts.pickle', 'wb') as outputfile:\n",
    "    pickle.dump( train_counter, outputfile )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
