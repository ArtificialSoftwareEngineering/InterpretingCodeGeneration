{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install 'tokenizers==0.9.3'\n",
    "#! pip install 'transformers==3.5.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariate Analysis and Feature Engineering \n",
    ">\n",
    ">@danaderp 11.17.20 .\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools \n",
    "from operator import or_\n",
    "from collections import Counter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sentencepiece as spm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import bootstrap_plot\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024\n",
    "#df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "dvc_path = Path('../dvc-icodegen/')\n",
    "def params():\n",
    "    return {\n",
    "        'tokenizer':dvc_path / 'models/bpe/tokenizer-java-v1.json',\n",
    "        'tb_01':dvc_path / 'nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/bug_fix_error_taxonomy.jsonl',\n",
    "        'tb_02':dvc_path / 'nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/bug_fix_cross_entropy.jsonl'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "logging.info( params['tokenizer'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-23 20:32:32,334 : INFO : Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2021-06-23 20:32:32,335 : INFO : Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-06-23 20:32:32,335 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "df_error_tax = pd.read_json(\n",
    "            params['tb_01'], orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Covariate Analysis: Token Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload tokenizers here\n",
    "#Use Tokenizer recent version\n",
    "#Load tokenizer\n",
    "#Update files (for one time) the main json files to add the column of tokenizers\n",
    "#Count the tokens per method\n",
    "#Visualize the most popular tokens (and less popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeHF():\n",
    "    def __init__( self ):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet (plain text)\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        #Load tokenizer\n",
    "        self.tokenizer = Tokenizer.from_file( str( params['tokenizer'] ) )\n",
    "        pass\n",
    "    \n",
    "    #@staticmethod\n",
    "    def encodingHF(self, method ):\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "             tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "             tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "    #@staticmethod\n",
    "    def decodingHF( self, idss ):\n",
    "        return self.tokenizer.decode( idss , skip_special_tokens=False )\n",
    "        #return self.tokenizer.convert_ids_to_tokens(idss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = df_error_tax['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty.mysql.OK<(><)><;><n>    ok.sequenceId <=> 2<;><n>    ok.setStatusFlag<(>Flags.SERVER_STATUS_AUTOCOMMIT<)><;><n>    out.writeBytes<(>ok.toPacket<(><)><)><;><n>    channel.writeAndFlush<(>out<)><;><n><}>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-23 21:15:35,433 : INFO : Encoding(num_tokens=112, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "2021-06-23 21:15:35,434 : INFO : [1, 35, 189, 50, 3905, 83, 1702, 105, 3342, 487, 105, 2867, 105, 1821, 2647, 84, 189, 79, 91, 195, 2841, 105, 4047, 2853, 105, 3342, 487, 105, 168, 2387, 1366, 105, 136, 180, 2298, 3943, 5731, 1072, 105, 1926, 105, 1025, 83, 93, 6150, 2399, 189, 38, 189, 23, 168, 464, 180, 2298, 3943, 5731, 1072, 93, 84, 85, 91, 195, 3644, 105, 3342, 487, 105, 2057, 105, 881, 3453, 1062, 189, 53, 2647, 105, 212, 2586, 83, 84, 105, 2057, 83, 84, 85, 91, 195, 2841, 105, 4047, 2853, 105, 3342, 487, 105, 168, 2387, 1366, 105, 4236, 4959, 189, 53, 189, 33, 2841, 105, 4047, 2853, 105, 3342, 487]\n",
      "2021-06-23 21:15:35,434 : INFO : ['<sos>', '<private>', 'Ġ', '<void>', 'Ġsuccess', '<(>', 'io', '.', 'net', 'ty', '.', 'channel', '.', 'Channel', 'Ġchannel', '<)>', 'Ġ', '<{>', '<n>', 'ĠĠĠ', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty', '.', 'm', 'ys', 'ql', '.', 'M', 'y', 'SQL', 'Hand', 'shake', 'Handler', '.', 'logger', '.', 'debug', '<(>', '\"', 'success', 'Ġinfo', 'Ġ', '<return>', 'Ġ', '<for>', 'm', 'ĠM', 'y', 'SQL', 'Hand', 'shake', 'Handler', '\"', '<)>', '<;>', '<n>', 'ĠĠĠ', 'Ġio', '.', 'net', 'ty', '.', 'buffer', '.', 'Byte', 'Buf', 'Ġout', 'Ġ', '<=>', 'Ġchannel', '.', 'al', 'loc', '<(>', '<)>', '.', 'buffer', '<(>', '<)>', '<;>', '<n>', 'ĠĠĠ', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty', '.', 'm', 'ys', 'ql', '.', 'OK', 'Ġok', 'Ġ', '<=>', 'Ġ', '<new>', 'Ġorg', '.', 'my', 'cat', '.', 'net', 'ty']\n"
     ]
    }
   ],
   "source": [
    "class_tokenize = TokenizeHF() \n",
    "input_ids = class_tokenize.encodingHF( method = code[0] )\n",
    "#input_ids = input_ids['input_ids']\n",
    "logging.info(input_ids)\n",
    "logging.info(input_ids.ids)\n",
    "logging.info(input_ids.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos><private> <void> success<(>io.netty.channel.Channel channel<)> <{><n>    org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug<(>\"success info <return> <for>m MySQLHandshakeHandler\"<)><;><n>    io.netty.buffer.ByteBuf out <=> channel.alloc<(><)>.buffer<(><)><;><n>    org.mycat.netty.mysql.OK ok <=> <new> org.mycat.netty'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_tokenize.decodingHF( input_ids.ids )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Covariate Analysis: SE Metrics\n",
    "> This module provides a tool for computing metrics (from static analysis) for python source code using Using <a href=\"https://github.com/mauricioaniche/ck\">CK Package</a>\n",
    "\n",
    "CK is a java package (jar) which is going to be executed from terminal. It requires the code which is going to be analyzed to be located at <i>physical</i> files. For that reason, the dataset is going to be used to produce some <i>.java</i> files.\n",
    "\n",
    "Each record, corresponds to a individual class. When working with method-level snippets, \"articial\" classes are created for performing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_dataset_to_files(df_series, destination_path):\n",
    "    \"\"\"\n",
    "    Function to generate .java files.\n",
    "    \n",
    "    Params:\n",
    "    # df_series: Pandas Series (DataFrame column) with the source code records.\n",
    "    # destination_path: (str) Absolute path to be used as directory for the generated files.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Collection of paths for the corresponding java files.\n",
    "    \n",
    "    \"\"\"\n",
    "    java_template = 'public class <class_name>{\\n    <code_snippet>\\n}'\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        logging.info('Creating directory.')\n",
    "        os.mkdir(destination_path)\n",
    "    \n",
    "    logging.info(\"Generating physical .java files.\")\n",
    "    \n",
    "    file_paths = []\n",
    "    for idx, value in df_series.iteritems():\n",
    "        class_name = f'ClassRecord{idx}'\n",
    "        code = java_template.replace('<class_name>', class_name)\n",
    "        code = code.replace('<code_snippet>', value)\n",
    "        file_path = f'{destination_path}/{class_name}.java'\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(code)\n",
    "            file_paths.append(file_path)\n",
    "            \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def jarWrapper(*args):\n",
    "    process = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE)\n",
    "    ret = []\n",
    "    while process.poll() is None:\n",
    "        line = process.stdout.readline()\n",
    "        if line != '' and line.endswith(b'\\n'):\n",
    "            ret.append(line[:-1])\n",
    "    stdout, stderr = process.communicate()\n",
    "    \n",
    "    ret += stdout.split(b'\\n')\n",
    "    if stderr != '':\n",
    "        ret += stderr.split(b'\\n')\n",
    "        \n",
    "    if '' in ret:\n",
    "        ret.remove('')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class JavaAnalyzer():\n",
    "    \"\"\"\n",
    "    Class get metrics f\n",
    "    \"\"\"\n",
    "    def __init__(self, ck_jar_path):\n",
    "        self.ck_jar_path = ck_jar_path\n",
    "    \n",
    "    def compute_metrics(self, df_series, files_destination_path):\n",
    "        \"\"\"\n",
    "        Computes metrics for a pandas series of java source code snippets\n",
    "        \n",
    "        Params\n",
    "        # df_series: Pandas series (df column) containing java source snippets\n",
    "        # files_destination_path: Path indicating where the physical .java files are going to be created (for metrics computation)\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        Pandas Dataframe containing metrics\n",
    "        \n",
    "        \"\"\"\n",
    "        file_paths = write_dataset_to_files(df_series, files_destination_path)\n",
    "        self.__call_ck_package(files_destination_path)\n",
    "        metrics_df = self.__get_metrics_df()\n",
    "        self.__remove_csv_files()\n",
    "        self.__remove_tmp_java_files(file_paths)\n",
    "        \n",
    "        return metrics_df\n",
    "        \n",
    "    def __call_ck_package(self, files_path):\n",
    "        \"\"\"\n",
    "        Performs call to external .jar package.\n",
    "        \"\"\"\n",
    "        args = [self.ck_jar_path, files_path, 'false', '0', 'True']\n",
    "        result = jarWrapper(*args)\n",
    "        logging.info(f'CK package produced this output:\\n{result}')\n",
    "        \n",
    "    def __get_metrics_df(self):\n",
    "        \"\"\"\n",
    "        Reads report files (csv) generated by the CK package.\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "        Pandas Dataframe containing appropriate metrics\n",
    "        \"\"\"\n",
    "        class_metrics_df = pd.read_csv('class.csv')\n",
    "        # method_metrics_df = pd.read_csv('method.csv')\n",
    "\n",
    "        # merged_df = pd.merge(left = class_metrics_df, right = method_metrics_df, left_on='file', right_on='file')\n",
    "\n",
    "        appropriate_columns = ['file','class', 'wmc', 'totalMethodsQty', 'staticMethodsQty', 'publicMethodsQty', 'privateMethodsQty',\n",
    "                          'protectedMethodsQty', 'defaultMethodsQty', 'abstractMethodsQty', 'finalMethodsQty','synchronizedMethodsQty',\n",
    "                          'totalFieldsQty', 'staticFieldsQty', 'publicFieldsQty', 'privateFieldsQty', 'protectedFieldsQty',\n",
    "                          'defaultFieldsQty', 'visibleFieldsQty', 'finalFieldsQty', 'synchronizedFieldsQty',\n",
    "                          'nosi', 'loc', 'returnQty', 'loopQty', 'comparisonsQty', 'tryCatchQty', 'parenthesizedExpsQty',\n",
    "                          'stringLiteralsQty', 'numbersQty', 'assignmentsQty', 'mathOperationsQty', 'variablesQty', 'maxNestedBlocksQty',\n",
    "                          'anonymousClassesQty', 'innerClassesQty', 'lambdasQty', 'uniqueWordsQty', 'modifiers']\n",
    "\n",
    "        class_metrics_df = class_metrics_df[appropriate_columns]\n",
    "\n",
    "        return class_metrics_df\n",
    "    \n",
    "    def __remove_csv_files(self):\n",
    "        \"\"\"\n",
    "        Removes files generated by CK package.\n",
    "        \"\"\"\n",
    "        if os.path.exists('class.csv'):\n",
    "            os.remove('class.csv')\n",
    "        if os.path.exists('method.csv'):\n",
    "            os.remove('method.csv')\n",
    "        if os.path.exists('field.csv'):\n",
    "            os.remove('field.csv')\n",
    "            \n",
    "    def __remove_tmp_java_files(self, paths):\n",
    "        \"\"\"\n",
    "        Removes the temporary generated java files.\n",
    "        \"\"\"\n",
    "        for file_path in paths:\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters for testing\n",
    "\n",
    "def get_default_params():\n",
    "    return {\n",
    "    'ck_jar_path': 'ck_metrics_tool/ck-metrics.jar',\n",
    "    'search_net_ds_path': '/tf/main/dvc-ds4se/code/searchnet/clean_java.csv',\n",
    "    'sampling_size': 100,\n",
    "    'physical_files_path': '/tf/main/nbs/test_data/test_metrics'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg Number of Subwords \n",
    "count_subwords = [len( eval(mtd) ) for mtd in df_train['code_tokens'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['count_tokens'] = count_subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Java Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "      <th>bpe32_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>467203.000000</td>\n",
       "      <td>467203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.079653</td>\n",
       "      <td>146.274557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>189.121245</td>\n",
       "      <td>303.804009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27192.000000</td>\n",
       "      <td>52975.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code_len      bpe32_len\n",
       "count  467203.000000  467203.000000\n",
       "mean      113.079653     146.274557\n",
       "std       189.121245     303.804009\n",
       "min        20.000000      20.000000\n",
       "25%        42.000000      50.000000\n",
       "50%        67.000000      81.000000\n",
       "75%       122.000000     150.000000\n",
       "max     27192.000000   52975.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Java Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "      <th>bpe32_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>384868.000000</td>\n",
       "      <td>384868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.720826</td>\n",
       "      <td>147.151002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>194.007951</td>\n",
       "      <td>313.904001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27192.000000</td>\n",
       "      <td>52975.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code_len      bpe32_len\n",
       "count  384868.000000  384868.000000\n",
       "mean      113.720826     147.151002\n",
       "std       194.007951     313.904001\n",
       "min        20.000000      20.000000\n",
       "25%        42.000000      50.000000\n",
       "50%        67.000000      81.000000\n",
       "75%       122.000000     151.000000\n",
       "max     27192.000000   52975.000000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:31:54,629 : INFO : [            code_len      bpe32_len\n",
      "count  384868.000000  384868.000000\n",
      "mean      113.720826     147.151002\n",
      "std       194.007951     313.904001\n",
      "min        20.000000      20.000000\n",
      "25%        42.000000      50.000000\n",
      "50%        67.000000      81.000000\n",
      "75%       122.000000     151.000000\n",
      "max     27192.000000   52975.000000,            code_len     bpe32_len\n",
      "count  14605.000000  14605.000000\n",
      "mean      94.331736    120.097843\n",
      "std      115.802231    171.079255\n",
      "min       21.000000     21.000000\n",
      "25%       39.000000     46.000000\n",
      "50%       59.000000     71.000000\n",
      "75%      104.000000    127.000000\n",
      "max     3099.000000   5747.000000,            code_len     bpe32_len\n",
      "count  25011.000000  25011.000000\n",
      "mean     114.274599    148.204710\n",
      "std      166.432695    245.938732\n",
      "min       21.000000     22.000000\n",
      "25%       43.000000     52.000000\n",
      "50%       69.000000     84.000000\n",
      "75%      125.000000    155.000000\n",
      "max     5685.000000  10015.000000,            code_len     bpe32_len\n",
      "count  42719.000000  42719.000000\n",
      "mean     113.013156    146.197781\n",
      "std      176.473185    275.722996\n",
      "min       20.000000     21.000000\n",
      "25%       42.000000     50.000000\n",
      "50%       67.000000     82.000000\n",
      "75%      123.000000    151.000000\n",
      "max     8404.000000  15552.000000]\n"
     ]
    }
   ],
   "source": [
    "logging.info([ p.describe() for p in list_all_partitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 12:29:30,408 : INFO : [45.9606, 37.065, 47.4432, 44.477999999999994]\n"
     ]
    }
   ],
   "source": [
    "logging.info([ stats.median_absolute_deviation(p['code_len'].values) for p in list_all_partitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent Characters\n",
    "train_tokens = df_train.code_tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab_tokens = [ eval(method)  for method in train_tokens ] #Evaluating given tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter_tokens = [ Counter(method) for method in train_vocab_tokens ] #Counting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = functools.reduce(lambda a,b : a+b, train_counter_tokens ) ## [Warning! Time Consuming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc0adc03a92e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_counter' is not defined"
     ]
    }
   ],
   "source": [
    "train_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persisting the counter object\n",
    "with open(params['eda']+'['+ str(datetime.datetime.now()) +']-codesearchnet_token_counts.pickle', 'wb') as outputfile:\n",
    "    pickle.dump( train_counter, outputfile )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
