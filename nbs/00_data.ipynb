{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> This model contains all the necessary functionality for managing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import icodegen\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from subprocess import CalledProcessError, check_output\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import pandas as pd\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>docstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>594</th>\n      <td>protected static int extractMatchingFieldsUnio...</td>\n      <td>==============================================...</td>\n    </tr>\n    <tr>\n      <th>3643</th>\n      <td>public static boolean rowKeyMatchsTSUID(final ...</td>\n      <td>Returns true if the given TSUID matches the ro...</td>\n    </tr>\n    <tr>\n      <th>20875</th>\n      <td>public void removeExtension(Extension extensio...</td>\n      <td>Removes the given extension and any components...</td>\n    </tr>\n    <tr>\n      <th>13055</th>\n      <td>public static int numberOfFreeParameters(Relat...</td>\n      <td>Compute the number of free parameters.\\n\\n@par...</td>\n    </tr>\n    <tr>\n      <th>23707</th>\n      <td>private static void createLikelySubtags(TypeSp...</td>\n      <td>Create likely subtags mapping.</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                                    code  \\\n",
       "594    protected static int extractMatchingFieldsUnio...   \n",
       "3643   public static boolean rowKeyMatchsTSUID(final ...   \n",
       "20875  public void removeExtension(Extension extensio...   \n",
       "13055  public static int numberOfFreeParameters(Relat...   \n",
       "23707  private static void createLikelySubtags(TypeSp...   \n",
       "\n",
       "                                               docstring  \n",
       "594    ==============================================...  \n",
       "3643   Returns true if the given TSUID matches the ro...  \n",
       "20875  Removes the given extension and any components...  \n",
       "13055  Compute the number of free parameters.\\n\\n@par...  \n",
       "23707                     Create likely subtags mapping.  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from ds4se.mgmnt.prep.i import jsonl_list_to_dataframe, get_dfs\n",
    "\n",
    "path = Path('/home/nathan/Downloads/')\n",
    "df_trn, df_val, df_tst = get_dfs(path/\"java/final/jsonl\")\n",
    "\n",
    "sample = 0.01\n",
    "df_trn = df_trn.sample(frac = sample)\n",
    "df_val = df_val.sample(frac = sample)\n",
    "df_tst = df_tst.sample(frac = sample)\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4545, 153, 269)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "len(df_trn), len(df_val), len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>this is a test</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>भारत test</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "             code\n",
       "0  this is a test\n",
       "1       भारत test"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(['this is a test', 'भारत test'], columns = ['code']);df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _isASCII(mthd: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given method contains only ASCII characters. From https://stackoverflow.com/a/27084708/5768407.\n",
    "\n",
    "    :param mthd: the method to verify contains only ASCII characters\n",
    "    :returns: returns a boolean representing whether or not the given method contains only ASCII characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mthd.encode(encoding = 'utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def remove_non_ascii(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all methods that contain non-ascii characters from a given pandas dataframe, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a new dataframe without methods that contain non-ascii characters\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df = df[df.code.apply(lambda x: _isASCII(x))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ASCII_DF = pd.DataFrame(['this is a test'], columns = ['code'])\n",
    "df_non_ascii = remove_non_ascii(df_fake)\n",
    "\n",
    "assert (NON_ASCII_DF == df_non_ascii).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 ms, sys: 329 µs, total: 6.85 ms\nWall time: 6.52 ms\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%time df_trn = remove_non_ascii(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>public void setPipelines(java.util.Collection&lt;...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                                code\n",
       "0  public void setPipelines(java.util.Collection<..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\n",
    "    '''public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "        if (pipelines == null) {\n",
    "            this.pipelines = null;\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(pipelines);\n",
    "    }\n",
    "    '''\n",
    "], columns = ['code']); df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _beautify(mthd: str) -> str:\n",
    "    \"\"\"\n",
    "    Beautifies a given method using uncrustify with the sun.cfg style, i.e., Oracle's style.\n",
    "\n",
    "    :param mthd: the method to beautify\n",
    "    :returns: returns a beautified version of the given method\n",
    "    \"\"\"\n",
    "    # get path of icodegen\n",
    "    icodegen_path = Path(icodegen.__path__[0])\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path('/tmp')\n",
    "    tmp_path.mkdir(parents = True, exist_ok = True)\n",
    "    with open(tmp_path/'tmp.java', 'w') as f:\n",
    "        f.write(mthd)\n",
    "\n",
    "    try:\n",
    "        beaut_mthd = check_output([\n",
    "            icodegen_path/'uncrustify', '-c', icodegen_path/'sun.cfg',\n",
    "            '-f', tmp_path/'tmp.java'\n",
    "        ]).decode('utf-8')\n",
    "    except CalledProcessError as e:\n",
    "        # Exception thrown when the method is malformed, i.e, it is missing a curly brace\n",
    "        beaut_mthd = e.output.decode('utf-8')\n",
    "\n",
    "    return beaut_mthd\n",
    "\n",
    "def beautify_code(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beautify the methods in a pandas dataframe using uncrustify with the sun.cfg style, i.e., Oracle's style, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the methods beautified\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _beautify(mthd))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUT_MTHD = '''public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "    if (pipelines == null) {\n",
    "\tthis.pipelines = null;\n",
    "\treturn;\n",
    "    }\n",
    "    this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(\n",
    "\tpipelines);\n",
    "}\n",
    "'''\n",
    "\n",
    "df_beaut = beautify_code(df_fake)\n",
    "\n",
    "assert BEAUT_MTHD == df_beaut.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 49.4 s, total: 52 s\nWall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%time df_beaut = beautify_code(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void parseBeanMap(Object bean, SerializerJson deser)\n  {\n    Event event;\n\n    while ((event = next()) == Event.KEY_NAME) {\n      String fieldName = getString();\n\n      deser.readField(this, bean, fieldName);\n    }\n\n    if (event != Event.END_OBJECT) {\n      throw new JsonParsingException(L.l(\"Unexpected JSON {0} while parsing Java object {1}\",\n                                        event,\n                                        bean));\n    }\n  }\npublic void parseBeanMap(Object bean, SerializerJson deser) {\n    Event event;\n\n    while ((event = next()) == Event.KEY_NAME) {\n\tString fieldName = getString();\n\n\tdeser.readField(this, bean, fieldName);\n    }\n    if (event != Event.END_OBJECT) {\n\tthrow new JsonParsingException(L.l(\n\t    \"Unexpected JSON {0} while parsing Java object {1}\", event, bean));\n    }\n}\n\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "idx = 0\n",
    "print(df_trn.code.values[idx])\n",
    "print(df_beaut.code.values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\n",
    "    '\\n': '<n>'\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    'abstract': '<abstract>', 'assert': '<assert>', 'boolean': '<boolean>',\n",
    "    'break': '<break>', 'byte': '<byte>', 'case': '<case>',\n",
    "    'catch': '<catch>', 'char': '<char>', 'class': '<class>',\n",
    "    'const': '<const>', 'continue': '<continue>', 'default': '<default>',\n",
    "    'do': '<do>', 'double': '<double>', 'else': '<else>',\n",
    "    'enum': '<enum>', 'extends': '<extends>', 'final': '<final>',\n",
    "    'finally': '<finally>', 'float': '<float>', 'for': '<for>',\n",
    "    'goto': '<goto>', 'if': '<if>', 'implements': '<implements>',\n",
    "    'import': '<import>', 'instanceof': '<instanceof>', 'int': '<int>',\n",
    "    'interface': '<interface>', 'long': '<long>', 'native': '<native>',\n",
    "    'new': '<new>', 'package': '<package>', 'private': '<private>',\n",
    "    'protected': '<protected>', 'public': '<public>', 'return': '<return>',\n",
    "    'short': '<short>', 'static': '<static>', 'strictfp': '<strictfp>',\n",
    "    'super': '<super>', 'switch': '<switch>', 'synchronized': '<synchronized>',\n",
    "    'this': '<this>', 'throw': '<throw>', 'throws': '<throws>',\n",
    "    'transient': '<transient>', 'try': '<try>', 'void': '<void>',\n",
    "    'volatile': '<volatile>', 'while': '<while>'\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    '=': '<=>', '+': '<+>', '-': '<->',\n",
    "    '*': '<*>', '/': '</>', '%': '<%>',\n",
    "    '++': '<++>', '--': '<-->', '!': '<!>',\n",
    "    '==': '<==>', '!=': '<!=>', '>': '<greater>',\n",
    "    '>=': '<greater_equal>', '<': '<lesser>', '<=': '<lesser_equal>',\n",
    "    '&&': '<&&>', '||': '<||>', '?': '<?>',\n",
    "    ':': '<:>', '~': '<~>', '<<': '<double_lesser>',\n",
    "    '>>': '<double_greater>', '>>>': '<triple_greater>', '&': '<&>',\n",
    "    '^': '<^>', '|': '<|>'\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    '{': '<{>', '}': '<}>', '[': '<[>',\n",
    "    ']': '<]>', '<': '<lesser>', '>': '<greater>',\n",
    "    '(': '<(>', ')': '<)>', ';': '<;>'\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    '@': '<@>', '...': '<...>',\n",
    "    'null': '<null>', 'true': '<true>', 'false': '<false>'\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens, **java_operator_tokens, **java_structural_tokens,\n",
    "    **java_extra_tokens, **extra_tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&gt;&gt;&gt; &gt; + public ++ \\n\\n \\t \\t \\t\\t</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                  code\n",
       "0  >>> > + public ++ \\n\\n \\t \\t \\t\\t  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(['>>> > + public ++ \\n\\n \\t \\t \\t\\t  '], columns = ['code']); df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _replace_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for replacing all special tokens in a given method. This will replace longer special tokens first in order to not mistakenly breakup a special token that is part of a longer sequence. Adapted from https://stackoverflow.com/a/6117124/5768407 and https://stackoverflow.com/a/11753945/5768407\n",
    "\n",
    "    :param mthd: the method to have its special tokens replaced\n",
    "    :param spec_toks: a dictionary containing the special tokens to replace and the new tokens to replace them with\n",
    "    :returns: returns the method with its special tokens replaced\n",
    "    \"\"\"\n",
    "    # construct escaped versions of keys for running through regex\n",
    "    spec_toks = dict((re.escape(k), spec_toks[k]) for k in sorted(spec_toks, key=len, reverse=True))\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "\n",
    "    return mthd\n",
    "\n",
    "def replace_special_tokens(df: pd.DataFrame, spec_toks: Dict[str, str], n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all the special tokens in a pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to replace special tokens in\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the special tokens replaced\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _replace_toks(mthd, spec_toks))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACED_MTHD = '<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  '\n",
    "df_replaced = replace_special_tokens(df_fake, java_special_tokens)\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<@>VisibleForTesting<n>  URIBuilder <const>ra<int>s<(>Constra<int>s <const>ra<int>s<)> <{><n>    options.putAll<(><const>ra<int>s.toQueryMap<(><)><)><;><n>    <this>.isView <=> <!><const>ra<int>s.isUnbounded<(><)><;><n>    <return> <this><;><n>  <}>\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "df_replaced = replace_special_tokens(df_trn, java_special_tokens)\n",
    "print(df_replaced.code.values[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fake_data = '<triple_greater> <greater> <+> <public> <++> <n><n>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_tokenizer(df: pd.DataFrame, n: Optional[int] = None, vocab_sz: Optional[int] = 20_000, min_freq: Optional[int] = 2, output: Optional[Path] = None) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Train a ByteLevel BPE tokenizer on a given pandas dataframe. Code adapted from https://github.com/huggingface/tokenizers/tree/master/bindings/python.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the tokenizer train on\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :param vocab_sz: the maximum vocabulary size of the trained tokenizer\n",
    "    :param min_freq: the minimum frequency a token has to occur to be considered\n",
    "    :returns: returns a trained ByteLevel BPE tokenizer\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path('/tmp')\n",
    "    tmp_path.mkdir(parents = True, exist_ok = True)\n",
    "    with open(tmp_path/'tmp_tokenize.txt', 'w') as f:\n",
    "        f.write('\\n'.join(df.code.values[:n]))\n",
    "\n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space = True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets = True)\n",
    "\n",
    "    # train tokenizer with data in tmp file\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size = vocab_sz, min_frequency = min_freq,\n",
    "        special_tokens = list(java_special_tokens.values())\n",
    "    )\n",
    "    tokenizer.train(trainer, [str(tmp_path/'tmp_tokenize.txt')])\n",
    "\n",
    "    # save tokenizer if output path given\n",
    "    if output is not None:\n",
    "        tokenizer.save(output, pretty = True)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_SPEC = [\n",
    "    '<triple_greater>', 'Ġ', '<greater>', 'Ġ', '<+>', 'Ġ',\n",
    "    '<public>', 'Ġ', '<++>', 'Ġ', '<n>', '<n>'\n",
    "]\n",
    "tokenizer = train_tokenizer(df_fake)\n",
    "encoded = tokenizer.encode(fake_data)\n",
    "\n",
    "assert TOKENIZED_SPEC == encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<public> <void> parseBeanMap<(>Object bean, SerializerJson deser<)> <{><n>    Event event<;><n><n>    <while> <(><(>event <=> next<(><)><)> <==> Event.KEY_NAME<)> <{><n>\tString fieldName <=> getString<(><)><;><n><n>\tdeser.readField<(><this>, bean, fieldName<)><;><n>    <}><n>    <if> <(>event <!=> Event.END_OBJECT<)> <{><n>\t<throw> <new> JsonParsingException<(>L.l<(><n>\t    \"Unexpected JSON <{>0<}> <while> parsing Java object <{>1<}>\", event, bean<)><)><;><n>    <}><n><}><n>\n====================================================================================================\n['<public>', 'Ġ', '<void>', 'Ġparse', 'Bean', 'Map', '<(>', 'ĠObject', 'Ġbean', ',', 'ĠSerial', 'izer', 'Json', 'Ġdeser', '<)>', 'Ġ', '<{>', '<n>', 'ĠĠĠ', 'ĠEvent', 'Ġevent', '<;>', '<n>', '<n>', 'ĠĠĠĠ', '<while>', 'Ġ', '<(>', '<(>', 'Ġevent', 'Ġ', '<=>', 'Ġnext', '<(>', '<)>', '<)>', 'Ġ', '<==>', 'ĠEvent', '.', 'KEY', '_', 'NAME', '<)>', 'Ġ', '<{>', '<n>', 'Ġ', 'ĉ', 'String', 'ĠfieldName', 'Ġ', '<=>', 'ĠgetString', '<(>', '<)>', '<;>', '<n>', '<n>', 'Ġ', 'ĉ', 'des', 'er', '.', 'read', 'Field', '<(>', '<this>', 'Ġ,', 'Ġbean', ',', 'ĠfieldName', '<)>', '<;>', '<n>', 'ĠĠĠĠ', '<}>', '<n>', 'ĠĠĠĠ', '<if>', 'Ġ', '<(>', 'Ġevent', 'Ġ', '<!=>', 'ĠEvent', '.', 'END', '_', 'OBJECT', '<)>', 'Ġ', '<{>', '<n>', 'Ġĉ', '<throw>', 'Ġ', '<new>', 'ĠJson', 'ParsingException', '<(>', 'ĠL', '.', 'l', '<(>', '<n>', 'ĠĉĠĠĠ', 'Ġ\"', 'Unexpected', 'ĠJSON', 'Ġ', '<{>', 'Ġ0', '<}>', 'Ġ', '<while>', 'Ġparsing', 'ĠJava', 'Ġobject', 'Ġ', '<{>', 'Ġ1', '<}>', 'Ġ\",', 'Ġevent', ',', 'Ġbean', '<)>', '<)>', '<;>', '<n>', 'ĠĠĠĠ', '<}>', '<n>', '<}>', '<n>']\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "idx = 0\n",
    "df_beaut = beautify_code(df_trn, n = 10)\n",
    "df_replaced = replace_special_tokens(df_beaut, java_special_tokens)\n",
    "\n",
    "tokenizer = train_tokenizer(df_trn)\n",
    "encoded = tokenizer.encode(df_replaced.code.values[idx])\n",
    "print(df_replaced.code.values[idx])\n",
    "print('=' * 100)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icodegen",
   "language": "python",
   "name": "icodegen"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
