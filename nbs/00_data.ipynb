{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> This model contains all the necessary functionality for managing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import icodegen\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from subprocess import CalledProcessError, check_output\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import pandas as pd\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-37f8f951532e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/nathan/Downloads/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"java/final/jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/ds4se/mgmnt/prep/i.py\u001b[0m in \u001b[0;36mget_dfs\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**/*.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonl_list_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"docstring\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/ds4se/mgmnt/prep/i.py\u001b[0m in \u001b[0;36mjsonl_list_to_dataframe\u001b[0;34m(file_list, columns)\u001b[0m\n\u001b[1;32m     20\u001b[0m                                    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                    lines=True)[columns]\n\u001b[0;32m---> 22\u001b[0;31m                       for f in file_list], sort=False)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/ds4se/mgmnt/prep/i.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m                                    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                    lines=True)[columns]\n\u001b[0;32m---> 22\u001b[0;31m                       for f in file_list], sort=False)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/icodegen/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from ds4se.mgmnt.prep.i import jsonl_list_to_dataframe, get_dfs\n",
    "\n",
    "path = Path('/home/nathan/Downloads/')\n",
    "df_trn, df_val, df_tst = get_dfs(path/\"java/final/jsonl\")\n",
    "\n",
    "sample = 0.01\n",
    "df_trn = df_trn.sample(frac = sample)\n",
    "df_val = df_val.sample(frac = sample)\n",
    "df_tst = df_tst.sample(frac = sample)\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "len(df_trn), len(df_val), len(df_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>this is a test</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>भारत test</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "             code\n",
       "0  this is a test\n",
       "1       भारत test"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(['this is a test', 'भारत test'], columns = ['code']);df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _isASCII(mthd: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given method contains only ASCII characters. From https://stackoverflow.com/a/27084708/5768407.\n",
    "\n",
    "    :param mthd: the method to verify contains only ASCII characters\n",
    "    :returns: returns a boolean representing whether or not the given method contains only ASCII characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mthd.encode(encoding = 'utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def remove_non_ascii(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all methods that contain non-ascii characters from a given pandas dataframe, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a new dataframe without methods that contain non-ascii characters\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df = df[df.code.apply(_isASCII)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ASCII_DF = pd.DataFrame(['this is a test'], columns = ['code'])\n",
    "df_non_ascii = remove_non_ascii(df_fake)\n",
    "\n",
    "assert (NON_ASCII_DF == df_non_ascii).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 ms, sys: 329 µs, total: 6.85 ms\nWall time: 6.52 ms\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%time df_trn = remove_non_ascii(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>public void setPipelines(java.util.Collection&lt;...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                                code\n",
       "0  public void setPipelines(java.util.Collection<..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\n",
    "    '''public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "        if (pipelines == null) {\n",
    "            this.pipelines = null;\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(pipelines);\n",
    "    }\n",
    "    '''\n",
    "], columns = ['code']); df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _beautify(mthd: str) -> str:\n",
    "    \"\"\"\n",
    "    Beautifies a given method using uncrustify with the sun.cfg style, i.e., Oracle's style.\n",
    "\n",
    "    :param mthd: the method to beautify\n",
    "    :returns: returns a beautified version of the given method\n",
    "    \"\"\"\n",
    "    # get path of icodegen\n",
    "    icodegen_path = Path(icodegen.__path__[0])\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path('/tmp')\n",
    "    tmp_path.mkdir(parents = True, exist_ok = True)\n",
    "    with open(tmp_path/'tmp.java', 'w') as f:\n",
    "        f.write(mthd)\n",
    "\n",
    "    try:\n",
    "        beaut_mthd = check_output([\n",
    "            icodegen_path/'uncrustify', '-c', icodegen_path/'sun.cfg',\n",
    "            '-f', tmp_path/'tmp.java'\n",
    "        ]).decode('utf-8')\n",
    "    except CalledProcessError as e:\n",
    "        # Exception thrown when the method is malformed, i.e, it is missing a curly brace\n",
    "        beaut_mthd = e.output.decode('utf-8')\n",
    "\n",
    "    return beaut_mthd\n",
    "\n",
    "def beautify_code(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beautify the methods in a pandas dataframe using uncrustify with the sun.cfg style, i.e., Oracle's style, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the methods beautified\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(_beautify)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUT_MTHD = '''public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "    if (pipelines == null) {\n",
    "\tthis.pipelines = null;\n",
    "\treturn;\n",
    "    }\n",
    "    this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(\n",
    "\tpipelines);\n",
    "}\n",
    "'''\n",
    "\n",
    "df_beaut = beautify_code(df_fake)\n",
    "\n",
    "assert BEAUT_MTHD == df_beaut.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 49.4 s, total: 52 s\nWall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# %time df_beaut = beautify_code(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void parseBeanMap(Object bean, SerializerJson deser)\n  {\n    Event event;\n\n    while ((event = next()) == Event.KEY_NAME) {\n      String fieldName = getString();\n\n      deser.readField(this, bean, fieldName);\n    }\n\n    if (event != Event.END_OBJECT) {\n      throw new JsonParsingException(L.l(\"Unexpected JSON {0} while parsing Java object {1}\",\n                                        event,\n                                        bean));\n    }\n  }\npublic void parseBeanMap(Object bean, SerializerJson deser) {\n    Event event;\n\n    while ((event = next()) == Event.KEY_NAME) {\n\tString fieldName = getString();\n\n\tdeser.readField(this, bean, fieldName);\n    }\n    if (event != Event.END_OBJECT) {\n\tthrow new JsonParsingException(L.l(\n\t    \"Unexpected JSON {0} while parsing Java object {1}\", event, bean));\n    }\n}\n\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "idx = 0\n",
    "print(df_trn.code.values[idx])\n",
    "print(df_beaut.code.values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\n",
    "    '\\n': '<n>'\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    'abstract': '<abstract>', 'assert': '<assert>', 'boolean': '<boolean>',\n",
    "    'break': '<break>', 'byte': '<byte>', 'case': '<case>',\n",
    "    'catch': '<catch>', 'char': '<char>', 'class': '<class>',\n",
    "    'const': '<const>', 'continue': '<continue>', 'default': '<default>',\n",
    "    'do': '<do>', 'double': '<double>', 'else': '<else>',\n",
    "    'enum': '<enum>', 'extends': '<extends>', 'final': '<final>',\n",
    "    'finally': '<finally>', 'float': '<float>', 'for': '<for>',\n",
    "    'goto': '<goto>', 'if': '<if>', 'implements': '<implements>',\n",
    "    'import': '<import>', 'instanceof': '<instanceof>', 'int': '<int>',\n",
    "    'interface': '<interface>', 'long': '<long>', 'native': '<native>',\n",
    "    'new': '<new>', 'package': '<package>', 'private': '<private>',\n",
    "    'protected': '<protected>', 'public': '<public>', 'return': '<return>',\n",
    "    'short': '<short>', 'static': '<static>', 'strictfp': '<strictfp>',\n",
    "    'super': '<super>', 'switch': '<switch>', 'synchronized': '<synchronized>',\n",
    "    'this': '<this>', 'throw': '<throw>', 'throws': '<throws>',\n",
    "    'transient': '<transient>', 'try': '<try>', 'void': '<void>',\n",
    "    'volatile': '<volatile>', 'while': '<while>'\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    '=': '<=>', '+': '<+>', '-': '<->',\n",
    "    '*': '<*>', '/': '</>', '%': '<%>',\n",
    "    '++': '<++>', '--': '<-->', '!': '<!>',\n",
    "    '==': '<==>', '!=': '<!=>', '>': '<greater>',\n",
    "    '>=': '<greater_equal>', '<': '<lesser>', '<=': '<lesser_equal>',\n",
    "    '&&': '<&&>', '||': '<||>', '?': '<?>',\n",
    "    ':': '<:>', '~': '<~>', '<<': '<double_lesser>',\n",
    "    '>>': '<double_greater>', '>>>': '<triple_greater>', '&': '<&>',\n",
    "    '^': '<^>', '|': '<|>'\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    '{': '<{>', '}': '<}>', '[': '<[>',\n",
    "    ']': '<]>', '<': '<lesser>', '>': '<greater>',\n",
    "    '(': '<(>', ')': '<)>', ';': '<;>'\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    '@': '<@>', '...': '<...>',\n",
    "    'null': '<null>', 'true': '<true>', 'false': '<false>'\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens, **java_operator_tokens, **java_structural_tokens,\n",
    "    **java_extra_tokens, **extra_tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&gt;&gt;&gt; &gt; + public ++ \\n\\n \\t \\t \\t\\t</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": [
       "                                  code\n",
       "0  >>> > + public ++ \\n\\n \\t \\t \\t\\t  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(['>>> > + public ++ \\n\\n \\t \\t \\t\\t  '], columns = ['code']); df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _replace_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for replacing all special tokens in a given method. This will replace longer special tokens first in order to not mistakenly breakup a special token that is part of a longer sequence. Adapted from https://stackoverflow.com/a/6117124/5768407 and https://stackoverflow.com/a/11753945/5768407\n",
    "\n",
    "    :param mthd: the method to have its special tokens replaced\n",
    "    :param spec_toks: a dictionary containing the special tokens to replace and the new tokens to replace them with\n",
    "    :returns: returns the method with its special tokens replaced\n",
    "    \"\"\"\n",
    "    # construct escaped versions of keys for running through regex\n",
    "    spec_toks = dict((re.escape(k), spec_toks[k]) for k in sorted(spec_toks, key=len, reverse=True))\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "\n",
    "    return mthd\n",
    "\n",
    "def replace_special_tokens(df: pd.DataFrame, spec_toks: Dict[str, str], n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all the special tokens in a pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to replace special tokens in\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the special tokens replaced\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _replace_toks(mthd, spec_toks))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACED_MTHD = '<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  '\n",
    "df_replaced = replace_special_tokens(df_fake, java_special_tokens)\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<@>VisibleForTesting<n>  URIBuilder <const>ra<int>s<(>Constra<int>s <const>ra<int>s<)> <{><n>    options.putAll<(><const>ra<int>s.toQueryMap<(><)><)><;><n>    <this>.isView <=> <!><const>ra<int>s.isUnbounded<(><)><;><n>    <return> <this><;><n>  <}>\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "df_replaced = replace_special_tokens(df_trn, java_special_tokens)\n",
    "print(df_replaced.code.values[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fake_data = '<triple_greater> <greater> <+> <public> <++> <n><n>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_tokenizer(df: pd.DataFrame, n: Optional[int] = None, vocab_sz: Optional[int] = 10_000, min_freq: Optional[int] = 2, output: Optional[Path] = None) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Train a ByteLevel BPE tokenizer on a given pandas dataframe. Code adapted from https://github.com/huggingface/tokenizers/tree/master/bindings/python.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the tokenizer train on\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :param vocab_sz: the maximum vocabulary size of the trained tokenizer. Defaulted was selected from: Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\n",
    "    :param min_freq: the minimum frequency a token has to occur to be considered\n",
    "    :returns: returns a trained ByteLevel BPE tokenizer\n",
    "    \"\"\"\n",
    "    if n is None: n = len(df)\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path('/tmp')\n",
    "    tmp_path.mkdir(parents = True, exist_ok = True)\n",
    "    with open(tmp_path/'tmp_tokenize.txt', 'w') as f:\n",
    "        f.write('\\n'.join(df.code.values[:n]))\n",
    "\n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space = True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets = True)\n",
    "\n",
    "    # train tokenizer with data in tmp file\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size = vocab_sz, min_frequency = min_freq,\n",
    "        special_tokens = list(java_special_tokens.values())\n",
    "    )\n",
    "    tokenizer.train(trainer, [str(tmp_path/'tmp_tokenize.txt')])\n",
    "\n",
    "    # save tokenizer if output path given\n",
    "    if output is not None:\n",
    "        tokenizer.save(output, pretty = True)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_SPEC = [\n",
    "    '<triple_greater>', 'Ġ', '<greater>', 'Ġ', '<+>', 'Ġ',\n",
    "    '<public>', 'Ġ', '<++>', 'Ġ', '<n>', '<n>'\n",
    "]\n",
    "tokenizer = train_tokenizer(df_fake)\n",
    "encoded = tokenizer.encode(fake_data)\n",
    "\n",
    "assert TOKENIZED_SPEC == encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<public> <void> parseBeanMap<(>Object bean, SerializerJson deser<)> <{><n>    Event event<;><n><n>    <while> <(><(>event <=> next<(><)><)> <==> Event.KEY_NAME<)> <{><n>\tString fieldName <=> getString<(><)><;><n><n>\tdeser.readField<(><this>, bean, fieldName<)><;><n>    <}><n>    <if> <(>event <!=> Event.END_OBJECT<)> <{><n>\t<throw> <new> JsonParsingException<(>L.l<(><n>\t    \"Unexpected JSON <{>0<}> <while> parsing Java object <{>1<}>\", event, bean<)><)><;><n>    <}><n><}><n>\n====================================================================================================\n['<public>', 'Ġ', '<void>', 'Ġparse', 'Bean', 'Map', '<(>', 'ĠObject', 'Ġbean', ',', 'ĠSerial', 'izer', 'Json', 'Ġdeser', '<)>', 'Ġ', '<{>', '<n>', 'ĠĠĠ', 'ĠEvent', 'Ġevent', '<;>', '<n>', '<n>', 'ĠĠĠĠ', '<while>', 'Ġ', '<(>', '<(>', 'Ġevent', 'Ġ', '<=>', 'Ġnext', '<(>', '<)>', '<)>', 'Ġ', '<==>', 'ĠEvent', '.', 'KEY', '_', 'NAME', '<)>', 'Ġ', '<{>', '<n>', 'Ġ', 'ĉ', 'String', 'ĠfieldName', 'Ġ', '<=>', 'ĠgetString', '<(>', '<)>', '<;>', '<n>', '<n>', 'Ġ', 'ĉ', 'des', 'er', '.', 'read', 'Field', '<(>', '<this>', 'Ġ,', 'Ġbean', ',', 'ĠfieldName', '<)>', '<;>', '<n>', 'ĠĠĠĠ', '<}>', '<n>', 'ĠĠĠĠ', '<if>', 'Ġ', '<(>', 'Ġevent', 'Ġ', '<!=>', 'ĠEvent', '.', 'END', '_', 'OBJECT', '<)>', 'Ġ', '<{>', '<n>', 'Ġĉ', '<throw>', 'Ġ', '<new>', 'ĠJson', 'ParsingException', '<(>', 'ĠL', '.', 'l', '<(>', '<n>', 'ĠĉĠĠĠ', 'Ġ\"', 'Unexpected', 'ĠJSON', 'Ġ', '<{>', 'Ġ0', '<}>', 'Ġ', '<while>', 'Ġparsing', 'ĠJava', 'Ġobject', 'Ġ', '<{>', 'Ġ1', '<}>', 'Ġ\",', 'Ġevent', ',', 'Ġbean', '<)>', '<)>', '<;>', '<n>', 'ĠĠĠĠ', '<}>', '<n>', '<}>', '<n>']\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# idx = 0\n",
    "# df_beaut = beautify_code(df_trn, n = 10)\n",
    "# df_replaced = replace_special_tokens(df_beaut, java_special_tokens)\n",
    "\n",
    "# tokenizer = train_tokenizer(df_trn)\n",
    "# encoded = tokenizer.encode(df_replaced.code.values[idx])\n",
    "# print(df_replaced.code.values[idx])\n",
    "# print('=' * 100)\n",
    "# print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icodegen",
   "language": "python",
   "name": "icodegen"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
