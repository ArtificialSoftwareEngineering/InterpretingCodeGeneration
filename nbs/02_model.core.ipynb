{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> API details. @nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Setup\n",
    "import pandas as pd\n",
    "\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer\n",
    "\n",
    "# from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Don't use this GPT2 TOeknizerFast API, it is not at all close to the\n",
    "# huggingface tokenizers lib API...\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\": \"<sos>\"})\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")\n",
    "# Tokenize the data\n",
    "# ds = convert_df_to_tfds(df_fake, tokenizer, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Model(ABC):\n",
    "    # TODO: Add generating the model config (but only the pieces we care about,\n",
    "    # i.e., the num layers, heads, dim size, emb size, etc) so that we can\n",
    "    # asily save it to a file for organizing evaluation results\n",
    "    # Also add loading from_path method to load models easily\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_probs(self, inputs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, method):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, ds, epochs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    # TODO: Add save method that handles saving model and tokenizer to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Tensorflow Huggingface Transformer\n",
    "class TransformerModel(Model):\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        return self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    def train(self, ds, epochs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RNNModel(Model):\n",
    "    _RNN_TYPE = {\n",
    "        \"rnn\": tf.keras.layers.SimpleRNN,\n",
    "        \"gru\": tf.keras.layers.GRU,\n",
    "        \"lstm\": tf.keras.layers.LSTM,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_layers,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        rnn_units,\n",
    "        batch_size,\n",
    "        out_path,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "        self.config_name = (\n",
    "            f\"{rnn_type}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    "        )\n",
    "        self.out_path = Path(out_path) / self.config_name\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        layer = RNNModel._RNN_TYPE[rnn_type]\n",
    "        rnn_layers = [\n",
    "            layer(\n",
    "                rnn_units,\n",
    "                return_sequences=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "                # following BigCode != Big Vocab Paper\n",
    "                dropout=0.1,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    mask_zero=True,  # Zero cannot be used in the vocabulary\n",
    "                    batch_input_shape=[batch_size, None],\n",
    "                ),\n",
    "            ]\n",
    "            + rnn_layers\n",
    "            + [\n",
    "                tf.keras.layers.Dense(vocab_size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        path = Path(path)\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "\n",
    "        model = RNNModel(\n",
    "            model_config[\"rnn_type\"],\n",
    "            model_config[\"n_layers\"],\n",
    "            model_config[\"vocab_size\"],\n",
    "            model_config[\"embedding_dim\"],\n",
    "            model_config[\"rnn_units\"],\n",
    "            1,\n",
    "            path,\n",
    "            tokenizer,\n",
    "        )\n",
    "        model.model = tf.keras.models.load_model(\n",
    "            str(path), custom_objects={\"_loss\": _loss}\n",
    "        )\n",
    "\n",
    "        return model\n",
    "        #         model.model.load_weights(tf.train.latest_checkpoint(path))\n",
    "        pass\n",
    "\n",
    "    def get_probs(self, method):\n",
    "        pass\n",
    "\n",
    "    def generate(self, n, temperature=1.0):\n",
    "        # Evaluation step (generating text using the learned model)\n",
    "\n",
    "        # Converting our start string to numbers (vectorizing)\n",
    "        input_eval = self.tokenizer.encode(\"<sos>\").ids\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "        # Empty string to store our results\n",
    "        text_generated = []\n",
    "\n",
    "        # Here batch size == 1\n",
    "        self.model.reset_states()\n",
    "        for i in range(n):\n",
    "            predictions = self.model(input_eval)\n",
    "            # remove the batch dimension\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            # using a categorical distribution to predict the character\n",
    "            # returned by the model\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[\n",
    "                -1, 0\n",
    "            ].numpy()\n",
    "\n",
    "            # Pass the predicted character as the next input to the model\n",
    "            # along with the previous hidden state\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "            text_generated.append(predicted_id)\n",
    "\n",
    "        return self.tokenizer.decode(text_generated, skip_special_tokens=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        self.model.save(str(self.out_path))\n",
    "        model_config = {\n",
    "            \"rnn_type\": self.rnn_type,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"rnn_units\": self.rnn_units,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as f:\n",
    "            json.dump(model_config, f)\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        return self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    # TODO add tensorboard call back for easy visualization\n",
    "    def train(self, dataset, epochs):\n",
    "        self.model.compile(optimizer=\"adam\", loss=_loss)\n",
    "        history = self.model.fit(dataset, epochs=epochs, callbacks=self.callbacks)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_COUNT = 13_015_378\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "assert PARAM_COUNT == gru.model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "chkpt_path = Path(out_path) / (\n",
    "    f\"{rnn_type}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    ")\n",
    "history = gru.train(ds, EPOCHS)\n",
    "\n",
    "assert chkpt_path.exists()\n",
    "assert EPOCHS == len(list(chkpt_path.glob(\"*.index\")))\n",
    "assert EPOCHS == len(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = 10\n",
    "text = gru.generate(NUM_TOKENS)\n",
    "\n",
    "assert NUM_TOKENS == len(tokenizer(text).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_json(\n",
    "    Path(\"/tmp\") / \"codesearchnet_java\" / \"train.jsonl\", orient=\"records\", lines=True\n",
    ")[:100]\n",
    "df_bpe = pd.read_json(\n",
    "    Path(\"/tmp\") / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    ")[:5_000]\n",
    "max_length = 100\n",
    "batch_size = 1\n",
    "tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "dataset = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 6.4802\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 5.5587\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 5.2675\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 4.8001\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 4.5838\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 4.3789\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 4.2329\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 4.0246\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 3.9389\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 3.6367\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 3.3243\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 3.0524\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 2.8538\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 2.7118\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 2.5870\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 2.3629\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 2.1745\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 2.0484\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.8839\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 1.7161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee9da28790>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    \"rnn\",\n",
    "    1,\n",
    "    tokenizer.get_vocab_size(),\n",
    "    256,\n",
    "    1024,\n",
    "    batch_size,\n",
    "    \"/tmp/models\",\n",
    "    tokenizer,\n",
    ")\n",
    "model.train(dataset, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<private><private><private><private><private> <void>DescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescriptionDescription\n"
     ]
    }
   ],
   "source": [
    "# Non-stateful\n",
    "NUM_TOKENS = 100\n",
    "text = model.generate(NUM_TOKENS, temperature=0.1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<public> <{><n>        <if> <(><)><;><n>        <}><pad>PRE the extractConnection <=> <(> scheduledConnection <{><n>        <if> <(> length<)> <{><n><n>        <}><pad> getIfc d.a<(><)><;><n>        <if> <(> \"<)><;><n>        <if> <(> \"<)><;><n>        <if> <(><!><(><)><;><n>        <}><pad><int> <void> setchedule <=> <(> \"<)><;><n>        <if> <(> length<)><;><n>        <if> \n"
     ]
    }
   ],
   "source": [
    "# Stateful\n",
    "NUM_TOKENS = 100\n",
    "text = model.generate(NUM_TOKENS, temperature=0.1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
