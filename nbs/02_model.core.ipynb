{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> API details. @nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Config, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "max_length = 16\n",
    "batch_size = 1\n",
    "vocab_sz = 100\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens, max_length, vocab_sz=vocab_sz)\n",
    "dataset = convert_df_to_tfds(df_fake, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Codesearchnet Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_df = pd.read_csv('/tf/main/dvc-icodegen/data/clean_java.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>code</th>\n",
       "      <th>code_len</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>cyclomatic_complexity</th>\n",
       "      <th>data_type</th>\n",
       "      <th>method_name</th>\n",
       "      <th>nloc</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>partition</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>protected final void fastPathOrderedEmit(U val...</td>\n",
       "      <td>134.0</td>\n",
       "      <td>['protected', 'final', 'void', 'fastPathOrdere...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>src</td>\n",
       "      <td>fastPathOrderedEmit</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>test</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@CheckReturnValue\\n    @NonNull\\n    @Schedule...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>['@', 'CheckReturnValue', '@', 'NonNull', '@',...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>src</td>\n",
       "      <td>amb</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@SuppressWarnings(\"unchecked\")\\n    @CheckRetu...</td>\n",
       "      <td>107.0</td>\n",
       "      <td>['@', 'SuppressWarnings', '(', '\"unchecked\"', ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>src</td>\n",
       "      <td>ambArray</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@SuppressWarnings({ \"unchecked\", \"rawtypes\" })...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>['@', 'SuppressWarnings', '(', '{', '\"unchecke...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>src</td>\n",
       "      <td>concat</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@SuppressWarnings({ \"unchecked\", \"rawtypes\" })...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>['@', 'SuppressWarnings', '(', '{', '\"unchecke...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>src</td>\n",
       "      <td>concat</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>test</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               code  code_len  \\\n",
       "0           0  protected final void fastPathOrderedEmit(U val...     134.0   \n",
       "1           1  @CheckReturnValue\\n    @NonNull\\n    @Schedule...      63.0   \n",
       "2           2  @SuppressWarnings(\"unchecked\")\\n    @CheckRetu...     107.0   \n",
       "3           3  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...      79.0   \n",
       "4           4  @SuppressWarnings({ \"unchecked\", \"rawtypes\" })...      91.0   \n",
       "\n",
       "                                         code_tokens  cyclomatic_complexity  \\\n",
       "0  ['protected', 'final', 'void', 'fastPathOrdere...                    7.0   \n",
       "1  ['@', 'CheckReturnValue', '@', 'NonNull', '@',...                    1.0   \n",
       "2  ['@', 'SuppressWarnings', '(', '\"unchecked\"', ...                    3.0   \n",
       "3  ['@', 'SuppressWarnings', '(', '{', '\"unchecke...                    1.0   \n",
       "4  ['@', 'SuppressWarnings', '(', '{', '\"unchecke...                    1.0   \n",
       "\n",
       "  data_type          method_name  nloc  parameter_count partition  token_count  \n",
       "0       src  fastPathOrderedEmit  20.0              3.0      test        131.0  \n",
       "1       src                  amb   4.0              1.0      test         43.0  \n",
       "2       src             ambArray  11.0              1.0      test         82.0  \n",
       "3       src               concat   4.0              1.0      test         50.0  \n",
       "4       src               concat   5.0              2.0      test         62.0  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>code_len</th>\n",
       "      <th>cyclomatic_complexity</th>\n",
       "      <th>nloc</th>\n",
       "      <th>parameter_count</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>496494.000000</td>\n",
       "      <td>496494.00000</td>\n",
       "      <td>496494.000000</td>\n",
       "      <td>496494.000000</td>\n",
       "      <td>496494.000000</td>\n",
       "      <td>496494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14574.923598</td>\n",
       "      <td>112.20115</td>\n",
       "      <td>3.784144</td>\n",
       "      <td>14.915854</td>\n",
       "      <td>1.633015</td>\n",
       "      <td>106.590708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8655.530474</td>\n",
       "      <td>227.68705</td>\n",
       "      <td>12.617511</td>\n",
       "      <td>31.642129</td>\n",
       "      <td>1.390480</td>\n",
       "      <td>221.178693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7041.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14346.000000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22046.000000</td>\n",
       "      <td>121.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29999.000000</td>\n",
       "      <td>68278.00000</td>\n",
       "      <td>4434.000000</td>\n",
       "      <td>11325.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>68274.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0      code_len  cyclomatic_complexity           nloc  \\\n",
       "count  496494.000000  496494.00000          496494.000000  496494.000000   \n",
       "mean    14574.923598     112.20115               3.784144      14.915854   \n",
       "std      8655.530474     227.68705              12.617511      31.642129   \n",
       "min         0.000000      20.00000               1.000000       2.000000   \n",
       "25%      7041.000000      42.00000               1.000000       6.000000   \n",
       "50%     14346.000000      66.00000               2.000000       9.000000   \n",
       "75%     22046.000000     121.00000               4.000000      16.000000   \n",
       "max     29999.000000   68278.00000            4434.000000   11325.000000   \n",
       "\n",
       "       parameter_count    token_count  \n",
       "count    496494.000000  496494.000000  \n",
       "mean          1.633015     106.590708  \n",
       "std           1.390480     221.178693  \n",
       "min           0.000000       7.000000  \n",
       "25%           1.000000      38.000000  \n",
       "50%           1.000000      62.000000  \n",
       "75%           2.000000     116.000000  \n",
       "max          54.000000   68274.000000  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496494, 11)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_train = java_df[java_df['partition'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454273, 11)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_trn_samples = java_train.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "max_length = 112\n",
    "batch_size = 16\n",
    "vocab_sz = 10000\n",
    "tokenizer = train_tokenizer(java_trn_samples, java_special_tokens, max_length, vocab_sz=vocab_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_trn_dataset = convert_df_to_tfds(java_trn_samples, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((16, 111), (16, 111)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_trn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Model(ABC):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_probs(self, inputs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, method):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, ds, epochs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace-based implementation, originally [this](!https://huggingface.co/transformers/model_doc/gpt2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TransformerHFModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_path,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        loss\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs a custom transformer model using a provided HF model\n",
    "        :param out_path: \n",
    "        :param model: HF transformer model\n",
    "        :tokenizer: HF tokenizer model\n",
    "        :optimizer: Keras optimizer\n",
    "        :loss: Custom loss function for model training\n",
    "        \"\"\"\n",
    "        \n",
    "        # HuggingFace model implementation is left outside this class in order to\n",
    "        # decouple from any specific implementation, besides aiming experimentation with multiple models\n",
    "        \n",
    "        self.timestamp = f'{model.name}-{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "        self.out_path = Path(out_path) / self.timestamp\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        \n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        tb_logs_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_path = tensorboard_path  / tb_logs_timestamp\n",
    "        \n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        super().__init__(tokenizer, model)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_path(model_path, hf_model, optimizer, loss):\n",
    "        path = Path(model_path)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        \n",
    "        # Load model\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "        full_path = path / \"model\"\n",
    "        loaded_model = hf_model.from_pretrained(full_path)\n",
    "        \n",
    "        model = TransformerHFModel(model_path, loaded_model, tokenizer, optimizer, loss)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def save(self):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        # Save HF model\n",
    "        self.model.save_pretrained(self.out_path / \"model\")\n",
    "        \n",
    "        # Save model config.\n",
    "        model_config = self.model.config\n",
    "        # TODO: extract attributes from configuration\n",
    "        config_dict = {\n",
    "          \"activation_function\": model_config.activation_function,\n",
    "          \"attn_pdrop\": model_config.attn_pdrop,\n",
    "          \"bos_token_id\": model_config.bos_token_id,\n",
    "          \"embd_pdrop\": model_config.embd_pdrop,\n",
    "          \"eos_token_id\": model_config.eos_token_id,\n",
    "          \"gradient_checkpointing\": model_config.gradient_checkpointing,\n",
    "          \"initializer_range\": model_config.initializer_range,\n",
    "          \"layer_norm_epsilon\": model_config.layer_norm_epsilon,\n",
    "          \"model_type\": model_config.model_type,\n",
    "          \"n_ctx\": model_config.n_ctx,\n",
    "          \"n_embd\": model_config.n_embd,\n",
    "          \"n_head\": model_config.n_head,\n",
    "          \"n_inner\": model_config.n_inner,\n",
    "          \"n_layer\": model_config.n_layer,\n",
    "          \"n_positions\": model_config.n_positions,\n",
    "          \"pad_token_id\": model_config.pad_token_id,\n",
    "          \"resid_pdrop\": model_config.resid_pdrop,\n",
    "          \"summary_activation\": model_config.summary_activation,\n",
    "          \"summary_first_dropout\": model_config.summary_first_dropout,\n",
    "          \"summary_proj_to_labels\": model_config.summary_proj_to_labels,\n",
    "          \"summary_type\": model_config.summary_type,\n",
    "          \"summary_use_proj\": model_config.summary_use_proj,\n",
    "          \"vocab_size\": model_config.vocab_size\n",
    "        }\n",
    "        \n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as file:\n",
    "            json.dump(config_dict, file)\n",
    "        \n",
    "    \n",
    "    def generate(self, max_length, n, top_k=10, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Performs generation process through sampling strategy (top-k) by default\n",
    "        :param max_length: Max. allowed length for generated sequences\n",
    "        :param n: Number of samples to generate\n",
    "        :param top_k: top-k most likely next words to be filtered for the sampling\n",
    "        :returns Generated sequences:\n",
    "        \"\"\"\n",
    "        \n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "        \n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=n\n",
    "        )\n",
    "        \n",
    "        generated = []\n",
    "        for i, sample in enumerate(gen_txt):\n",
    "            generated.append(self.tokenizer.decode(sample, skip_special_tokens=False))\n",
    "            \n",
    "        return generated\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        return probs    \n",
    "    \n",
    "    def sample_top_k(self, max_length, top_k=50):\n",
    "        \"\"\"\n",
    "        Performs sampling (generation) using top-k mechanism\n",
    "        :param max_length: Max. length allowed in the generation process\n",
    "        :param top_k: top-k most likely next words to be filtered for the sampling\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "        \n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "    \n",
    "    def sample_nucleus(self, max_length, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Perform sampling using top-p (nucleus) mechanism\n",
    "        :param  max_length: Max. allowed length of generated sequences \n",
    "        :param top_p: Probability value to find possible set of words whose cumulative probability exceeds top-p value\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "\n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_p=top_p,\n",
    "            top_k=0\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "    \n",
    "    def sample_temperature(self, max_length, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Performs sampling using temperature strategy\n",
    "        :param max_length: Max. allowed length of generated sequences \n",
    "        :param temperature: Temperature parameter of the softmax\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "\n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=0,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "        \n",
    "\n",
    "    def tokenize(self, method):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet (plain text)\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "        # return self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    def train(self, ds_train, ds_val, epochs):\n",
    "        \"\"\"\n",
    "        Performs training process leveraging keras methods.\n",
    "        :param ds_train: Tensorflow dataset for training\n",
    "        :param ds_val: Tensorflow dataset for validation\n",
    "        :param epochs: Number of epochs to execute training\n",
    "        \"\"\"\n",
    "        # Be careful with the definition of the loss function when compiling\n",
    "        # Gotta consider the number of layers (n_layer param.) --> actually number of decoder blocks\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[self.loss, *[None] * self.model.config.n_layer])\n",
    "        history = self.model.fit(ds_train, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val)\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test GPT2 - HuggingFace-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish configuration for HF model, in this case by means of GPT2Config class\n",
    "\n",
    "custom_config_gpt2 = GPT2Config(\n",
    "    vocab_size = tokenizer.get_vocab_size(),\n",
    "    bos_token_id = tokenizer.token_to_id(\"<sos>\"),\n",
    "    eos_token_id = tokenizer.token_to_id(\"<eos>\"),\n",
    "    pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "gpt2_hf_model_path = \"models/GPT2-HF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the corresponding HuggingFace model using the custom configuration\n",
    "\n",
    "hf_gpt_model = TFGPT2LMHeadModel(custom_config_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = TransformerHFModel(gpt2_hf_model_path, hf_gpt_model, tokenizer, optimizer, _loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6250/6250 [==============================] - 1210s 192ms/step - loss: 3.6715 - output_1_loss: 3.6715 - val_loss: 2.4627 - val_output_1_loss: 2.4627\n",
      "Epoch 2/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 2.4219 - output_1_loss: 2.4219 - val_loss: 1.9568 - val_output_1_loss: 1.9568\n",
      "Epoch 3/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.9869 - output_1_loss: 1.9869 - val_loss: 1.6407 - val_output_1_loss: 1.6407\n",
      "Epoch 4/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.7135 - output_1_loss: 1.7135 - val_loss: 1.4658 - val_output_1_loss: 1.4658\n",
      "Epoch 5/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.5522 - output_1_loss: 1.5522 - val_loss: 1.3510 - val_output_1_loss: 1.3510\n",
      "Epoch 6/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.4429 - output_1_loss: 1.4429 - val_loss: 1.2603 - val_output_1_loss: 1.2603\n",
      "Epoch 7/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.3578 - output_1_loss: 1.3578 - val_loss: 1.1825 - val_output_1_loss: 1.1825\n",
      "Epoch 8/20\n",
      "6250/6250 [==============================] - 1198s 192ms/step - loss: 1.2858 - output_1_loss: 1.2858 - val_loss: 1.1121 - val_output_1_loss: 1.1121\n",
      "Epoch 9/20\n",
      "6250/6250 [==============================] - 1198s 192ms/step - loss: 1.2214 - output_1_loss: 1.2214 - val_loss: 1.0478 - val_output_1_loss: 1.0478\n",
      "Epoch 10/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.1626 - output_1_loss: 1.1626 - val_loss: 0.9863 - val_output_1_loss: 0.9863\n",
      "Epoch 11/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 1.1079 - output_1_loss: 1.1079 - val_loss: 0.9270 - val_output_1_loss: 0.9270\n",
      "Epoch 12/20\n",
      "6250/6250 [==============================] - 1198s 192ms/step - loss: 1.0557 - output_1_loss: 1.0557 - val_loss: 0.8723 - val_output_1_loss: 0.8723\n",
      "Epoch 13/20\n",
      "6250/6250 [==============================] - 1198s 192ms/step - loss: 1.0064 - output_1_loss: 1.0064 - val_loss: 0.8188 - val_output_1_loss: 0.8188\n",
      "Epoch 14/20\n",
      "6250/6250 [==============================] - 1198s 192ms/step - loss: 0.9596 - output_1_loss: 0.9596 - val_loss: 0.7674 - val_output_1_loss: 0.7674\n",
      "Epoch 15/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.9150 - output_1_loss: 0.9150 - val_loss: 0.7197 - val_output_1_loss: 0.7197\n",
      "Epoch 16/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.8719 - output_1_loss: 0.8719 - val_loss: 0.6749 - val_output_1_loss: 0.6749\n",
      "Epoch 17/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.8314 - output_1_loss: 0.8314 - val_loss: 0.6311 - val_output_1_loss: 0.6311\n",
      "Epoch 18/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.7924 - output_1_loss: 0.7924 - val_loss: 0.5914 - val_output_1_loss: 0.5914\n",
      "Epoch 19/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.7554 - output_1_loss: 0.7554 - val_loss: 0.5522 - val_output_1_loss: 0.5522\n",
      "Epoch 20/20\n",
      "6250/6250 [==============================] - 1199s 192ms/step - loss: 0.7206 - output_1_loss: 0.7206 - val_loss: 0.5164 - val_output_1_loss: 0.5164\n",
      "Time elapsed training: 24017.52529978752 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_trn_time = time.time()\n",
    "gpt2_model.train(dataset, dataset, epochs=TRAIN_EPOCHS)\n",
    "end_trn_time = time.time()\n",
    "print(f'Time elapsed training: {end_trn_time - start_trn_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_GEN_SEQS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed generating: 10.738839387893677 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_gen_time = time.time()\n",
    "generated_seqs = gpt2_model.generate(n=NUMBER_GEN_SEQS, max_length=max_length)\n",
    "end_gen_time = time.time()\n",
    "print(f'Time elapsed generating: {end_gen_time - start_gen_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: <sos>public void marshall(DomainName domainName, ProtocolMarshaller protocolMarshaller) {\n",
      "\n",
      "        if (domainName == null) {\n",
      "            throw new SdkClientException(\"Invalid argument passed to marshall(...)\");\n",
      "        }\n",
      "\n",
      "        try {\n",
      "            protocolMarshaller.marshall(domainName.getDomainName(), DOMAINNAME_BINDING);\n",
      "            protocolMarshaller.marshall(domainName.getWebDomainName(), APPROMAINNAME_BINDING);\n",
      "        } catch (Exception e) {\n",
      "            throw new SdkClientException(\"Unable to marshall request to JSON: \" + e.\n",
      "sample: <sos>public void setAttribute(String key, Object attribute)\n",
      "    {\n",
      "        if (this.element != null)\n",
      "            this.element.attributes.put(key, attribute);\n",
      "    }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public List<CmsUser> getUsersOfUser(\n",
      "        CmsObject cms,\n",
      "        String ou,\n",
      "        boolean recursive) {\n",
      "\n",
      "        return m_UsersOfUser.get(m_user.getOrgUnitOfUser(cms, ou, recursive));\n",
      "    }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public int get(String name, String defaultValue) {\n",
      "\t\tString argument = get(name);\n",
      "\t\treturn argument == null\n",
      "\t\t\t? defaultValue\n",
      "\t\t\t: Integer.parseInt(argument);\n",
      "\t}<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public final static void setDefaultValueForDefault (@Nonnull final IDefaultWithLoader aFactoryBuilder,\n",
      "                                      @Nonnull finalDefaultValueWithDefaultValueFactory aDefaultValueFactory)\n",
      "  {\n",
      "    setDefaultValueWithDefaultValueFactory (aFactoryBuilder, aDefaultValueFactory);\n",
      "  }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public void setClearTime(String v) {\n",
      "    if (CouchbaseMicros.Type.featOkTst && ((CouchbaseMicros.Type)jcasType).casFeat_casType != null)\n",
      "      jcasType.jcas.throwFeatMissing(\"casType\", \"de.julielab.jules.types.CouchbaseMicros.Micros.types.CouchbaseMicros.CouchbaseMicros.types.CouchbaseMicros.CouchbaseM\n",
      "sample: <sos>private static String getSimpleClassName(TypeElement type) {\n",
      "    ClassDoc[] declaredClasses = clazz.getDeclaredClasses();\n",
      "    for (ClassDoc cd : declaredClasses) {\n",
      "      if (cd.equals(type)) {\n",
      "        return cd.name();\n",
      "      }\n",
      "    }\n",
      "    return null;\n",
      "  }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public Object invokeStaticMethod() {\n",
      "\n",
      "\t\tClass<?> clazz = null;\n",
      "\t\ttry {\n",
      "\t\t\tclazz = Class.forName(className);\n",
      "\t\t} catch (ClassNotFoundException ignore) {\n",
      "\t\t}\n",
      "\n",
      "\t\treturn null;\n",
      "\t}<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "sample: <sos>public static String getRandomString(String s, int maxLength) {\n",
      "        try {\n",
      "            MessageDigest md = MessageDigest.getInstance(SHA1_SIZE);\n",
      "\n",
      "            int length = md.digest(s, 0, maxLength);\n",
      "\n",
      "            byte[] bytes = new byte[length];\n",
      "\n",
      "            for (int i = 0; i < length; i++) {\n",
      "                bytes[i] = Integer.parseInt(md.digest(s, 0, maxLength);\n",
      "            }\n",
      "\n",
      "            return String.valueOf\n",
      "sample: <sos>@Override\n",
      "    public String getFormattedMessage() {\n",
      "\n",
      "        // Get the value for the specified message with the specified message\n",
      "        String formattedMessage;\n",
      "\n",
      "        formattedMessage = super.getMessage();\n",
      "\n",
      "        // Get the message with the expected message\n",
      "        msg = super.getMessage();\n",
      "        msg = super.getMessage();\n",
      "\n",
      "        // Return the message from the message\n",
      "        msg = formatReply(null);\n",
      "        return message;\n",
      "    }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for seq in generated_seqs:\n",
    "    print(f'sample: {seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at models/GPT2-HF/tfgp_t2lm_head_model_4-20210330-171318/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_gpt2_model = TransformerHFModel.from_path(\"models/GPT2-HF/tfgp_t2lm_head_model_4-20210330-171318\", TFGPT2LMHeadModel, optimizer, _loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TransformerHFModel"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>aabbaa(aaaa(b)<assert>()baa', '<sos>aa(aa(()aabdbbcbc)', '<sos>aaaab<assert>c(bbbcccc))', '<sos>aaaaaa(bbc(cc(()((', '<sos>aaaa(aaccaa(c)caac))', '<sos>aa(aa(b)bb(aac)c)<static>', '<sos>aabcbbaabaa(((()))', '<sos>aa(b(aaaa(b(bbc(b(', '<sos>aaaac(aabbc(d(aa))(', '<sos>aaaa()(cbbbc(aac(b']\n"
     ]
    }
   ],
   "source": [
    "generated_seqs = gpt2_model.generate(n=10, max_length=max_length)\n",
    "print(generated_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"test\"\n",
    "encoded_text = tokenizer.encode(\"<sos>\" + text).ids\n",
    "input_eval = tf.expand_dims(encoded_text, 0)\n",
    "logits = gpt2_model.model(input_eval)[0].numpy()\n",
    "inputs = gpt2_model.tokenize(text)\n",
    "probs = gpt2_model.get_probs(inputs)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(probs)):\n",
    "#     assert np.isclose(1.0, probs[i].sum())\n",
    "#     assert np.argmax(logits[i]) == np.argmax(probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>aaaaaa(</>aa))b(((<||>ccaacb('"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model.sample_nucleus(max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>aabaaaa((b()ccb))((c(('"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model.sample_top_k(max_length=20, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras-based implementation (based on [this example](!https://keras.io/examples/generative/text_generation_with_miniature_gpt/#prepare-the-data-for-wordlevel-language-modelling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliar classes / methods for building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    \n",
    "# export    class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e43a2b15fdf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMiniatureGPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     def __init__(\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "class MiniatureGPTModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_path, \n",
    "        max_length,\n",
    "        batch_size,\n",
    "        ff_dim,\n",
    "        embedding_dim,\n",
    "        n_heads,\n",
    "        n_transformer_blocks,\n",
    "        vocab_size,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        loss\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds a transformer model using tf-keras API\n",
    "        :param out_path: Path for saving model data\n",
    "        :param max_length: Max. length allowed for the sequences\n",
    "        :param batch_size: \n",
    "        :param ff_dim:\n",
    "        :param embedding_dim:\n",
    "        :param n_heads:\n",
    "        :param n_transformer_blocks:\n",
    "        :param vocab_size:\n",
    "        :param tokenizer:\n",
    "        :param optimizer:\n",
    "        :param loss:\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_transformer_blocks = n_transformer_blocks\n",
    "        self.vocab_size = vocab_size\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        model = self.__create_model()\n",
    "        \n",
    "        model._name = \"Miniature-GPT\"\n",
    "        \n",
    "        name = f'{model._name}-{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "        self.out_path = Path(out_path)/name\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = tensorboard_path / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "     \n",
    "    def __create_model(self):\n",
    "        inputs = layers.Input(shape=(self.max_length-1,), dtype=tf.int32)\n",
    "        embedding_layer = TokenAndPositionEmbedding(self.max_length-1, self.vocab_size, self.embedding_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        transformerLayers = []\n",
    "        outputs_x = []\n",
    "        for i in range(self.n_transformer_blocks):\n",
    "            transformerLayers.append(TransformerBlock(self.embedding_dim, self.n_heads, self.ff_dim))\n",
    "\n",
    "        for i in range(self.n_transformer_blocks):\n",
    "            x = transformerLayers[i](x)\n",
    "            outputs_x.append(x)\n",
    "        # transformer_block = TransformerBlock(self.embedding_dim, self.n_heads, self.ff_dim)\n",
    "        # x = transformer_block(x)\n",
    "\n",
    "        outputs = layers.Dense(self.vocab_size)(outputs_x[self.n_transformer_blocks-1])\n",
    "        # outputs = layers.Dense(self.vocab_size)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=[outputs, *outputs_x])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_path(path, optimizer, loss):\n",
    "        path = Path(path)\n",
    "        # Load tokenizer\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        # Load model\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "            \n",
    "        min_model = MiniatureGPTModel(\n",
    "            path,\n",
    "            **model_config,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizer=optimizer, \n",
    "            loss=loss\n",
    "        )\n",
    "        \n",
    "        loaded_model = tf.keras.models.load_model(str(path), custom_objects={\"_loss\": _loss})\n",
    "        min_model.model = loaded_model\n",
    "        \n",
    "        return min_model\n",
    "        \n",
    "    def save(self):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        # Save model\n",
    "        self.model.save(str(self.out_path))\n",
    "        # Save config.\n",
    "        model_config = {\n",
    "            \"max_length\": self.max_length,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"n_transformer_blocks\": self.n_transformer_blocks,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as file:\n",
    "            json.dump(model_config, file)\n",
    "        \n",
    "    def tokenize(self, method):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def generate(self, n, top_k=10):\n",
    "        \"\"\"Performs generation process through top-k sampling process\"\"\"\n",
    "         # Converting our start string to numbers (vectorizing)\n",
    "        start_tokens = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        print(start_tokens)\n",
    "        generated_txt = []\n",
    "        \n",
    "        while len(generated_txt) < n:\n",
    "            gen_sample = self.__sample_top_k(start_tokens, top_k)\n",
    "            generated_txt.append(gen_sample)\n",
    "            \n",
    "        # TODO: Tokenize\n",
    "        return generated_txt\n",
    "        \n",
    "    def __sample_top_k(self, start_tokens, top_k=10):\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = [] \n",
    "        gen_sequence = start_tokens[:]\n",
    "        while len(gen_sequence) <= self.max_length-1:\n",
    "            pad_len = (self.max_length-1) - len(gen_sequence)\n",
    "            sample_index = len(gen_sequence) - 1\n",
    "            if pad_len < 0:\n",
    "                x = gen_sequence[:self.max_length-1]\n",
    "                sample_index = self.max_length - 1\n",
    "            elif pad_len > 0:\n",
    "                x = gen_sequence + [0] * pad_len\n",
    "            else:\n",
    "                x = gen_sequence[:]\n",
    "            sample_index = len(gen_sequence)-1\n",
    "            x = np.array([x])\n",
    "            prediction_output = self.model.predict(x)\n",
    "            y = prediction_output[0]\n",
    "            sample_token = self.__sample_from(y[0][sample_index], top_k)\n",
    "            tokens_generated.append(sample_token)\n",
    "            gen_sequence.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            \n",
    "        return tokens_generated\n",
    "            \n",
    "    def __sample_from(self, logits, top_k):\n",
    "        logits, indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         input_eval = tf.expand_dims(ids, 0)\n",
    "\n",
    "        logits = self.model(inputs[\"input_ids\"])\n",
    "        probs = tf.nn.softmax(logits)  # [0].numpy()\n",
    "\n",
    "        return probs\n",
    "        \n",
    "    def train(self, ds_train, ds_val, epochs):\n",
    "        \"\"\"\n",
    "        Performs training process leveraging keras methods\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[self.loss, *[None]* self.n_transformer_blocks])\n",
    "        history = self.model.fit(ds_train, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val)\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Miniature GPT. Keras-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "# \n",
    "min_gpt_config_params = {\n",
    "    \"out_path\": \"models/Min-GPT\",\n",
    "    \"max_length\" : max_length,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"ff_dim\": 64,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_transformer_blocks\": 1,\n",
    "    \"vocab_size\": tokenizer.get_vocab_size()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss definition\n",
    "\n",
    "min_gpt_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "min_gpt_loss = _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gpt_config_params[\"optimizer\"] = min_gpt_optimizer\n",
    "min_gpt_config_params[\"loss\"] = min_gpt_loss\n",
    "min_gpt_config_params[\"tokenizer\"] = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gpt_model = MiniatureGPTModel(**min_gpt_config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6250/6250 [==============================] - 100s 16ms/step - loss: 6.0205 - dense_103_loss: 6.0205 - val_loss: 3.6836 - val_dense_103_loss: 3.6836\n",
      "Epoch 2/20\n",
      "6250/6250 [==============================] - 97s 16ms/step - loss: 3.5820 - dense_103_loss: 3.5820 - val_loss: 3.3297 - val_dense_103_loss: 3.3297\n",
      "Epoch 3/20\n",
      "6250/6250 [==============================] - 98s 16ms/step - loss: 3.2802 - dense_103_loss: 3.2802 - val_loss: 3.1048 - val_dense_103_loss: 3.1048\n",
      "Epoch 4/20\n",
      "6250/6250 [==============================] - 97s 16ms/step - loss: 3.0777 - dense_103_loss: 3.0777 - val_loss: 2.9419 - val_dense_103_loss: 2.9419\n",
      "Epoch 5/20\n",
      "6250/6250 [==============================] - 98s 16ms/step - loss: 2.9302 - dense_103_loss: 2.9302 - val_loss: 2.8194 - val_dense_103_loss: 2.8194\n",
      "Epoch 6/20\n",
      "6250/6250 [==============================] - 97s 16ms/step - loss: 2.8198 - dense_103_loss: 2.8198 - val_loss: 2.7255 - val_dense_103_loss: 2.7255\n",
      "Epoch 7/20\n",
      "6250/6250 [==============================] - 98s 16ms/step - loss: 2.7343 - dense_103_loss: 2.7343 - val_loss: 2.6501 - val_dense_103_loss: 2.6501\n",
      "Epoch 8/20\n",
      "6250/6250 [==============================] - 98s 16ms/step - loss: 2.6647 - dense_103_loss: 2.6647 - val_loss: 2.5867 - val_dense_103_loss: 2.5867\n",
      "Epoch 9/20\n",
      "6250/6250 [==============================] - 97s 16ms/step - loss: 2.6061 - dense_103_loss: 2.6061 - val_loss: 2.5327 - val_dense_103_loss: 2.5327\n",
      "Epoch 10/20\n",
      "6250/6250 [==============================] - 104s 17ms/step - loss: 2.5564 - dense_103_loss: 2.5564 - val_loss: 2.4861 - val_dense_103_loss: 2.4861\n",
      "Epoch 11/20\n",
      "6250/6250 [==============================] - 111s 18ms/step - loss: 2.5128 - dense_103_loss: 2.5128 - val_loss: 2.4447 - val_dense_103_loss: 2.4447\n",
      "Epoch 12/20\n",
      "6250/6250 [==============================] - 111s 18ms/step - loss: 2.4742 - dense_103_loss: 2.4742 - val_loss: 2.4073 - val_dense_103_loss: 2.4073\n",
      "Epoch 13/20\n",
      "6250/6250 [==============================] - 103s 16ms/step - loss: 2.4393 - dense_103_loss: 2.4393 - val_loss: 2.3729 - val_dense_103_loss: 2.3729\n",
      "Epoch 14/20\n",
      "6250/6250 [==============================] - 102s 16ms/step - loss: 2.4068 - dense_103_loss: 2.4068 - val_loss: 2.3409 - val_dense_103_loss: 2.3409\n",
      "Epoch 15/20\n",
      "6250/6250 [==============================] - 115s 18ms/step - loss: 2.3769 - dense_103_loss: 2.3769 - val_loss: 2.3110 - val_dense_103_loss: 2.3110\n",
      "Epoch 16/20\n",
      "6250/6250 [==============================] - 113s 18ms/step - loss: 2.3488 - dense_103_loss: 2.3488 - val_loss: 2.2827 - val_dense_103_loss: 2.2827\n",
      "Epoch 17/20\n",
      "6250/6250 [==============================] - 112s 18ms/step - loss: 2.3222 - dense_103_loss: 2.3222 - val_loss: 2.2560 - val_dense_103_loss: 2.2560\n",
      "Epoch 18/20\n",
      "6250/6250 [==============================] - 106s 17ms/step - loss: 2.2971 - dense_103_loss: 2.2971 - val_loss: 2.2307 - val_dense_103_loss: 2.2307\n",
      "Epoch 19/20\n",
      "6250/6250 [==============================] - 109s 17ms/step - loss: 2.2733 - dense_103_loss: 2.2733 - val_loss: 2.2068 - val_dense_103_loss: 2.2068\n",
      "Epoch 20/20\n",
      "6250/6250 [==============================] - 112s 18ms/step - loss: 2.2507 - dense_103_loss: 2.2507 - val_loss: 2.1842 - val_dense_103_loss: 2.1842\n",
      "Time elapsed: 2081.474970817566 seconds\n"
     ]
    }
   ],
   "source": [
    "start_trn_min_time = time.time()\n",
    "min_gpt_history = min_gpt_model.train(dataset, dataset, epochs=TRAIN_EPOCHS)\n",
    "end_trn_min_time = time.time()\n",
    "print(f'Time elapsed: {end_trn_min_time - start_trn_min_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((1, 15), (1, 15)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_gpt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91cba6871452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_gpt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'min_gpt_model' is not defined"
     ]
    }
   ],
   "source": [
    "min_gpt_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "gen_sequences = min_gpt_model.generate(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "seq: private int getMaxSize() {\n",
      "        long totalSize = Math.minTime;\n",
      "        int numRows = -1];\n",
      "        for (int i = 0; i < maxNumsize; i++) {\n",
      "            if (i > 1) {\n",
      "                if (currentCapacity > MAX_VALUE) {\n",
      "                    currentTime = currentTime;\n",
      "                } else if (lastSize <= 0) {\n",
      "                    m_index = new IndexOutOfBoundsException();\n",
      "                    if (size() != - 1) {\n",
      "                        return - 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: @Override\n",
      "\tpublic void set_PUT_First(long value)\n",
      "\t\tthrows ServletException, IOException\n",
      "\t{\n",
      "\t\tif (value == value.value_value = new Long();\n",
      "\t\tif (value == null) {\n",
      "\t\t\treturn value;\n",
      "\t\t}\n",
      "\t}<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: public int get(long index) {\r\n",
      "\t\tif (index >= 1) {\r\n",
      "\t\t\treturn this.union(index - 1, 1, 0);\r\n",
      "\t\tthis.out += Math.min(index);\r\n",
      "\t\t} else {\r\n",
      "\t\t\tif (index >= 0x > -1) {\r\n",
      "\t\t\tthrow new IllegalArgumentException(\r\n",
      "\t\t\t\tthrow new IllegalArgumentException(e);\r\n",
      "\t\t\t}\r\n",
      "\t\t}\r\n",
      "\t\treturn new Couch(x, (int) 0, 0, 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: private final int getOffset() {\n",
      "    if (offset > 0) {\n",
      "      return -1;\n",
      "      }\n",
      "    return getLength(pos++m_index, index, false);\n",
      "\n",
      "    if (index == null) {\n",
      "      if (index == 0) {\n",
      "        return true;\n",
      "      } else if (!m_index > 0) {\n",
      "        } else {\n",
      "      throw new IllegalArgumentException(\"The index is not found for empty index : \" + this.length - 1 + \" + this.getName\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: public void marshall(PutItemRequest setItemRequest, ProtocolMarshaller protocolMarshaller) {\n",
      "\n",
      "        if (m_BINDING);\n",
      "        throw new SdkClientException(\"Invalid argument passed to marshall(...)\");\n",
      "\n",
      "        try {\n",
      "            protocolMarshaller.marshall(updateItemRequest.getApiId(), DESCRIPTION_BINDING);\n",
      "            protocolMarshaller.marshall(updateItemRequest.getArn(), NAME_BINDING);\n",
      "            protocolMarshaller.marshall(describeItemRequest.getMessage(), NEXTTOKEN_BINDING);\n",
      "            protocolMarshaller.marshall(getUpdateEventAttributesRequest.getNextToken(), NEXTTOKEN\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: @Override\n",
      "    public boolean contains(Object a,\n",
      "    int n) {\n",
      "      return false;\n",
      "  }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: public static List<CommerceShippingFixedOptionRel> findByCommerceShippingFixedOption(\n",
      "\t\tlong commerceShippingFixedOptionCategoryId,\n",
      "\t\tOrderByComparator<CommerceShippingFixedOption> orderByComparator) {\n",
      "\t\treturn getPersistence()\n",
      "\t\t\t\t   .filter(commerceShippingFixedOptionRelId.commerceShippingFixedOptionId.get(groupId, primaryKey);\n",
      "\t\t}<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: public void setDefaultView(@NonNull ViewView view, View view) {\n",
      "\n",
      "\t\tfinal View viewView = newView(View) -> {\n",
      "\t\t\tif (contextViewView != mViewView.mViewView.setViewView(Layout.ViewLayout.ViewViewViewView(View.GUI_ViewWidthViewViewViewViewViewViewViewLayoutView viewView viewViewViewViewViewViewLayoutViewViewView view) {\t\tView viewView = view.getColorViewStyleViewViewViewViewViewViewView\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: public void marshall(Group groupByUuid, ProtocolMarshaller protocolMarshaller) {\n",
      "        if (groupIds == null) {\n",
      "            throw new SdkClientException(\"Invalid argument passed to marshall(...)\");\n",
      "        }\n",
      "\n",
      "        try {\n",
      "            protocolMarshaller.marshall(deleteGroupRequest.getScheme(), SUBJECTID_BINDING);\n",
      "            protocolMarshaller.marshall(updateGroupRequest.getNextToken(getArn(), DESCRIPTION_BINDING);\n",
      "        } catch (Exception e) {\n",
      "            throw new SdkClientException(\"Unable to marshall request to marshall request to JSON: e);\n",
      "----------------------------------------------------------------------------------------------------\n",
      "seq: private static final PrivateKey getPrivateKeyForUser(\n",
      "            final String name) {\n",
      "        return Optional.ofNullable(new Func1<String, Observable<CertificateMetadataInner>, Operation<String>> listPasswordAsync(final String subscriptionName) {\n",
      "            final Integer keyValue =\n",
      "                ? (String) : key;\n",
      "            final String keyName = \" + key + key + \"/\" + key + \"Name + \" + value + value );\n",
      "            return value;\n",
      "        }\n",
      "    }<pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for seq in gen_sequences:\n",
    "    print('-'*100)\n",
    "    print(f'seq: {tokenizer.decode(seq, skip_special_tokens=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_58_layer_call_fn, embedding_58_layer_call_and_return_conditional_losses, embedding_59_layer_call_fn, embedding_59_layer_call_and_return_conditional_losses, multi_head_attention_29_layer_call_fn while saving (showing 5 of 65). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as embedding_58_layer_call_fn, embedding_58_layer_call_and_return_conditional_losses, embedding_59_layer_call_fn, embedding_59_layer_call_and_return_conditional_losses, multi_head_attention_29_layer_call_fn while saving (showing 5 of 65). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Min-GPT/Miniature-GPT-20210330-200608/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Min-GPT/Miniature-GPT-20210330-200608/assets\n"
     ]
    }
   ],
   "source": [
    "min_gpt_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_min_gpt = MiniatureGPTModel.from_path(\"models/Min-GPT/Miniature-GPT-20210330-200608\", optimizer, min_gpt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MiniatureGPTModel"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_min_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RNNModel(Model):\n",
    "    _RNN_TYPE = {\n",
    "        \"rnn\": tf.keras.layers.SimpleRNN,\n",
    "        \"gru\": tf.keras.layers.GRU,\n",
    "        \"lstm\": tf.keras.layers.LSTM,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_layers,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        rnn_units,\n",
    "        batch_size,\n",
    "        out_path,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "        self.config_name = f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    "        self.out_path = Path(out_path) / self.config_name\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        layer = RNNModel._RNN_TYPE[rnn_type]\n",
    "        rnn_layers = [\n",
    "            layer(\n",
    "                rnn_units,\n",
    "                return_sequences=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "                # following BigCode != Big Vocab Paper\n",
    "                dropout=0.5,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    mask_zero=True,  # Zero cannot be used in the vocabulary\n",
    "                ),\n",
    "            ]\n",
    "            + rnn_layers\n",
    "            + [\n",
    "                tf.keras.layers.Dense(vocab_size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        path = Path(path)\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "\n",
    "        model = RNNModel(\n",
    "            model_config[\"rnn_type\"],\n",
    "            model_config[\"n_layers\"],\n",
    "            model_config[\"vocab_size\"],\n",
    "            model_config[\"embedding_dim\"],\n",
    "            model_config[\"rnn_units\"],\n",
    "            1,\n",
    "            path,\n",
    "            tokenizer,\n",
    "        )\n",
    "        model.model = tf.keras.models.load_model(\n",
    "            str(path), custom_objects={\"_loss\": _loss}\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         input_eval = tf.expand_dims(ids, 0)\n",
    "\n",
    "        logits = self.model(inputs[\"input_ids\"])\n",
    "        probs = tf.nn.softmax(logits)  # [0].numpy()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def generate(self, n, temperature=1.0):\n",
    "        # Converting our start string to numbers (vectorizing)\n",
    "        text_generated = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        # Here batch size == 1\n",
    "        self.model.reset_states()\n",
    "        for i in range(n):\n",
    "            predictions = self.model(input_eval)\n",
    "            # remove the batch dimension\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            # using a categorical distribution to predict the character\n",
    "            # returned by the model\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[\n",
    "                -1, 0\n",
    "            ].numpy()\n",
    "\n",
    "            text_generated.append(predicted_id)\n",
    "            # Pass the predicted character as the next input to the model\n",
    "            # along with the previous hidden state\n",
    "            input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        return self.tokenizer.decode(text_generated, skip_special_tokens=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        self.model.save(str(self.out_path))\n",
    "        model_config = {\n",
    "            \"rnn_type\": self.rnn_type,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"rnn_units\": self.rnn_units,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as f:\n",
    "            json.dump(model_config, f)\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         inputs = tf.expand_dims(ids, 0)\n",
    "        output = {}\n",
    "        # encod method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "        return output  # self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    # TODO add tensorboard call back for easy visualization\n",
    "    def train(self, ds_trn, ds_val, epochs):\n",
    "        self.model.compile(optimizer=\"adam\", loss=_loss)\n",
    "        history = self.model.fit(\n",
    "            ds_trn, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val\n",
    "        )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_COUNT = 124_772\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "assert PARAM_COUNT == gru.model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_configuration = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = TransformerModel('gpt2_model', , tokenizer, optimizer, _loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_history = gpt2_model.train(dataset, dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model.generate(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 2s/step - loss: 4.6063 - val_loss: 4.5652\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "chkpt_path = Path(out_path) / (\n",
    "    f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    ")\n",
    "history = gru.train(dataset, dataset, EPOCHS)\n",
    "\n",
    "assert chkpt_path.exists()\n",
    "assert EPOCHS == len(list(chkpt_path.glob(\"*.index\")))\n",
    "assert EPOCHS == len(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do I test this?\n",
    "text = \"test\"\n",
    "text_generated = tokenizer.encode(\"<sos>\" + text).ids\n",
    "input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "logits = gru.model(input_eval)[0].numpy()\n",
    "inputs = gru.tokenize(text)\n",
    "probs = gru.get_probs(inputs)[0].numpy()\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    assert np.isclose(1.0, probs[i].sum())\n",
    "    assert np.argmax(logits[i]) == np.argmax(probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test case for earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this test will break sometimes due to encoding adding space prefixes to some\n",
    "# # tokens when add_prefix_space=False it should be fine :/\n",
    "# NUM_TOKENS = 10\n",
    "# text = gru.generate(NUM_TOKENS)\n",
    "# tokenizer.no_padding()\n",
    "# ids = tokenizer.encode(text).ids\n",
    "# tokenizer.enable_padding(length=max_length)\n",
    "\n",
    "# # -1 for the <sos> token that's always prepended\n",
    "# assert NUM_TOKENS == len(ids) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/gru_layers1_vocab100_embed128_units128/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/gru_layers1_vocab100_embed128_units128/assets\n"
     ]
    }
   ],
   "source": [
    "gru.save()\n",
    "\n",
    "assert (gru.out_path / \"assets\").exists()\n",
    "assert (gru.out_path / \"model_config.json\").exists()\n",
    "assert (gru.out_path / \"saved_model.pb\").exists()\n",
    "assert (gru.out_path / \"tokenizer.json\").exists()\n",
    "assert (gru.out_path / \"variables\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 4.5705\n",
      "2/2 - 1s - loss: 4.5705\n"
     ]
    }
   ],
   "source": [
    "loaded_gru = RNNModel.from_path(str(gru.out_path))\n",
    "\n",
    "assert gru.tokenizer.get_vocab() == loaded_gru.tokenizer.get_vocab()\n",
    "assert gru.model.count_params() == loaded_gru.model.count_params()\n",
    "assert gru.model.evaluate(dataset, verbose=2) == loaded_gru.model.evaluate(\n",
    "    dataset, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Experiment 0.0.0\n",
    "VANILLA_CONFIG = {\n",
    "    \"rnn_type\": \"rnn\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.0.0\n",
    "GRU_CONFIG_1 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.0\n",
    "GRU_CONFIG_2 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 2,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.1\n",
    "GRU_CONFIG_3 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 3,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.0\n",
    "GRU_CONFIG_4 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 512,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.1\n",
    "GRU_CONFIG_5 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 2_048,\n",
    "}\n",
    "\n",
    "_RNN_CONFIGs = [\n",
    "    VANILLA_CONFIG,\n",
    "    GRU_CONFIG_1,\n",
    "    GRU_CONFIG_2,\n",
    "    GRU_CONFIG_3,\n",
    "    GRU_CONFIG_4,\n",
    "    GRU_CONFIG_5,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train(data_path, out_path, epochs=64, max_length=300, batch_size=64, n=None):\n",
    "    \"\"\"Function for training models related to the library.\"\"\"\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Load in the datasets\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    # Check if the path where the tokenizer is to be saved is not empty\n",
    "    # if it is not empty then just load the tokenizer there.\n",
    "    if (out_path / \"tokenizer.json\").exists():\n",
    "        logging.info(f\"Loading tokenizer from {str(out_path / 'tokenizer.json')}.\")\n",
    "        tokenizer = Tokenizer.from_file(str(out_path / \"tokenizer.json\"))\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}.\"\n",
    "        )\n",
    "        df_bpe = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    "        )[:n]\n",
    "        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "        tokenizer.save(str(out_path / \"tokenizer.json\"), pretty=True)\n",
    "        del df_bpe\n",
    "\n",
    "    # Tokenize the dataset and convert it to tfds.\n",
    "    tfds_trn_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_trn_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_trn_path.exists():\n",
    "        ds_trn = tf.data.experimental.load(\n",
    "            str(tfds_trn_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_trn = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)\n",
    "        tfds_trn_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_trn, str(tfds_trn_path))\n",
    "        del df_trn\n",
    "\n",
    "    tfds_val_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_val_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_val_path.exists():\n",
    "        ds_val = tf.data.experimental.load(\n",
    "            str(tfds_val_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_val = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"valid.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)\n",
    "        tfds_val_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_val, str(tfds_val_path))\n",
    "        del df_val\n",
    "\n",
    "    logging.info(\"Starting the training of all RNN based models.\")\n",
    "    # Train RNN based models\n",
    "    for config in _RNN_CONFIGs:\n",
    "        rnn_model = RNNModel(\n",
    "            config[\"rnn_type\"],\n",
    "            config[\"n_layers\"],\n",
    "            tokenizer.get_vocab_size(),\n",
    "            config[\"embedding_dim\"],\n",
    "            config[\"rnn_units\"],\n",
    "            batch_size,\n",
    "            str(out_path),\n",
    "            tokenizer,\n",
    "        )\n",
    "        rnn_model.train(ds_trn, ds_val, epochs)\n",
    "        rnn_model.save()\n",
    "\n",
    "    logging.info(\"Starting the training of all Transformer based models.\")\n",
    "    # Train Transformer models\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer from /tmp/models/tokenizer.json.\n",
      "INFO:root:Starting the training of all RNN based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12/12 [==============================] - 8s 656ms/step - loss: 7.2490 - val_loss: 5.1655\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 7s 625ms/step - loss: 5.2661 - val_loss: 5.2514\n",
      "INFO:tensorflow:Assets written to: /tmp/models/rnn_layers1_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/models/rnn_layers1_vocab10000_embed256_units1024/assets\n",
      "INFO:root:Starting the training of all Transformer based models.\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "models_path = Path(\"/tmp/models\")\n",
    "rnn_path = models_path / \"rnn_layers1_vocab10000_embed256_units1024\"\n",
    "EPOCHS = 2\n",
    "MAX_LEN = 100\n",
    "BS = 8\n",
    "N = 100\n",
    "_RNN_CONFIGs = [VANILLA_CONFIG]\n",
    "\n",
    "train(\n",
    "    data_path=data_path,\n",
    "    out_path=models_path,\n",
    "    epochs=EPOCHS,\n",
    "    max_length=MAX_LEN,\n",
    "    batch_size=BS,\n",
    "    n=N,\n",
    ")\n",
    "\n",
    "assert (models_path / \"tokenizer.json\").exists()\n",
    "assert rnn_path.exists()\n",
    "assert (rnn_path / \"assets\").exists()\n",
    "assert (rnn_path / \"model_config.json\").exists()\n",
    "assert (rnn_path / \"saved_model.pb\").exists()\n",
    "assert (rnn_path / \"variables\").exists()\n",
    "assert EPOCHS == len(list(rnn_path.glob(\"*.index\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
