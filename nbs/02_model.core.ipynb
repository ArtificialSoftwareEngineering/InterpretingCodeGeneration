{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> API details. @nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "max_length = 16\n",
    "batch_size = 1\n",
    "vocab_sz = 100\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens, max_length, vocab_sz=vocab_sz)\n",
    "dataset = convert_df_to_tfds(df_fake, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Model(ABC):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_probs(self, inputs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, method):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, ds, epochs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TransformerModel(Model):\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    def generate(self, n):\n",
    "        pass\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        output = {}\n",
    "        # encod method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    #         return self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    def train(self, ds, epochs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "df_trn = pd.read_json(\n",
    "    data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")[:50]\n",
    "ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = tokenizer.encode_batch(df_trn.code.values)\n",
    "hf_fast_tok_form = {\n",
    "    \"input_ids\": [x.ids for x in tok_trn],\n",
    "    \"attention_mask\": [x.attention_mask for x in tok_trn],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (dict(train_encodings), train_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_path = Path(\"/tmp/models/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[17250, 612], [16973, 7252, 47]], 'attention_mask': [[1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer([\"Hi there\", \"WasaaP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-02b17b568bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'"
     ]
    }
   ],
   "source": [
    "GPT2TokenizerFast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-da4f2361f6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "new_tokenizer.backend_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=model.compute_loss\n",
    ")  # can also use any keras loss fn\n",
    "# model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RNNModel(Model):\n",
    "    _RNN_TYPE = {\n",
    "        \"rnn\": tf.keras.layers.SimpleRNN,\n",
    "        \"gru\": tf.keras.layers.GRU,\n",
    "        \"lstm\": tf.keras.layers.LSTM,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_layers,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        rnn_units,\n",
    "        batch_size,\n",
    "        out_path,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "        self.config_name = f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    "        self.out_path = Path(out_path) / self.config_name\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        layer = RNNModel._RNN_TYPE[rnn_type]\n",
    "        rnn_layers = [\n",
    "            layer(\n",
    "                rnn_units,\n",
    "                return_sequences=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "                # following BigCode != Big Vocab Paper\n",
    "                dropout=0.5,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    mask_zero=True,  # Zero cannot be used in the vocabulary\n",
    "                ),\n",
    "            ]\n",
    "            + rnn_layers\n",
    "            + [\n",
    "                tf.keras.layers.Dense(vocab_size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        path = Path(path)\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "\n",
    "        model = RNNModel(\n",
    "            model_config[\"rnn_type\"],\n",
    "            model_config[\"n_layers\"],\n",
    "            model_config[\"vocab_size\"],\n",
    "            model_config[\"embedding_dim\"],\n",
    "            model_config[\"rnn_units\"],\n",
    "            1,\n",
    "            path,\n",
    "            tokenizer,\n",
    "        )\n",
    "        model.model = tf.keras.models.load_model(\n",
    "            str(path), custom_objects={\"_loss\": _loss}\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         input_eval = tf.expand_dims(ids, 0)\n",
    "\n",
    "        logits = self.model(inputs[\"input_ids\"])\n",
    "        probs = tf.nn.softmax(logits)  # [0].numpy()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def generate(self, n, temperature=1.0):\n",
    "        # Converting our start string to numbers (vectorizing)\n",
    "        text_generated = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        # Here batch size == 1\n",
    "        self.model.reset_states()\n",
    "        for i in range(n):\n",
    "            predictions = self.model(input_eval)\n",
    "            # remove the batch dimension\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            # using a categorical distribution to predict the character\n",
    "            # returned by the model\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[\n",
    "                -1, 0\n",
    "            ].numpy()\n",
    "\n",
    "            text_generated.append(predicted_id)\n",
    "            # Pass the predicted character as the next input to the model\n",
    "            # along with the previous hidden state\n",
    "            input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        return self.tokenizer.decode(text_generated, skip_special_tokens=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        self.model.save(str(self.out_path))\n",
    "        model_config = {\n",
    "            \"rnn_type\": self.rnn_type,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"rnn_units\": self.rnn_units,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as f:\n",
    "            json.dump(model_config, f)\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         inputs = tf.expand_dims(ids, 0)\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "        return output  # self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    # TODO add tensorboard call back for easy visualization\n",
    "    def train(self, ds_trn, ds_val, epochs):\n",
    "        self.model.compile(optimizer=\"adam\", loss=_loss)\n",
    "        history = self.model.fit(\n",
    "            ds_trn, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val\n",
    "        )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_COUNT = 124_772\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "assert PARAM_COUNT == gru.model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step - loss: 4.6008 - val_loss: 4.5705\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "chkpt_path = Path(out_path) / (\n",
    "    f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    ")\n",
    "history = gru.train(dataset, dataset, EPOCHS)\n",
    "\n",
    "assert chkpt_path.exists()\n",
    "assert EPOCHS == len(list(chkpt_path.glob(\"*.index\")))\n",
    "assert EPOCHS == len(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do I test this?\n",
    "text = \"test\"\n",
    "text_generated = tokenizer.encode(\"<sos>\" + text).ids\n",
    "input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "logits = gru.model(input_eval)[0].numpy()\n",
    "inputs = gru.tokenize(text)\n",
    "probs = gru.get_probs(inputs)[0].numpy()\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    assert np.isclose(1.0, probs[i].sum())\n",
    "    assert np.argmax(logits[i]) == np.argmax(probs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test case for earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this test will break sometimes due to encoding adding space prefixes to some\n",
    "# # tokens when add_prefix_space=False it should be fine :/\n",
    "# NUM_TOKENS = 10\n",
    "# text = gru.generate(NUM_TOKENS)\n",
    "# tokenizer.no_padding()\n",
    "# ids = tokenizer.encode(text).ids\n",
    "# tokenizer.enable_padding(length=max_length)\n",
    "\n",
    "# # -1 for the <sos> token that's always prepended\n",
    "# assert NUM_TOKENS == len(ids) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/gru_layers1_vocab100_embed128_units128/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/gru_layers1_vocab100_embed128_units128/assets\n"
     ]
    }
   ],
   "source": [
    "gru.save()\n",
    "\n",
    "assert (gru.out_path / \"assets\").exists()\n",
    "assert (gru.out_path / \"model_config.json\").exists()\n",
    "assert (gru.out_path / \"saved_model.pb\").exists()\n",
    "assert (gru.out_path / \"tokenizer.json\").exists()\n",
    "assert (gru.out_path / \"variables\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 4.5705\n",
      "2/2 - 1s - loss: 4.5705\n"
     ]
    }
   ],
   "source": [
    "loaded_gru = RNNModel.from_path(str(gru.out_path))\n",
    "\n",
    "assert gru.tokenizer.get_vocab() == loaded_gru.tokenizer.get_vocab()\n",
    "assert gru.model.count_params() == loaded_gru.model.count_params()\n",
    "assert gru.model.evaluate(dataset, verbose=2) == loaded_gru.model.evaluate(\n",
    "    dataset, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Experiment 0.0.0\n",
    "VANILLA_CONFIG = {\n",
    "    \"rnn_type\": \"rnn\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.0.0\n",
    "GRU_CONFIG_1 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.0\n",
    "GRU_CONFIG_2 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 2,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.1\n",
    "GRU_CONFIG_3 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 3,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.0\n",
    "GRU_CONFIG_4 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 512,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.1\n",
    "GRU_CONFIG_5 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 2_048,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train(\n",
    "    data_path, out_path, epochs=64, max_length=300, batch_size=64, configs=[], n=None\n",
    "):\n",
    "    \"\"\"Function for training models related to the library.\"\"\"\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Load in the datasets\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    # Check if the path where the tokenizer is to be saved is not empty\n",
    "    # if it is not empty then just load the tokenizer there.\n",
    "    if (out_path / \"tokenizer.json\").exists():\n",
    "        logging.info(f\"Loading tokenizer from {str(out_path / 'tokenizer.json')}.\")\n",
    "        tokenizer = Tokenizer.from_file(str(out_path / \"tokenizer.json\"))\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}.\"\n",
    "        )\n",
    "        df_bpe = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    "        )[:n]\n",
    "        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "        tokenizer.save(str(out_path / \"tokenizer.json\"), pretty=True)\n",
    "        del df_bpe\n",
    "\n",
    "    # Tokenize the dataset and convert it to tfds.\n",
    "    tfds_trn_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_trn_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_trn_path.exists():\n",
    "        ds_trn = tf.data.experimental.load(\n",
    "            str(tfds_trn_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_trn = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)\n",
    "        tfds_trn_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_trn, str(tfds_trn_path))\n",
    "        del df_trn\n",
    "\n",
    "    tfds_val_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_val_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_val_path.exists():\n",
    "        ds_val = tf.data.experimental.load(\n",
    "            str(tfds_val_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_val = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"valid.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)\n",
    "        tfds_val_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_val, str(tfds_val_path))\n",
    "        del df_val\n",
    "\n",
    "    logging.info(\"Starting the training of all RNN based models.\")\n",
    "    # Train RNN based models\n",
    "    for config in configs:\n",
    "        rnn_model = RNNModel(\n",
    "            config[\"rnn_type\"],\n",
    "            config[\"n_layers\"],\n",
    "            tokenizer.get_vocab_size(),\n",
    "            config[\"embedding_dim\"],\n",
    "            config[\"rnn_units\"],\n",
    "            batch_size,\n",
    "            str(out_path),\n",
    "            tokenizer,\n",
    "        )\n",
    "        rnn_model.train(ds_trn, ds_val, epochs)\n",
    "        rnn_model.save()\n",
    "\n",
    "    logging.info(\"Starting the training of all Transformer based models.\")\n",
    "    # Train Transformer models\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer from /tmp/models/tokenizer.json.\n",
      "INFO:root:Starting the training of all RNN based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12/12 [==============================] - 8s 656ms/step - loss: 7.2490 - val_loss: 5.1655\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 7s 625ms/step - loss: 5.2661 - val_loss: 5.2514\n",
      "INFO:tensorflow:Assets written to: /tmp/models/rnn_layers1_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/models/rnn_layers1_vocab10000_embed256_units1024/assets\n",
      "INFO:root:Starting the training of all Transformer based models.\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "models_path = Path(\"/tmp/models\")\n",
    "rnn_path = models_path / \"rnn_layers1_vocab10000_embed256_units1024\"\n",
    "EPOCHS = 2\n",
    "MAX_LEN = 100\n",
    "BS = 8\n",
    "N = 100\n",
    "_RNN_CONFIGs = [VANILLA_CONFIG]\n",
    "\n",
    "train(\n",
    "    data_path=data_path,\n",
    "    out_path=models_path,\n",
    "    epochs=EPOCHS,\n",
    "    max_length=MAX_LEN,\n",
    "    batch_size=BS,\n",
    "    configs=_RNN_CONFIGs,\n",
    "    n=N,\n",
    ")\n",
    "\n",
    "assert (models_path / \"tokenizer.json\").exists()\n",
    "assert rnn_path.exists()\n",
    "assert (rnn_path / \"assets\").exists()\n",
    "assert (rnn_path / \"model_config.json\").exists()\n",
    "assert (rnn_path / \"saved_model.pb\").exists()\n",
    "assert (rnn_path / \"variables\").exists()\n",
    "assert EPOCHS == len(list(rnn_path.glob(\"*.index\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
