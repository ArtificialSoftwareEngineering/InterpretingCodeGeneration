{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp evaluation.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from icodegen.data.core import replace_spec_toks_to_original, java_special_tokens\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.model.core import Model, RNNModel\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at sshleifer/tiny-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# Setting up testing data\n",
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "from icodegen.model.core import RNNModel\n",
    "\n",
    "# Using tiny-gpt2 for just quick tests since it is... tiny :)\n",
    "trnsfr_tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "tokenizer = trnsfr_tokenizer.backend_tokenizer\n",
    "trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# trnsfr_model = TransformerModel(tokenizer, trnsfr)\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru_model = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        2.01237513e-05,\n",
    "        1.98944481e-05,\n",
    "        2.01449202e-05,\n",
    "        2.04353437e-05,\n",
    "        2.02043060e-05,\n",
    "        2.02826177e-05,\n",
    "        2.09888076e-05,\n",
    "        2.07051467e-05,\n",
    "        1.98100976e-05,\n",
    "        2.02152678e-05,\n",
    "        2.02035244e-05,\n",
    "        2.10283021e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, trnsfr_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        1.99270412e-05,\n",
    "        1.99168703e-05,\n",
    "        1.98815596e-05,\n",
    "        1.99057849e-05,\n",
    "        1.98800869e-05,\n",
    "        1.98893995e-05,\n",
    "        1.98797388e-05,\n",
    "        1.98960342e-05,\n",
    "        1.99086674e-05,\n",
    "        1.98605580e-05,\n",
    "        1.98807957e-05,\n",
    "        1.98842057e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, gru_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_parens(toks: List[str], opening: str, closing: str) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Get the indices for the opening and closing tokens.\n",
    "    From https://stackoverflow.com/a/29992065/5768407\n",
    "    by user Baltasarq (https://stackoverflow.com/users/266978/baltasarq).\n",
    "\n",
    "    :param toks: the tokenized version of a method\n",
    "    :param opening: the opening token that will be matched against the closing token\n",
    "    :param closing: the closing token that will be matched against the opening token\n",
    "    :returns: returns a dictionary with the opening token indices as the keys and the closing token indices as the values\n",
    "    \"\"\"\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        if tok == opening:\n",
    "            pstack.append(i)\n",
    "        elif tok == closing:\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "\n",
    "    return toret\n",
    "\n",
    "\n",
    "def _get_dist_probs(\n",
    "    mthd: str, model: Model, opening: str, closing: str\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Get the distances and mean probabilities between opening and closing tokens in a given method.\n",
    "\n",
    "    :param mthd: the method to get the ranges of the opening and closing tokens and their probabilities\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :returns: returns a dictionary with the distance between the opening and closing tokens as keys and their mean probabilities as values\n",
    "    \"\"\"\n",
    "    # WARNING: Careful when using different tokenizers since HF tokenizers lib have diff API then HF transformers lib tokenizers... You will need to update this when using custom model and tokenizer...\n",
    "\n",
    "    # get the distances for the opening and closing tokens\n",
    "    toks = model.tokenizer.encode(mthd).tokens\n",
    "    idxs = find_parens(toks, opening, closing)\n",
    "\n",
    "    # get the model probabilities for the given method\n",
    "    inputs = model.tokenize(mthd)\n",
    "    probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    # sum up the probabilities of the different distances for the closing token\n",
    "    dist_probs = defaultdict(float)\n",
    "    for open_id, close_id in idxs.items():\n",
    "        dist_probs[close_id - open_id] += probs[close_id][\n",
    "            inputs[\"input_ids\"][0][close_id]\n",
    "        ]\n",
    "\n",
    "    # get the mean of the summed probabilities\n",
    "    dist_cnts = Counter([close_id - open_id for open_id, close_id in idxs.items()])\n",
    "    dist_probs = {dist: dist_probs[dist] / n for dist, n in dist_cnts.items()}\n",
    "    return dist_probs\n",
    "\n",
    "\n",
    "def mean_dist_probs(\n",
    "    df: pd.DataFrame,\n",
    "    model: Model,\n",
    "    opening: Optional[str] = \"<{>\",\n",
    "    closing: Optional[str] = \"<}>\",\n",
    "    n: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the distance between opening and closing tokens and the mean probability of each closing token that the model should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a dataframe with the distances between opening and closing tokens and their mean probabilities\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # get the probabilities for the different distances for an entire dataframe\n",
    "    df = df.iloc[:n].copy()\n",
    "    dist_probs = df.code.apply(\n",
    "        lambda mthd: _get_dist_probs(mthd, model, opening, closing)\n",
    "    ).values\n",
    "\n",
    "    # flatten the keys of the different distances into a list\n",
    "    dist_keys = []\n",
    "    for probs in dist_probs:\n",
    "        dist_keys.extend(probs.keys())\n",
    "    # merge dictionaries across methods by taking the mean of probs with the same distance. Modified from https://stackoverflow.com/a/10461916/5768407,\n",
    "    # users georg https://stackoverflow.com/users/989121/georg and Rémy Hosseinkhan Boucher https://stackoverflow.com/users/12149730/r%c3%a9my-hosseinkhan-boucher\n",
    "    mean_dist_probs = {\n",
    "        k: np.nanmean(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    std_dist_probs = {\n",
    "        k: np.nanstd(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "\n",
    "    med_dist_probs = {\n",
    "        k: np.nanmedian(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    mad_dist_probs = {\n",
    "        k: stats.median_abs_deviation(\n",
    "            np.array([probs.get(k, np.nan) for probs in dist_probs]), nan_policy=\"omit\"\n",
    "        )\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    # TODO: convert to dictionary\n",
    "    df_dist = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"dist\": list(mean_dist_probs.keys()),\n",
    "                \"mean_prob\": list(mean_dist_probs.values()),\n",
    "                \"std_prob\": list(std_dist_probs.values()),\n",
    "                \"med_prob\": list(med_dist_probs.values()),\n",
    "                \"mad_prob\": list(mad_dist_probs.values()),\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"dist\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, gru_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, trnsfr_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "token_taxonomy = {\n",
    "  \"blocks\": {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "    \"<return>\": \"return\"\n",
    "  },\n",
    "  \"exceptions\": {\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\"\n",
    "  },\n",
    "  \"oop\": {\n",
    "    \"<class>\": \"class\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<super>\": \"super\"\n",
    "  },\n",
    "  \"tests\": {\n",
    "    \"<assert>\": \"assert\"\n",
    "  },\n",
    "  \"declarations\": {\n",
    "    \"<native>\": \"native\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<enum>\": \"enum\"\n",
    "  },\n",
    "  \"conditionals\": {\n",
    "    \"<else>\": \"else\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<default>\": \"default\"\n",
    "  },\n",
    "  \"loops\": {\n",
    "    \"<break>\": \"break\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<while>\": \"while\",\n",
    "    \"<continue>\": \"continue\"\n",
    "  },\n",
    "  \"operators\": {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\"\n",
    "  },\n",
    "  \"datatypes\": {\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<strictfp>\": \"strictfp\"\n",
    "  },\n",
    "  \"extra_tokens\": {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "    \"<n>\": \"\\n\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "non_wordy = [\"<n>\", \"<...>\", \"<@>\", *token_taxonomy[\"operators\"], *token_taxonomy[\"blocks\"]]\n",
    "non_wordy.remove(\"<return>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    err_cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            cnts[idx] += 1\n",
    "            if p[idx] < ERROR_THRESHOLD:\n",
    "                err_cnts[idx] += 1\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    cnts = np.array(cnts)\n",
    "    err_cnts = np.array(err_cnts)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(cnts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_errs = np.divide(err_cnts, cnts, out=nans, where=cnts != 0)\n",
    "    \n",
    "    error_taxonomy = token_taxonomy.copy()\n",
    "    \n",
    "    for cat, tokens in error_taxonomy.items():\n",
    "        errs = []\n",
    "        cnt_sum = 0\n",
    "        for token, keyword in tokens.items():\n",
    "            idx = model.tokenizer.token_to_id(token)\n",
    "            error_taxonomy[cat][token] = {\"error_rate\": mean_errs[idx], \"count\": cnts[idx]}\n",
    "            errs.append(mean_errs[idx])\n",
    "            cnt_sum += cnts[idx]\n",
    "\n",
    "        errs = np.array(errs)\n",
    "        error_taxonomy[cat][\"stats\"] = {\n",
    "            \"mean_error_rate\": np.nanmean(errs),\n",
    "            \"stdev_error_rate\": np.nanstd(errs),\n",
    "            \"median_error_rate\": np.nanmedian(errs),\n",
    "            \"mad_error_rate\": stats.median_abs_deviation(errs, nan_policy=\"omit\"),\n",
    "        }\n",
    "    \n",
    "    return error_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blocks': {'<{>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<}>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<[>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<]>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<(>': {'error_rate': 1.0, 'count': 81},\n",
       "  '<)>': {'error_rate': 0.8888888888888888, 'count': 81},\n",
       "  '<;>': {'error_rate': 1.0, 'count': 53},\n",
       "  '<return>': {'error_rate': 1.0, 'count': 10},\n",
       "  'stats': {'mean_error_rate': 0.9861111111111112,\n",
       "   'stdev_error_rate': 0.03674654598700822,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'exceptions': {'<catch>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<try>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<finally>': {'error_rate': nan, 'count': 0},\n",
       "  '<throw>': {'error_rate': nan, 'count': 0},\n",
       "  '<throws>': {'error_rate': 1.0, 'count': 1},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'oop': {'<class>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<instanceof>': {'error_rate': nan, 'count': 0},\n",
       "  '<interface>': {'error_rate': nan, 'count': 0},\n",
       "  '<private>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<protected>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<public>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<abstract>': {'error_rate': nan, 'count': 0},\n",
       "  '<extends>': {'error_rate': nan, 'count': 0},\n",
       "  '<package>': {'error_rate': nan, 'count': 0},\n",
       "  '<this>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<implements>': {'error_rate': nan, 'count': 0},\n",
       "  '<import>': {'error_rate': nan, 'count': 0},\n",
       "  '<new>': {'error_rate': 1.0, 'count': 6},\n",
       "  '<super>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'tests': {'<assert>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'declarations': {'<native>': {'error_rate': nan, 'count': 0},\n",
       "  '<static>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<synchronized>': {'error_rate': nan, 'count': 0},\n",
       "  '<transient>': {'error_rate': nan, 'count': 0},\n",
       "  '<volatile>': {'error_rate': nan, 'count': 0},\n",
       "  '<void>': {'error_rate': 1.0, 'count': 7},\n",
       "  '<final>': {'error_rate': nan, 'count': 0},\n",
       "  '<enum>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'conditionals': {'<else>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<if>': {'error_rate': 1.0, 'count': 11},\n",
       "  '<switch>': {'error_rate': nan, 'count': 0},\n",
       "  '<case>': {'error_rate': nan, 'count': 0},\n",
       "  '<default>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'loops': {'<break>': {'error_rate': nan, 'count': 0},\n",
       "  '<do>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<for>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<while>': {'error_rate': nan, 'count': 0},\n",
       "  '<continue>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'operators': {'<=>': {'error_rate': 1.0, 'count': 18},\n",
       "  '<+>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<->': {'error_rate': 1.0, 'count': 2},\n",
       "  '<*>': {'error_rate': 1.0, 'count': 2},\n",
       "  '</>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<%>': {'error_rate': nan, 'count': 0},\n",
       "  '<++>': {'error_rate': nan, 'count': 0},\n",
       "  '<-->': {'error_rate': nan, 'count': 0},\n",
       "  '<!>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<==>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<!=>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<greater_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<lesser_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<&&>': {'error_rate': nan, 'count': 0},\n",
       "  '<||>': {'error_rate': nan, 'count': 0},\n",
       "  '<?>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<:>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<~>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_lesser>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<triple_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<&>': {'error_rate': nan, 'count': 0},\n",
       "  '<^>': {'error_rate': nan, 'count': 0},\n",
       "  '<|>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'datatypes': {'<byte>': {'error_rate': nan, 'count': 0},\n",
       "  '<char>': {'error_rate': nan, 'count': 0},\n",
       "  '<float>': {'error_rate': nan, 'count': 0},\n",
       "  '<boolean>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<double>': {'error_rate': nan, 'count': 0},\n",
       "  '<int>': {'error_rate': 1.0, 'count': 15},\n",
       "  '<long>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<short>': {'error_rate': nan, 'count': 0},\n",
       "  '<strictfp>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'extra_tokens': {'<@>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<...>': {'error_rate': nan, 'count': 0},\n",
       "  '<null>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<true>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<false>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<n>': {'error_rate': 1.0, 'count': 98},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :10\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/gru_layers1_vocab10000_embed256_units512\")\n",
    "err_tax = get_error_rates(df_buggy, model)\n",
    "err_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates_df(df: pd.DataFrame, model: Model, bs: int = 16, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    rows = []\n",
    "    # loop through each method\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Error Rates\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        logits = model.model(inputs)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            row = {\n",
    "                \"y_\" + k: np.array([0.] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            row_cnt = {\n",
    "                \"y_\" + k: np.array([0] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            # loop through each token and its probability and update the container lists\n",
    "            for j, (idx, p) in enumerate(zip(inputs[i], probs[i])):\n",
    "                if p[idx] < ERROR_THRESHOLD:\n",
    "                    tok = model.tokenizer.id_to_token(idx)\n",
    "                    for k in token_taxonomy:\n",
    "                        if tok in token_taxonomy[k]:\n",
    "                            # Check if token is wordy and could be part of variable or method\n",
    "                            if tok not in non_wordy:\n",
    "                                # Get the token version of the token behind the token under study\n",
    "                                # and check if the last character in the token contains a letter\n",
    "                                inp_tok_prev = model.tokenizer.id_to_token(inputs[i][j - 1])\n",
    "                                if re.search('[a-zA-Z]', inp_tok_prev[-1]):\n",
    "                                    break\n",
    "                                # Check if there is a token infront of the token under study\n",
    "                                # if there is, get the token version of it\n",
    "                                # and check if the first character in the token contains a letter\n",
    "                                if j + 1 < len(inputs[i]):\n",
    "                                    inp_tok_next = model.tokenizer.id_to_token(inputs[i][j + 1])\n",
    "                                    if re.search('[a-zA-Z]', inp_tok_next[0]):\n",
    "                                        break\n",
    "                            row[\"y_\" + k][idx] += p[idx]\n",
    "                            row_cnt[\"y_\" + k][idx] += 1\n",
    "\n",
    "            for k in row:\n",
    "                # Check if there were no tokens found in this method for this particular taxonomy category\n",
    "                if not row_cnt[k].any():\n",
    "                    row[k] = np.nan\n",
    "                else:\n",
    "                    sum_cnt = np.sum(row_cnt[k])\n",
    "                    row[k] = np.sum(row[k]) / sum_cnt\n",
    "\n",
    "            rows.append(row)\n",
    "        \n",
    "    error_df = pd.DataFrame(rows)\n",
    "    error_df[\"original_code\"] = replace_spec_toks_to_original(df, java_special_tokens, n).code.values\n",
    "    error_df[\"transformed_code\"] = df.code.values[:n]\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56437e62730148648c442ab759ce927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=7.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030134</td>\n",
       "      <td>6.314229e-06</td>\n",
       "      <td>1.747945e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.479936e-06</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>8.963920e-06</td>\n",
       "      <td>1.727996e-03</td>\n",
       "      <td>1.199283e-05</td>\n",
       "      <td>0.130223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019572</td>\n",
       "      <td>7.585470e-06</td>\n",
       "      <td>3.444690e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.380155e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>6.504990e-06</td>\n",
       "      <td>5.010473e-03</td>\n",
       "      <td>1.776130e-05</td>\n",
       "      <td>0.039746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002529</td>\n",
       "      <td>1.614634e-08</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.552259e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.059797e-07</td>\n",
       "      <td>6.955367e-07</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.010788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.016589</td>\n",
       "      <td>1.111324e-06</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.712802e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.271065e-06</td>\n",
       "      <td>1.845865e-04</td>\n",
       "      <td>8.623591e-08</td>\n",
       "      <td>0.103158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025835</td>\n",
       "      <td>2.241509e-06</td>\n",
       "      <td>1.344181e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.463187e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>7.019739e-06</td>\n",
       "      <td>4.883553e-04</td>\n",
       "      <td>3.648138e-06</td>\n",
       "      <td>0.130483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.040499</td>\n",
       "      <td>1.035260e-05</td>\n",
       "      <td>2.141599e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.845131e-07</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.417009e-05</td>\n",
       "      <td>1.127912e-03</td>\n",
       "      <td>1.580466e-05</td>\n",
       "      <td>0.158202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.087365</td>\n",
       "      <td>2.462625e-05</td>\n",
       "      <td>2.073873e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.730417e-05</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>1.905133e-05</td>\n",
       "      <td>4.579088e-02</td>\n",
       "      <td>7.690572e-05</td>\n",
       "      <td>0.224735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "count  100.000000  2.800000e+01  9.800000e+01      0.0    6.400000e+01   \n",
       "mean     0.030134  6.314229e-06  1.747945e-03      NaN    1.479936e-06   \n",
       "std      0.019572  7.585470e-06  3.444690e-03      NaN    8.380155e-06   \n",
       "min      0.002529  1.614634e-08  1.236387e-08      NaN    3.552259e-08   \n",
       "25%      0.016589  1.111324e-06  8.302875e-08      NaN    1.712802e-07   \n",
       "50%      0.025835  2.241509e-06  1.344181e-05      NaN    3.463187e-07   \n",
       "75%      0.040499  1.035260e-05  2.141599e-03      NaN    3.845131e-07   \n",
       "max      0.087365  2.462625e-05  2.073873e-02      NaN    6.730417e-05   \n",
       "\n",
       "       y_conditionals       y_loops   y_operators   y_datatypes  \\\n",
       "count       57.000000  1.900000e+01  9.700000e+01  4.100000e+01   \n",
       "mean         0.000023  8.963920e-06  1.727996e-03  1.199283e-05   \n",
       "std          0.000025  6.504990e-06  5.010473e-03  1.776130e-05   \n",
       "min          0.000005  4.059797e-07  6.955367e-07  2.637347e-08   \n",
       "25%          0.000009  4.271065e-06  1.845865e-04  8.623591e-08   \n",
       "50%          0.000013  7.019739e-06  4.883553e-04  3.648138e-06   \n",
       "75%          0.000025  1.417009e-05  1.127912e-03  1.580466e-05   \n",
       "max          0.000137  1.905133e-05  4.579088e-02  7.690572e-05   \n",
       "\n",
       "       y_extra_tokens  \n",
       "count      100.000000  \n",
       "mean         0.130223  \n",
       "std          0.039746  \n",
       "min          0.010788  \n",
       "25%          0.103158  \n",
       "50%          0.130483  \n",
       "75%          0.158202  \n",
       "max          0.224735  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax = get_error_rates_df(df_buggy, model)\n",
    "err_tax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>original_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.925746e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189852</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.131604</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.757487e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.093981e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>2.802167e-05</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.097829</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.955031e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.820177e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124276</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "0  0.016703           NaN  8.925746e-06      NaN    3.519077e-07   \n",
       "1  0.019511           NaN  1.236387e-08      NaN    3.519077e-07   \n",
       "2  0.014283           NaN  1.757487e-04      NaN    3.093981e-07   \n",
       "3  0.025020           NaN  8.302875e-08      NaN             NaN   \n",
       "4  0.070037           NaN  2.955031e-04      NaN    1.820177e-07   \n",
       "\n",
       "   y_conditionals   y_loops  y_operators   y_datatypes  y_extra_tokens  \\\n",
       "0             NaN       NaN     0.000315           NaN        0.189852   \n",
       "1         0.00001       NaN     0.004352           NaN        0.131604   \n",
       "2             NaN       NaN     0.000488  2.802167e-05        0.162338   \n",
       "3         0.00001  0.000006     0.000996  2.637347e-08        0.097829   \n",
       "4             NaN       NaN     0.000131           NaN        0.124276   \n",
       "\n",
       "                                    transformed_code  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...   \n",
       "1  <private> <void> handleConnectRequest<(>com.as...   \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...   \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...   \n",
       "4  <@>org.junit.Test<n><public> <void> configures...   \n",
       "\n",
       "                                       original_code  \n",
       "0  private void success(io.netty.channel.Channel ...  \n",
       "1  private void handleConnectRequest(com.assistan...  \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...  \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...  \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    # Need to change to sparse_categorical_crossentropy\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs[\"input_ids\"], probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.append(losses)\n",
    "\n",
    "    # flatten list of cross entropies and calculate the mean, median, std, and mad\n",
    "    cross_entropy_losses = np.concatenate(cross_entropy_losses)\n",
    "    return {\n",
    "        \"mean\": np.mean(cross_entropy_losses),\n",
    "        \"median\": np.median(cross_entropy_losses),\n",
    "        \"std\": np.std(cross_entropy_losses),\n",
    "        \"mad\": stats.median_abs_deviation(cross_entropy_losses),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = gru_model.tokenize(mthd)\n",
    "    probs = gru_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, gru_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = trnsfr_model.tokenize(mthd)\n",
    "    probs = trnsfr_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, trnsfr_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy_df(df: pd.DataFrame, model: Model, bs = 16, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Cross Entropies\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        logits = model.model(inputs)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs, probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.extend(np.mean(losses, axis = 1))\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        zip(replace_spec_toks_to_original(df, java_special_tokens, n).code.values, df.code.values[:n], cross_entropy_losses),\n",
    "        columns=[\"original_code\", \"transformed_code\", \"y_cross_entropy\"]\n",
    "    )\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units2048/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers2_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units512/\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "path = Path(\"/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses\")\n",
    "for d in glob(str(path) + \"/*/\"):\n",
    "    if \"gru_layers3_vocab10000_embed256_units1024\" in d: continue\n",
    "    print(d)\n",
    "    bug_fix_cross_entropy = pd.read_json(\n",
    "        d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    bug_fix_cross_entropy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "        bug_fix_cross_entropy, java_special_tokens\n",
    "    ).code.values\n",
    "    bug_fix_cross_entropy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)\n",
    "    bug_fix_cross_entropy.to_json(d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "#     bug_fix_error_taxonomy = pd.read_json(\n",
    "#         d + \"/bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    "#     )\n",
    "#     bug_fix_error_taxonomy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "#         bug_fix_error_taxonomy, java_special_tokens\n",
    "#     ).code.values\n",
    "#     bug_fix_error_taxonomy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d846b7d302c4170b89deb9b7005415b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cross Entropies', max=7.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.036340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.529498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.113172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.934342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.905706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.866514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.877866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_cross_entropy\n",
       "count       100.000000\n",
       "mean          6.036340\n",
       "std           1.529498\n",
       "min           3.113172\n",
       "25%           4.934342\n",
       "50%           5.905706\n",
       "75%           6.866514\n",
       "max           9.877866"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = get_mean_cross_entropy_df(df_buggy, model)\n",
    "cross_entropy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>3.711157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>6.497345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_code  \\\n",
       "0  private void success(io.netty.channel.Channel ...   \n",
       "1  private void handleConnectRequest(com.assistan...   \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...   \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...   \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...   \n",
       "\n",
       "                                    transformed_code  y_cross_entropy  \n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006714  \n",
       "1  <private> <void> handleConnectRequest<(>com.as...         6.368460  \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...         3.927362  \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         3.711157  \n",
       "4  <@>org.junit.Test<n><public> <void> configures...         6.497345  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_TRANSFORMs = {\n",
    "#     \"randomized_tokens\": code_token_randomizer,\n",
    "#     \"randomized_lines\": line_randomizer,\n",
    "    \"comments_removed\": java_comment_remover,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_metrics(df, model):\n",
    "#     mean_probs = get_mean_probs(df, model)\n",
    "    error_taxonomy_df = get_error_rates_df(df, model, bs = 192)\n",
    "#     df_dist = mean_dist_probs(df, model)\n",
    "#     mean_cross_entropy_df = get_mean_cross_entropy_df(df, model, bs = 192)\n",
    "\n",
    "    return {\n",
    "        \"error_taxonomy\": error_taxonomy_df,\n",
    "#         \"dist_mean\": df_dist,\n",
    "#         \"mean_cross_entropy\": mean_cross_entropy_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def _long_range(bigclone_path, bugfix_path, codesearchnet_path, model, out_path, n=None):\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    long_range_results = {}\n",
    "\n",
    "    # TODO add bigclone data\n",
    "\n",
    "#     df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "#         :n\n",
    "#     ]\n",
    "#     buggy_metrics = _get_metrics(df_buggy, model)\n",
    "\n",
    "#     df_fixed = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)[\n",
    "#         :n\n",
    "#     ]\n",
    "#     fixed_metrics = _get_metrics(df_fixed, model)\n",
    "    \n",
    "#     bug_fix_err_df = pd.concat(\n",
    "#         [buggy_metrics[\"error_taxonomy\"], fixed_metrics[\"error_taxonomy\"]]\n",
    "#     ).sort_index().reset_index(drop=True)\n",
    "#     bug_fix_err_df[\"x_treatment\"] = [False, True] * len(buggy_metrics[\"error_taxonomy\"])\n",
    "#     bug_fix_err_df.to_json(out_path / \"bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "#     bug_fix_cross_df = pd.concat(\n",
    "#         [buggy_metrics[\"mean_cross_entropy\"], fixed_metrics[\"mean_cross_entropy\"]]\n",
    "#     ).sort_index().reset_index(drop=True)\n",
    "#     bug_fix_cross_df[\"x_treatment\"] = [False, True] * len(buggy_metrics[\"mean_cross_entropy\"])\n",
    "#     bug_fix_cross_df.to_json(out_path / \"bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     df_codesearchnet = pd.read_json(\n",
    "#         codesearchnet_path / \"codesearchnet_java\" / \"test.jsonl\",\n",
    "#         orient=\"records\",\n",
    "#         lines=True,\n",
    "#     )[:n]\n",
    "#     long_range_results[\"codesearchnet_original\"] = _get_metrics(df_codesearchnet, model)\n",
    "\n",
    "#     for transform in _TRANSFORMs:\n",
    "#         df_transformed = transform_df(df_codesearchnet, _TRANSFORMs[transform])\n",
    "#         long_range_results[\"codesearchnet_\" + transform] = _get_metrics(\n",
    "#             df_transformed, model\n",
    "#         )\n",
    "\n",
    "    return long_range_results\n",
    "\n",
    "\n",
    "def _counterfactual(control_results, treatment_results):\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(data_path, model_path, experiment_path):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    testbed_path = data_path / \"testbed\"\n",
    "    #     models = []\n",
    "    # These model folders will need to contain the config of the model as well\n",
    "    # to differentiate them\n",
    "    for m_path in model_path.glob(\"*/\"):\n",
    "        model = None\n",
    "        if \"dvc\" in m_path.name:\n",
    "            continue\n",
    "        elif \".gitignore\" in m_path.name:\n",
    "            continue\n",
    "        print(m_path)\n",
    "        model = RNNModel.from_path(m_path)\n",
    "#         if m_path.name == \"Transformer\":\n",
    "#             model = TransformerModel.from_path(m_path)\n",
    "#         elif \"rnn\" in m_path.name:\n",
    "#             model = RNNModel.from_path(m_path)\n",
    "#         elif m_path.name == \"RNN\":\n",
    "#             pass\n",
    "#         return model\n",
    "    \n",
    "        bigclone_path = testbed_path / \"ts-bigclone-types\"\n",
    "        bugfix_path = testbed_path / \"ts-bug-fix\"\n",
    "        codesearchnet_path = testbed_path / \"ts-comments\"\n",
    "\n",
    "        # Long-Range Interactions\n",
    "        _long_range(\n",
    "            bigclone_path, bugfix_path, codesearchnet_path,\n",
    "            model, experiment_path / m_path.name\n",
    "        )\n",
    "        # Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/testbed/ts-bug-fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "df_fixed = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64722, 64722)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_buggy), len(df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8fba320fc64ea492c5634d67606337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=338.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8075e392dbf4eb89d20d2b9909d4088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=338.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jovyan/work/dvc-icodegen/models/controlled/rnns/gru_layers1_vocab10000_embed256_units1024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df010dc8b5cf49e58e7ced60515ee7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=338.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f787cb33b0541f5a81d90713f3b075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=338.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jovyan/work/dvc-icodegen/models/controlled/rnns/gru_layers1_vocab10000_embed256_units512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60ba9fc7e26497ab6d300bbbbf3c779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=338.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = Path(\"/home/jovyan/work\")\n",
    "data_path = path / \"dvc-icodegen\"\n",
    "model_path = path / \"dvc-icodegen/models/controlled/rnns/\"\n",
    "experiment_path = path / \"dvc-icodegennbs/nbs_experiments/results/analyses\"\n",
    "results = evaluate(data_path, model_path, experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_fix_cross_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006220</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.008038</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368741</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.337473</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927699</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.910351</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>3.711497</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>4.510262</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>6.497474</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>5.749438</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  y_cross_entropy  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006220   \n",
       "1  <private> <void> success<(>io.netty.channel.Ch...         6.008038   \n",
       "2  <private> <void> handleConnectRequest<(>com.as...         6.368741   \n",
       "3  <private> <void> handleConnectRequest<(>com.as...         6.337473   \n",
       "4  <@>java.lang.Override<n><protected> <void> onS...         3.927699   \n",
       "5  <@>java.lang.Override<n><protected> <void> onS...         3.910351   \n",
       "6  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         3.711497   \n",
       "7  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         4.510262   \n",
       "8  <@>org.junit.Test<n><public> <void> configures...         6.497474   \n",
       "9  <@>org.junit.Test<n><public> <void> configures...         5.749438   \n",
       "\n",
       "   x_treatment  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3         True  \n",
       "4        False  \n",
       "5         True  \n",
       "6        False  \n",
       "7         True  \n",
       "8        False  \n",
       "9         True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_fix_cross_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_taxonomy_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blocks</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>oop</th>\n",
       "      <th>tests</th>\n",
       "      <th>declarations</th>\n",
       "      <th>conditionals</th>\n",
       "      <th>loops</th>\n",
       "      <th>operators</th>\n",
       "      <th>datatypes</th>\n",
       "      <th>extra_tokens</th>\n",
       "      <th>code</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blocks  exceptions     oop   tests  declarations  conditionals   loops  \\\n",
       "0  0.0027         0.0  0.0002  0.0000        0.0001        0.0000  0.0001   \n",
       "1  0.0027         0.0  0.0002  0.0000        0.0001        0.0000  0.0001   \n",
       "2  0.0027         0.0  0.0001  0.0000        0.0001        0.0002  0.0000   \n",
       "3  0.0027         0.0  0.0001  0.0000        0.0001        0.0002  0.0000   \n",
       "4  0.0018         0.0  0.0002  0.0000        0.0001        0.0000  0.0000   \n",
       "5  0.0018         0.0  0.0002  0.0000        0.0001        0.0000  0.0000   \n",
       "6  0.0030         0.0  0.0001  0.0000        0.0000        0.0002  0.0002   \n",
       "7  0.0029         0.0  0.0001  0.0000        0.0000        0.0002  0.0002   \n",
       "8  0.0023         0.0  0.0003  0.0002        0.0001        0.0000  0.0001   \n",
       "9  0.0023         0.0  0.0003  0.0002        0.0001        0.0000  0.0001   \n",
       "\n",
       "   operators  datatypes  extra_tokens  \\\n",
       "0     0.0003     0.0000        0.0008   \n",
       "1     0.0003     0.0000        0.0008   \n",
       "2     0.0002     0.0000        0.0012   \n",
       "3     0.0002     0.0000        0.0012   \n",
       "4     0.0006     0.0005        0.0009   \n",
       "5     0.0006     0.0005        0.0009   \n",
       "6     0.0005     0.0001        0.0013   \n",
       "7     0.0005     0.0001        0.0012   \n",
       "8     0.0001     0.0000        0.0006   \n",
       "9     0.0001     0.0000        0.0006   \n",
       "\n",
       "                                                code  x_treatment  \n",
       "0  <private> <void> success<(>io.netty.channel.Ch...        False  \n",
       "1  <private> <void> success<(>io.netty.channel.Ch...         True  \n",
       "2  <private> <void> handleConnectRequest<(>com.as...        False  \n",
       "3  <private> <void> handleConnectRequest<(>com.as...         True  \n",
       "4  <@>java.lang.Override<n><protected> <void> onS...        False  \n",
       "5  <@>java.lang.Override<n><protected> <void> onS...         True  \n",
       "6  <public> <boolean> <do>esEdgesHaveWeight<(><)>...        False  \n",
       "7  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         True  \n",
       "8  <@>org.junit.Test<n><public> <void> configures...        False  \n",
       "9  <@>org.junit.Test<n><public> <void> configures...         True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_taxonomy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
