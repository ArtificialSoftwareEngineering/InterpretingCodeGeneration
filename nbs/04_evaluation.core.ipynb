{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp evaluation.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from icodegen.data.core import replace_spec_toks_to_original, java_special_tokens\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.model.core import Model, RNNModel, TransformerHFModel, _loss\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at sshleifer/tiny-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# Setting up testing data\n",
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "from icodegen.model.core import RNNModel\n",
    "\n",
    "# Using tiny-gpt2 for just quick tests since it is... tiny :)\n",
    "trnsfr_tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "tokenizer = trnsfr_tokenizer.backend_tokenizer\n",
    "trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# trnsfr_model = TransformerModel(tokenizer, trnsfr)\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru_model = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        2.01237513e-05,\n",
    "        1.98944481e-05,\n",
    "        2.01449202e-05,\n",
    "        2.04353437e-05,\n",
    "        2.02043060e-05,\n",
    "        2.02826177e-05,\n",
    "        2.09888076e-05,\n",
    "        2.07051467e-05,\n",
    "        1.98100976e-05,\n",
    "        2.02152678e-05,\n",
    "        2.02035244e-05,\n",
    "        2.10283021e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, trnsfr_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        1.99270412e-05,\n",
    "        1.99168703e-05,\n",
    "        1.98815596e-05,\n",
    "        1.99057849e-05,\n",
    "        1.98800869e-05,\n",
    "        1.98893995e-05,\n",
    "        1.98797388e-05,\n",
    "        1.98960342e-05,\n",
    "        1.99086674e-05,\n",
    "        1.98605580e-05,\n",
    "        1.98807957e-05,\n",
    "        1.98842057e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, gru_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_parens(toks: List[str], opening: str, closing: str) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Get the indices for the opening and closing tokens.\n",
    "    From https://stackoverflow.com/a/29992065/5768407\n",
    "    by user Baltasarq (https://stackoverflow.com/users/266978/baltasarq).\n",
    "\n",
    "    :param toks: the tokenized version of a method\n",
    "    :param opening: the opening token that will be matched against the closing token\n",
    "    :param closing: the closing token that will be matched against the opening token\n",
    "    :returns: returns a dictionary with the opening token indices as the keys and the closing token indices as the values\n",
    "    \"\"\"\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        if tok == opening:\n",
    "            pstack.append(i)\n",
    "        elif tok == closing:\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "\n",
    "    return toret\n",
    "\n",
    "\n",
    "def _get_dist_probs(\n",
    "    mthd: str, model: Model, opening: str, closing: str\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Get the distances and mean probabilities between opening and closing tokens in a given method.\n",
    "\n",
    "    :param mthd: the method to get the ranges of the opening and closing tokens and their probabilities\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :returns: returns a dictionary with the distance between the opening and closing tokens as keys and their mean probabilities as values\n",
    "    \"\"\"\n",
    "    # WARNING: Careful when using different tokenizers since HF tokenizers lib have diff API then HF transformers lib tokenizers... You will need to update this when using custom model and tokenizer...\n",
    "\n",
    "    # get the distances for the opening and closing tokens\n",
    "    toks = model.tokenizer.encode(mthd).tokens\n",
    "    idxs = find_parens(toks, opening, closing)\n",
    "\n",
    "    # get the model probabilities for the given method\n",
    "    inputs = model.tokenize(mthd)\n",
    "    probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    # sum up the probabilities of the different distances for the closing token\n",
    "    dist_probs = defaultdict(float)\n",
    "    for open_id, close_id in idxs.items():\n",
    "        dist_probs[close_id - open_id] += probs[close_id][\n",
    "            inputs[\"input_ids\"][0][close_id]\n",
    "        ]\n",
    "\n",
    "    # get the mean of the summed probabilities\n",
    "    dist_cnts = Counter([close_id - open_id for open_id, close_id in idxs.items()])\n",
    "    dist_probs = {dist: dist_probs[dist] / n for dist, n in dist_cnts.items()}\n",
    "    return dist_probs\n",
    "\n",
    "\n",
    "def mean_dist_probs(\n",
    "    df: pd.DataFrame,\n",
    "    model: Model,\n",
    "    opening: Optional[str] = \"<{>\",\n",
    "    closing: Optional[str] = \"<}>\",\n",
    "    n: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the distance between opening and closing tokens and the mean probability of each closing token that the model should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a dataframe with the distances between opening and closing tokens and their mean probabilities\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # get the probabilities for the different distances for an entire dataframe\n",
    "    df = df.iloc[:n].copy()\n",
    "    dist_probs = df.code.apply(\n",
    "        lambda mthd: _get_dist_probs(mthd, model, opening, closing)\n",
    "    ).values\n",
    "\n",
    "    # flatten the keys of the different distances into a list\n",
    "    dist_keys = []\n",
    "    for probs in dist_probs:\n",
    "        dist_keys.extend(probs.keys())\n",
    "    # merge dictionaries across methods by taking the mean of probs with the same distance. Modified from https://stackoverflow.com/a/10461916/5768407,\n",
    "    # users georg https://stackoverflow.com/users/989121/georg and Rémy Hosseinkhan Boucher https://stackoverflow.com/users/12149730/r%c3%a9my-hosseinkhan-boucher\n",
    "    mean_dist_probs = {\n",
    "        k: np.nanmean(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    std_dist_probs = {\n",
    "        k: np.nanstd(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "\n",
    "    med_dist_probs = {\n",
    "        k: np.nanmedian(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    mad_dist_probs = {\n",
    "        k: stats.median_abs_deviation(\n",
    "            np.array([probs.get(k, np.nan) for probs in dist_probs]), nan_policy=\"omit\"\n",
    "        )\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    # TODO: convert to dictionary\n",
    "    df_dist = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"dist\": list(mean_dist_probs.keys()),\n",
    "                \"mean_prob\": list(mean_dist_probs.values()),\n",
    "                \"std_prob\": list(std_dist_probs.values()),\n",
    "                \"med_prob\": list(med_dist_probs.values()),\n",
    "                \"mad_prob\": list(mad_dist_probs.values()),\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"dist\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, gru_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, trnsfr_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "token_taxonomy = {\n",
    "  \"blocks\": {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "    \"<return>\": \"return\"\n",
    "  },\n",
    "  \"exceptions\": {\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\"\n",
    "  },\n",
    "  \"oop\": {\n",
    "    \"<class>\": \"class\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<super>\": \"super\"\n",
    "  },\n",
    "  \"tests\": {\n",
    "    \"<assert>\": \"assert\"\n",
    "  },\n",
    "  \"declarations\": {\n",
    "    \"<native>\": \"native\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<enum>\": \"enum\"\n",
    "  },\n",
    "  \"conditionals\": {\n",
    "    \"<else>\": \"else\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<default>\": \"default\"\n",
    "  },\n",
    "  \"loops\": {\n",
    "    \"<break>\": \"break\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<while>\": \"while\",\n",
    "    \"<continue>\": \"continue\"\n",
    "  },\n",
    "  \"operators\": {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\"\n",
    "  },\n",
    "  \"datatypes\": {\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<strictfp>\": \"strictfp\"\n",
    "  },\n",
    "  \"extra_tokens\": {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "    \"<n>\": \"\\n\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "non_wordy = [\"<n>\", \"<...>\", \"<@>\", *token_taxonomy[\"operators\"], *token_taxonomy[\"blocks\"]]\n",
    "non_wordy.remove(\"<return>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    err_cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            cnts[idx] += 1\n",
    "            if p[idx] < ERROR_THRESHOLD:\n",
    "                err_cnts[idx] += 1\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    cnts = np.array(cnts)\n",
    "    err_cnts = np.array(err_cnts)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(cnts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_errs = np.divide(err_cnts, cnts, out=nans, where=cnts != 0)\n",
    "    \n",
    "    error_taxonomy = token_taxonomy.copy()\n",
    "    \n",
    "    for cat, tokens in error_taxonomy.items():\n",
    "        errs = []\n",
    "        cnt_sum = 0\n",
    "        for token, keyword in tokens.items():\n",
    "            idx = model.tokenizer.token_to_id(token)\n",
    "            error_taxonomy[cat][token] = {\"error_rate\": mean_errs[idx], \"count\": cnts[idx]}\n",
    "            errs.append(mean_errs[idx])\n",
    "            cnt_sum += cnts[idx]\n",
    "\n",
    "        errs = np.array(errs)\n",
    "        error_taxonomy[cat][\"stats\"] = {\n",
    "            \"mean_error_rate\": np.nanmean(errs),\n",
    "            \"stdev_error_rate\": np.nanstd(errs),\n",
    "            \"median_error_rate\": np.nanmedian(errs),\n",
    "            \"mad_error_rate\": stats.median_abs_deviation(errs, nan_policy=\"omit\"),\n",
    "        }\n",
    "    \n",
    "    return error_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blocks': {'<{>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<}>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<[>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<]>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<(>': {'error_rate': 1.0, 'count': 81},\n",
       "  '<)>': {'error_rate': 0.8888888888888888, 'count': 81},\n",
       "  '<;>': {'error_rate': 1.0, 'count': 53},\n",
       "  '<return>': {'error_rate': 1.0, 'count': 10},\n",
       "  'stats': {'mean_error_rate': 0.9861111111111112,\n",
       "   'stdev_error_rate': 0.03674654598700822,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'exceptions': {'<catch>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<try>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<finally>': {'error_rate': nan, 'count': 0},\n",
       "  '<throw>': {'error_rate': nan, 'count': 0},\n",
       "  '<throws>': {'error_rate': 1.0, 'count': 1},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'oop': {'<class>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<instanceof>': {'error_rate': nan, 'count': 0},\n",
       "  '<interface>': {'error_rate': nan, 'count': 0},\n",
       "  '<private>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<protected>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<public>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<abstract>': {'error_rate': nan, 'count': 0},\n",
       "  '<extends>': {'error_rate': nan, 'count': 0},\n",
       "  '<package>': {'error_rate': nan, 'count': 0},\n",
       "  '<this>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<implements>': {'error_rate': nan, 'count': 0},\n",
       "  '<import>': {'error_rate': nan, 'count': 0},\n",
       "  '<new>': {'error_rate': 1.0, 'count': 6},\n",
       "  '<super>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'tests': {'<assert>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'declarations': {'<native>': {'error_rate': nan, 'count': 0},\n",
       "  '<static>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<synchronized>': {'error_rate': nan, 'count': 0},\n",
       "  '<transient>': {'error_rate': nan, 'count': 0},\n",
       "  '<volatile>': {'error_rate': nan, 'count': 0},\n",
       "  '<void>': {'error_rate': 1.0, 'count': 7},\n",
       "  '<final>': {'error_rate': nan, 'count': 0},\n",
       "  '<enum>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'conditionals': {'<else>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<if>': {'error_rate': 1.0, 'count': 11},\n",
       "  '<switch>': {'error_rate': nan, 'count': 0},\n",
       "  '<case>': {'error_rate': nan, 'count': 0},\n",
       "  '<default>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'loops': {'<break>': {'error_rate': nan, 'count': 0},\n",
       "  '<do>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<for>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<while>': {'error_rate': nan, 'count': 0},\n",
       "  '<continue>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'operators': {'<=>': {'error_rate': 1.0, 'count': 18},\n",
       "  '<+>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<->': {'error_rate': 1.0, 'count': 2},\n",
       "  '<*>': {'error_rate': 1.0, 'count': 2},\n",
       "  '</>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<%>': {'error_rate': nan, 'count': 0},\n",
       "  '<++>': {'error_rate': nan, 'count': 0},\n",
       "  '<-->': {'error_rate': nan, 'count': 0},\n",
       "  '<!>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<==>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<!=>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<greater_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<lesser_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<&&>': {'error_rate': nan, 'count': 0},\n",
       "  '<||>': {'error_rate': nan, 'count': 0},\n",
       "  '<?>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<:>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<~>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_lesser>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<triple_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<&>': {'error_rate': nan, 'count': 0},\n",
       "  '<^>': {'error_rate': nan, 'count': 0},\n",
       "  '<|>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'datatypes': {'<byte>': {'error_rate': nan, 'count': 0},\n",
       "  '<char>': {'error_rate': nan, 'count': 0},\n",
       "  '<float>': {'error_rate': nan, 'count': 0},\n",
       "  '<boolean>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<double>': {'error_rate': nan, 'count': 0},\n",
       "  '<int>': {'error_rate': 1.0, 'count': 15},\n",
       "  '<long>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<short>': {'error_rate': nan, 'count': 0},\n",
       "  '<strictfp>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'extra_tokens': {'<@>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<...>': {'error_rate': nan, 'count': 0},\n",
       "  '<null>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<true>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<false>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<n>': {'error_rate': 1.0, 'count': 98},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :10\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/gru_layers1_vocab10000_embed256_units512\")\n",
    "err_tax = get_error_rates(df_buggy, model)\n",
    "err_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates_df(df: pd.DataFrame, model: Model, bs: int = 16, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    rows = []\n",
    "    # loop through each method\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Error Rates\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        probs = model.get_probs(inputs)\n",
    "#         logits = model.model(inputs)\n",
    "#         probs = tf.nn.softmax(logits).numpy()\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            row = {\n",
    "                \"y_\" + k: np.array([0.] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            row_cnt = {\n",
    "                \"y_\" + k: np.array([0] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            # loop through each token and its probability and update the container lists\n",
    "            for j, (idx, p) in enumerate(zip(inputs[i], probs[i])):\n",
    "                if p[idx] < ERROR_THRESHOLD:\n",
    "                    tok = model.tokenizer.id_to_token(idx)\n",
    "                    for k in token_taxonomy:\n",
    "                        if tok in token_taxonomy[k]:\n",
    "                            # Check if token is wordy and could be part of variable or method\n",
    "                            if tok not in non_wordy:\n",
    "                                # Get the token version of the token behind the token under study\n",
    "                                # and check if the last character in the token contains a letter\n",
    "                                inp_tok_prev = model.tokenizer.id_to_token(inputs[i][j - 1])\n",
    "                                if re.search('[a-zA-Z]', inp_tok_prev[-1]):\n",
    "                                    break\n",
    "                                # Check if there is a token infront of the token under study\n",
    "                                # if there is, get the token version of it\n",
    "                                # and check if the first character in the token contains a letter\n",
    "                                if j + 1 < len(inputs[i]):\n",
    "                                    inp_tok_next = model.tokenizer.id_to_token(inputs[i][j + 1])\n",
    "                                    if re.search('[a-zA-Z]', inp_tok_next[0]):\n",
    "                                        break\n",
    "                            row[\"y_\" + k][idx] += p[idx]\n",
    "                            row_cnt[\"y_\" + k][idx] += 1\n",
    "\n",
    "            for k in row:\n",
    "                # Check if there were no tokens found in this method for this particular taxonomy category\n",
    "                if not row_cnt[k].any():\n",
    "                    row[k] = np.nan\n",
    "                else:\n",
    "                    sum_cnt = np.sum(row_cnt[k])\n",
    "                    row[k] = np.sum(row[k]) / sum_cnt\n",
    "\n",
    "            rows.append(row)\n",
    "        \n",
    "    error_df = pd.DataFrame(rows)\n",
    "    error_df[\"original_code\"] = replace_spec_toks_to_original(df, java_special_tokens, n).code.values\n",
    "    error_df[\"transformed_code\"] = df.code.values[:n]\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56437e62730148648c442ab759ce927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=7.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030134</td>\n",
       "      <td>6.314229e-06</td>\n",
       "      <td>1.747945e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.479936e-06</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>8.963920e-06</td>\n",
       "      <td>1.727996e-03</td>\n",
       "      <td>1.199283e-05</td>\n",
       "      <td>0.130223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019572</td>\n",
       "      <td>7.585470e-06</td>\n",
       "      <td>3.444690e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.380155e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>6.504990e-06</td>\n",
       "      <td>5.010473e-03</td>\n",
       "      <td>1.776130e-05</td>\n",
       "      <td>0.039746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002529</td>\n",
       "      <td>1.614634e-08</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.552259e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.059797e-07</td>\n",
       "      <td>6.955367e-07</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.010788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.016589</td>\n",
       "      <td>1.111324e-06</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.712802e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.271065e-06</td>\n",
       "      <td>1.845865e-04</td>\n",
       "      <td>8.623591e-08</td>\n",
       "      <td>0.103158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025835</td>\n",
       "      <td>2.241509e-06</td>\n",
       "      <td>1.344181e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.463187e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>7.019739e-06</td>\n",
       "      <td>4.883553e-04</td>\n",
       "      <td>3.648138e-06</td>\n",
       "      <td>0.130483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.040499</td>\n",
       "      <td>1.035260e-05</td>\n",
       "      <td>2.141599e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.845131e-07</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.417009e-05</td>\n",
       "      <td>1.127912e-03</td>\n",
       "      <td>1.580466e-05</td>\n",
       "      <td>0.158202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.087365</td>\n",
       "      <td>2.462625e-05</td>\n",
       "      <td>2.073873e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.730417e-05</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>1.905133e-05</td>\n",
       "      <td>4.579088e-02</td>\n",
       "      <td>7.690572e-05</td>\n",
       "      <td>0.224735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "count  100.000000  2.800000e+01  9.800000e+01      0.0    6.400000e+01   \n",
       "mean     0.030134  6.314229e-06  1.747945e-03      NaN    1.479936e-06   \n",
       "std      0.019572  7.585470e-06  3.444690e-03      NaN    8.380155e-06   \n",
       "min      0.002529  1.614634e-08  1.236387e-08      NaN    3.552259e-08   \n",
       "25%      0.016589  1.111324e-06  8.302875e-08      NaN    1.712802e-07   \n",
       "50%      0.025835  2.241509e-06  1.344181e-05      NaN    3.463187e-07   \n",
       "75%      0.040499  1.035260e-05  2.141599e-03      NaN    3.845131e-07   \n",
       "max      0.087365  2.462625e-05  2.073873e-02      NaN    6.730417e-05   \n",
       "\n",
       "       y_conditionals       y_loops   y_operators   y_datatypes  \\\n",
       "count       57.000000  1.900000e+01  9.700000e+01  4.100000e+01   \n",
       "mean         0.000023  8.963920e-06  1.727996e-03  1.199283e-05   \n",
       "std          0.000025  6.504990e-06  5.010473e-03  1.776130e-05   \n",
       "min          0.000005  4.059797e-07  6.955367e-07  2.637347e-08   \n",
       "25%          0.000009  4.271065e-06  1.845865e-04  8.623591e-08   \n",
       "50%          0.000013  7.019739e-06  4.883553e-04  3.648138e-06   \n",
       "75%          0.000025  1.417009e-05  1.127912e-03  1.580466e-05   \n",
       "max          0.000137  1.905133e-05  4.579088e-02  7.690572e-05   \n",
       "\n",
       "       y_extra_tokens  \n",
       "count      100.000000  \n",
       "mean         0.130223  \n",
       "std          0.039746  \n",
       "min          0.010788  \n",
       "25%          0.103158  \n",
       "50%          0.130483  \n",
       "75%          0.158202  \n",
       "max          0.224735  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax = get_error_rates_df(df_buggy, model)\n",
    "err_tax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>original_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.925746e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189852</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.131604</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.757487e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.093981e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>2.802167e-05</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.097829</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.955031e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.820177e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124276</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "0  0.016703           NaN  8.925746e-06      NaN    3.519077e-07   \n",
       "1  0.019511           NaN  1.236387e-08      NaN    3.519077e-07   \n",
       "2  0.014283           NaN  1.757487e-04      NaN    3.093981e-07   \n",
       "3  0.025020           NaN  8.302875e-08      NaN             NaN   \n",
       "4  0.070037           NaN  2.955031e-04      NaN    1.820177e-07   \n",
       "\n",
       "   y_conditionals   y_loops  y_operators   y_datatypes  y_extra_tokens  \\\n",
       "0             NaN       NaN     0.000315           NaN        0.189852   \n",
       "1         0.00001       NaN     0.004352           NaN        0.131604   \n",
       "2             NaN       NaN     0.000488  2.802167e-05        0.162338   \n",
       "3         0.00001  0.000006     0.000996  2.637347e-08        0.097829   \n",
       "4             NaN       NaN     0.000131           NaN        0.124276   \n",
       "\n",
       "                                    transformed_code  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...   \n",
       "1  <private> <void> handleConnectRequest<(>com.as...   \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...   \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...   \n",
       "4  <@>org.junit.Test<n><public> <void> configures...   \n",
       "\n",
       "                                       original_code  \n",
       "0  private void success(io.netty.channel.Channel ...  \n",
       "1  private void handleConnectRequest(com.assistan...  \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...  \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...  \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    # Need to change to sparse_categorical_crossentropy\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs[\"input_ids\"], probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.append(losses)\n",
    "\n",
    "    # flatten list of cross entropies and calculate the mean, median, std, and mad\n",
    "    cross_entropy_losses = np.concatenate(cross_entropy_losses)\n",
    "    return {\n",
    "        \"mean\": np.mean(cross_entropy_losses),\n",
    "        \"median\": np.median(cross_entropy_losses),\n",
    "        \"std\": np.std(cross_entropy_losses),\n",
    "        \"mad\": stats.median_abs_deviation(cross_entropy_losses),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = gru_model.tokenize(mthd)\n",
    "    probs = gru_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, gru_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = trnsfr_model.tokenize(mthd)\n",
    "    probs = trnsfr_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, trnsfr_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy_df(df: pd.DataFrame, model: Model, bs = 16, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Cross Entropies\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        logits = model.model(inputs)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs, probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.extend(np.mean(losses, axis = 1))\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        zip(replace_spec_toks_to_original(df, java_special_tokens, n).code.values, df.code.values[:n], cross_entropy_losses),\n",
    "        columns=[\"original_code\", \"transformed_code\", \"y_cross_entropy\"]\n",
    "    )\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units2048/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers2_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units512/\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "path = Path(\"/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses\")\n",
    "for d in glob(str(path) + \"/*/\"):\n",
    "    if \"gru_layers3_vocab10000_embed256_units1024\" in d: continue\n",
    "    print(d)\n",
    "    bug_fix_cross_entropy = pd.read_json(\n",
    "        d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    bug_fix_cross_entropy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "        bug_fix_cross_entropy, java_special_tokens\n",
    "    ).code.values\n",
    "    bug_fix_cross_entropy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)\n",
    "    bug_fix_cross_entropy.to_json(d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "#     bug_fix_error_taxonomy = pd.read_json(\n",
    "#         d + \"/bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    "#     )\n",
    "#     bug_fix_error_taxonomy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "#         bug_fix_error_taxonomy, java_special_tokens\n",
    "#     ).code.values\n",
    "#     bug_fix_error_taxonomy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d846b7d302c4170b89deb9b7005415b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cross Entropies', max=7.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.036340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.529498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.113172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.934342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.905706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.866514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.877866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_cross_entropy\n",
       "count       100.000000\n",
       "mean          6.036340\n",
       "std           1.529498\n",
       "min           3.113172\n",
       "25%           4.934342\n",
       "50%           5.905706\n",
       "75%           6.866514\n",
       "max           9.877866"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = get_mean_cross_entropy_df(df_buggy, model)\n",
    "cross_entropy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>3.711157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>6.497345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_code  \\\n",
       "0  private void success(io.netty.channel.Channel ...   \n",
       "1  private void handleConnectRequest(com.assistan...   \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...   \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...   \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...   \n",
       "\n",
       "                                    transformed_code  y_cross_entropy  \n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006714  \n",
       "1  <private> <void> handleConnectRequest<(>com.as...         6.368460  \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...         3.927362  \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         3.711157  \n",
       "4  <@>org.junit.Test<n><public> <void> configures...         6.497345  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "TYPES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_metrics(df, model):\n",
    "#     mean_probs = get_mean_probs(df, model)\n",
    "    error_taxonomy_df = get_error_rates_df(df, model, bs = 192)\n",
    "#     df_dist = mean_dist_probs(df, model)\n",
    "    mean_cross_entropy_df = get_mean_cross_entropy_df(df, model, bs = 192)\n",
    "\n",
    "    return {\n",
    "        \"error_taxonomy\": error_taxonomy_df,\n",
    "#         \"dist_mean\": df_dist,\n",
    "        \"mean_cross_entropy\": mean_cross_entropy_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(control_df, treatment_df, model, err_path, cross_path):\n",
    "    control_metrics = _get_metrics(control_df, model)\n",
    "    treatment_metrics = _get_metrics(treatment_df, model)\n",
    "    \n",
    "    err_df = pd.concat(\n",
    "        [control_metrics[\"error_taxonomy\"], treatment_metrics[\"error_taxonomy\"]]\n",
    "    ).sort_index().reset_index(drop=True)\n",
    "    err_df[\"x_treatment\"] = [False, True] * len(control_metrics[\"error_taxonomy\"])\n",
    "    err_df.to_json(err_path, orient=\"records\", lines=True)\n",
    "\n",
    "    cross_df = pd.concat(\n",
    "        [control_metrics[\"mean_cross_entropy\"], treatment_metrics[\"mean_cross_entropy\"]]\n",
    "    ).sort_index().reset_index(drop=True)\n",
    "    cross_df[\"x_treatment\"] = [False, True] * len(control_metrics[\"mean_cross_entropy\"])\n",
    "    cross_df.to_json(cross_path, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def _long_range(bigclone_path, bugfix_path, cmt_path, model, out_path, n=None):\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(1, TYPES + 1):\n",
    "        df = pd.read_json(bigclone_path / f\"bigclone-type-{i}.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "        control_df = df[\"function_1\"].to_frame().rename(columns={\"function_1\": \"code\"})\n",
    "        treatment_df = df[\"function_2\"].to_frame().rename(columns={\"function_2\": \"code\"})\n",
    "        \n",
    "        err_path = out_path / f\"bigclone_type_{i}_error_taxonomy.jsonl\"\n",
    "        cross_path = out_path / f\"bigclone_type_{i}_cross_entropy.jsonl\"\n",
    "        save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "    \n",
    "#     control_df = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "#     treatment_df = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "\n",
    "#     err_path = out_path / f\"bug_fix_error_taxonomy.jsonl\"\n",
    "#     cross_path = out_path / f\"bug_fix_cross_entropy.jsonl\"\n",
    "#     save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "    \n",
    "#     control_df = pd.read_json(cmt_path / \"uncommented_code.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "#     treatment_df = pd.read_json(cmt_path / \"commented_code.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    \n",
    "#     err_path = out_path / f\"commenting_error_taxonomy.jsonl\"\n",
    "#     cross_path = out_path / f\"commenting_cross_entropy.jsonl\"\n",
    "#     save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "\n",
    "\n",
    "def evaluate(data_path, model_path, experiment_path):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    testbed_path = data_path / \"testbed\"\n",
    "    #     models = []\n",
    "    # These model folders will need to contain the config of the model as well\n",
    "    # to differentiate them\n",
    "    for m_path in model_path.glob(\"*/\"):\n",
    "        model = None\n",
    "        if \"dvc\" in m_path.name:\n",
    "            continue\n",
    "        elif \".gitignore\" in m_path.name:\n",
    "            continue\n",
    "        elif \"deprecated\" in m_path.name:\n",
    "            continue\n",
    "        \n",
    "        print(m_path)\n",
    "        if \"tfr\" in m_path.name:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "            model = TransformerHFModel.from_path(m_path, TFGPT2LMHeadModel, optimizer, _loss)\n",
    "        else:\n",
    "            model = RNNModel.from_path(m_path)\n",
    "        \n",
    "#         return model\n",
    "#         if m_path.name == \"Transformer\":\n",
    "#             model = TransformerModel.from_path(m_path)\n",
    "#         elif \"rnn\" in m_path.name:\n",
    "#             model = RNNModel.from_path(m_path)\n",
    "#         elif m_path.name == \"RNN\":\n",
    "#             pass\n",
    "#         return model\n",
    "    \n",
    "        bigclone_path = testbed_path / \"ts-bigclone-types\"\n",
    "        bugfix_path = testbed_path / \"ts-bug-fix\"\n",
    "        cmt_path = testbed_path / \"ts-comments\"\n",
    "\n",
    "        # Long-Range Interactions\n",
    "        _long_range(\n",
    "            bigclone_path, bugfix_path, cmt_path,\n",
    "            model, experiment_path / m_path.name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/dvc-icodegen/models/controlled/transformers/tfr_layers6_vocab10000_embd768_heads12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /home/jovyan/work/dvc-icodegen/models/controlled/transformers/tfr_layers6_vocab10000_embd768_heads12/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcb8915a19a43e59ed41a55a0354c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=6.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2ecb261d09a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dvc-icodegen/models/controlled/transformers/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexperiment_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dvc-icodegen/nbs/nbs_experiments/results/analyses/transformers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-e37fdbfeba8f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_path, model_path, experiment_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Long-Range Interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         _long_range(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mbigclone_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbugfix_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-e37fdbfeba8f>\u001b[0m in \u001b[0;36m_long_range\u001b[0;34m(bigclone_path, bugfix_path, cmt_path, model, out_path, n)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0merr_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"bigclone_type_{i}_error_taxonomy.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mcross_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"bigclone_type_{i}_cross_entropy.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     control_df = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[:n]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-e37fdbfeba8f>\u001b[0m in \u001b[0;36msave_results\u001b[0;34m(control_df, treatment_df, model, err_path, cross_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontrol_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtreatment_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreatment_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-e37fdbfeba8f>\u001b[0m in \u001b[0;36m_get_metrics\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     mean_probs = get_mean_probs(df, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0merror_taxonomy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_error_rates_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#     df_dist = mean_dist_probs(df, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmean_cross_entropy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_cross_entropy_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b8c5f9020fe5>\u001b[0m in \u001b[0;36mget_error_rates_df\u001b[0;34m(df, model, bs, n)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# loop through each token and its probability and update the container lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mERROR_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_taxonomy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0;32m-> 1023\u001b[0;31m                                                   stack(strides))\n\u001b[0m\u001b[1;32m   1024\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n\u001b[1;32m   1025\u001b[0m           \u001b[0mpacked_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;34m\"\"\"Implementation of constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = Path(\"/home/jovyan/work\")\n",
    "data_path = path / \"dvc-icodegen\"\n",
    "model_path = path / \"dvc-icodegen/models/controlled/transformers/\"\n",
    "experiment_path = path / \"dvc-icodegen/nbs/nbs_experiments/results/analyses/transformers\"\n",
    "model = evaluate(data_path, model_path, experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgp_t2lm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFGPT2MainLayer multiple                  50995200  \n",
      "=================================================================\n",
      "Total params: 50,995,200\n",
      "Trainable params: 50,995,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos><public> <void> <do>Command<(>String strCommand, ScreenField sourceSField, <int>',\n",
       " '<sos><void> writeInt<(><int> bits<)> <throws> IOException <{><n>        <if> <(>bitmap',\n",
       " '<sos><private> <void> createAndSend<(><n>    HashMap<lesser>String, Object<greater> data,<n>   ',\n",
       " '<sos><@>Override<n>    <public> <void> write<(><byte><[><]> data<)> <throws> IOException <{>',\n",
       " '<sos><static> <boolean> isUnsafe<(>Type type<)> <{><n>\\t\\t<return> type <instanceof> Un']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_fix_cross_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bigclone_type_2_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public static void copyFile (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; copyFile &lt;(&gt;File sour...</td>\n",
       "      <td>11.688641</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public static void fileCopy (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; fileCopy &lt;(&gt;File sour...</td>\n",
       "      <td>11.691650</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public static void fileCopy (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; fileCopy &lt;(&gt;File sour...</td>\n",
       "      <td>11.691650</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public static void copy (File source, File des...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; copy &lt;(&gt;File source, ...</td>\n",
       "      <td>11.724876</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public static void fileCopy (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; fileCopy &lt;(&gt;File sour...</td>\n",
       "      <td>11.691650</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>public static void copyFiles (File source, Fil...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; copyFiles &lt;(&gt;File sou...</td>\n",
       "      <td>11.693068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>public static void fileCopy (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; fileCopy &lt;(&gt;File sour...</td>\n",
       "      <td>11.691650</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>public static void copyFile (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; copyFile &lt;(&gt;File sour...</td>\n",
       "      <td>11.688641</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>public static void fileCopy (File source, File...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; fileCopy &lt;(&gt;File sour...</td>\n",
       "      <td>11.691650</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>public static void copy (File source, File des...</td>\n",
       "      <td>&lt;public&gt; &lt;static&gt; &lt;void&gt; copy &lt;(&gt;File source, ...</td>\n",
       "      <td>11.724876</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_code  \\\n",
       "0  public static void copyFile (File source, File...   \n",
       "1  public static void fileCopy (File source, File...   \n",
       "2  public static void fileCopy (File source, File...   \n",
       "3  public static void copy (File source, File des...   \n",
       "4  public static void fileCopy (File source, File...   \n",
       "5  public static void copyFiles (File source, Fil...   \n",
       "6  public static void fileCopy (File source, File...   \n",
       "7  public static void copyFile (File source, File...   \n",
       "8  public static void fileCopy (File source, File...   \n",
       "9  public static void copy (File source, File des...   \n",
       "\n",
       "                                    transformed_code  y_cross_entropy  \\\n",
       "0  <public> <static> <void> copyFile <(>File sour...        11.688641   \n",
       "1  <public> <static> <void> fileCopy <(>File sour...        11.691650   \n",
       "2  <public> <static> <void> fileCopy <(>File sour...        11.691650   \n",
       "3  <public> <static> <void> copy <(>File source, ...        11.724876   \n",
       "4  <public> <static> <void> fileCopy <(>File sour...        11.691650   \n",
       "5  <public> <static> <void> copyFiles <(>File sou...        11.693068   \n",
       "6  <public> <static> <void> fileCopy <(>File sour...        11.691650   \n",
       "7  <public> <static> <void> copyFile <(>File sour...        11.688641   \n",
       "8  <public> <static> <void> fileCopy <(>File sour...        11.691650   \n",
       "9  <public> <static> <void> copy <(>File source, ...        11.724876   \n",
       "\n",
       "   x_treatment  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3         True  \n",
       "4        False  \n",
       "5         True  \n",
       "6        False  \n",
       "7         True  \n",
       "8        False  \n",
       "9         True  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_fix_cross_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_taxonomy_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bigclone_type_2_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "      <th>original_code</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>0.044290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.390000e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044786</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.150427</td>\n",
       "      <td>public void actionPerformed (ActionEvent e) {\\...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; actionPer&lt;for&gt;med &lt;(&gt;ActionEve...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0.043681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.390000e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044786</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.179864</td>\n",
       "      <td>public void actionPerformed (ActionEvent e) {\\...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; actionPer&lt;for&gt;med &lt;(&gt;ActionEve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>0.015717</td>\n",
       "      <td>5.667000e-07</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.390000e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.171809</td>\n",
       "      <td>public void mouseClicked (MouseEvent me) {\\n  ...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;MouseEvent me&lt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>0.025243</td>\n",
       "      <td>5.860000e-07</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.390000e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>public void mouseClicked (MouseEvent me) {\\n  ...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;MouseEvent me&lt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>0.018860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.814000e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.189038</td>\n",
       "      <td>@Override\\npublic void mouseClicked (MouseEven...</td>\n",
       "      <td>&lt;@&gt;Override&lt;n&gt;&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>0.018844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.814000e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.188913</td>\n",
       "      <td>@Override\\npublic void mouseClicked (MouseEven...</td>\n",
       "      <td>&lt;@&gt;Override&lt;n&gt;&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>5.428000e-07</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.402800e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.169759</td>\n",
       "      <td>public void actionPerformed (final ActionEvent...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; actionPer&lt;for&gt;med &lt;(&gt;&lt;final&gt; A...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>0.024963</td>\n",
       "      <td>6.071000e-07</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.412700e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.164255</td>\n",
       "      <td>public void actionPerformed (final ActionEvent...</td>\n",
       "      <td>&lt;public&gt; &lt;void&gt; actionPer&lt;for&gt;med &lt;(&gt;&lt;final&gt; A...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>0.023634</td>\n",
       "      <td>4.350000e-07</td>\n",
       "      <td>0.087063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.814000e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.138084</td>\n",
       "      <td>@Override\\npublic void mouseClicked (int id) {...</td>\n",
       "      <td>&lt;@&gt;Override&lt;n&gt;&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>0.023835</td>\n",
       "      <td>5.892000e-07</td>\n",
       "      <td>0.087062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.814000e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>@Override\\npublic void mouseClicked (int id) {...</td>\n",
       "      <td>&lt;@&gt;Override&lt;n&gt;&lt;public&gt; &lt;void&gt; mouseClicked &lt;(&gt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_blocks  y_exceptions     y_oop  y_tests  y_declarations  \\\n",
       "1322  0.044290           NaN  0.000008      NaN    1.390000e-07   \n",
       "1323  0.043681           NaN  0.000008      NaN    1.390000e-07   \n",
       "1324  0.015717  5.667000e-07  0.000019      NaN    1.390000e-07   \n",
       "1325  0.025243  5.860000e-07  0.000019      NaN    1.390000e-07   \n",
       "1326  0.018860           NaN  0.261180      NaN    4.814000e-07   \n",
       "1327  0.018844           NaN  0.261180      NaN    4.814000e-07   \n",
       "1328  0.035550  5.428000e-07  0.000033      NaN    2.402800e-06   \n",
       "1329  0.024963  6.071000e-07  0.000035      NaN    2.412700e-06   \n",
       "1330  0.023634  4.350000e-07  0.087063      NaN    4.814000e-07   \n",
       "1331  0.023835  5.892000e-07  0.087062      NaN    4.814000e-07   \n",
       "\n",
       "      y_conditionals  y_loops  y_operators  y_datatypes  y_extra_tokens  \\\n",
       "1322        0.000011      NaN     0.044786     0.000002        0.150427   \n",
       "1323        0.000011      NaN     0.044786     0.000002        0.179864   \n",
       "1324        0.000014      NaN     0.017078          NaN        0.171809   \n",
       "1325        0.000014      NaN     0.023643          NaN        0.167114   \n",
       "1326        0.000010      NaN     0.000472     0.000003        0.189038   \n",
       "1327        0.000010      NaN     0.000464     0.000003        0.188913   \n",
       "1328        0.000007      NaN     0.022849          NaN        0.169759   \n",
       "1329        0.000007      NaN     0.027689          NaN        0.164255   \n",
       "1330        0.000005      NaN     0.000949     0.000092        0.138084   \n",
       "1331        0.000005      NaN     0.000629     0.000092        0.134850   \n",
       "\n",
       "                                          original_code  \\\n",
       "1322  public void actionPerformed (ActionEvent e) {\\...   \n",
       "1323  public void actionPerformed (ActionEvent e) {\\...   \n",
       "1324  public void mouseClicked (MouseEvent me) {\\n  ...   \n",
       "1325  public void mouseClicked (MouseEvent me) {\\n  ...   \n",
       "1326  @Override\\npublic void mouseClicked (MouseEven...   \n",
       "1327  @Override\\npublic void mouseClicked (MouseEven...   \n",
       "1328  public void actionPerformed (final ActionEvent...   \n",
       "1329  public void actionPerformed (final ActionEvent...   \n",
       "1330  @Override\\npublic void mouseClicked (int id) {...   \n",
       "1331  @Override\\npublic void mouseClicked (int id) {...   \n",
       "\n",
       "                                       transformed_code  x_treatment  \n",
       "1322  <public> <void> actionPer<for>med <(>ActionEve...        False  \n",
       "1323  <public> <void> actionPer<for>med <(>ActionEve...         True  \n",
       "1324  <public> <void> mouseClicked <(>MouseEvent me<...        False  \n",
       "1325  <public> <void> mouseClicked <(>MouseEvent me<...         True  \n",
       "1326  <@>Override<n><public> <void> mouseClicked <(>...        False  \n",
       "1327  <@>Override<n><public> <void> mouseClicked <(>...         True  \n",
       "1328  <public> <void> actionPer<for>med <(><final> A...        False  \n",
       "1329  <public> <void> actionPer<for>med <(><final> A...         True  \n",
       "1330  <@>Override<n><public> <void> mouseClicked <(>...        False  \n",
       "1331  <@>Override<n><public> <void> mouseClicked <(>...         True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_taxonomy_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
