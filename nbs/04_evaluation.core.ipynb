{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp evaluation.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.model.core import Model, RNNModel\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at sshleifer/tiny-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# Setting up testing data\n",
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "from icodegen.model.core import RNNModel\n",
    "\n",
    "# Using tiny-gpt2 for just quick tests since it is... tiny :)\n",
    "trnsfr_tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "tokenizer = trnsfr_tokenizer.backend_tokenizer\n",
    "trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# trnsfr_model = TransformerModel(tokenizer, trnsfr)\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru_model = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        2.01237513e-05,\n",
    "        1.98944481e-05,\n",
    "        2.01449202e-05,\n",
    "        2.04353437e-05,\n",
    "        2.02043060e-05,\n",
    "        2.02826177e-05,\n",
    "        2.09888076e-05,\n",
    "        2.07051467e-05,\n",
    "        1.98100976e-05,\n",
    "        2.02152678e-05,\n",
    "        2.02035244e-05,\n",
    "        2.10283021e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, trnsfr_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        1.99270412e-05,\n",
    "        1.99168703e-05,\n",
    "        1.98815596e-05,\n",
    "        1.99057849e-05,\n",
    "        1.98800869e-05,\n",
    "        1.98893995e-05,\n",
    "        1.98797388e-05,\n",
    "        1.98960342e-05,\n",
    "        1.99086674e-05,\n",
    "        1.98605580e-05,\n",
    "        1.98807957e-05,\n",
    "        1.98842057e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, gru_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_parens(toks: List[str], opening: str, closing: str) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Get the indices for the opening and closing tokens.\n",
    "    From https://stackoverflow.com/a/29992065/5768407\n",
    "    by user Baltasarq (https://stackoverflow.com/users/266978/baltasarq).\n",
    "\n",
    "    :param toks: the tokenized version of a method\n",
    "    :param opening: the opening token that will be matched against the closing token\n",
    "    :param closing: the closing token that will be matched against the opening token\n",
    "    :returns: returns a dictionary with the opening token indices as the keys and the closing token indices as the values\n",
    "    \"\"\"\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        if tok == opening:\n",
    "            pstack.append(i)\n",
    "        elif tok == closing:\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "\n",
    "    return toret\n",
    "\n",
    "\n",
    "def _get_dist_probs(\n",
    "    mthd: str, model: Model, opening: str, closing: str\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Get the distances and mean probabilities between opening and closing tokens in a given method.\n",
    "\n",
    "    :param mthd: the method to get the ranges of the opening and closing tokens and their probabilities\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :returns: returns a dictionary with the distance between the opening and closing tokens as keys and their mean probabilities as values\n",
    "    \"\"\"\n",
    "    # WARNING: Careful when using different tokenizers since HF tokenizers lib have diff API then HF transformers lib tokenizers... You will need to update this when using custom model and tokenizer...\n",
    "\n",
    "    # get the distances for the opening and closing tokens\n",
    "    toks = model.tokenizer.encode(mthd).tokens\n",
    "    idxs = find_parens(toks, opening, closing)\n",
    "\n",
    "    # get the model probabilities for the given method\n",
    "    inputs = model.tokenize(mthd)\n",
    "    probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    # sum up the probabilities of the different distances for the closing token\n",
    "    dist_probs = defaultdict(float)\n",
    "    for open_id, close_id in idxs.items():\n",
    "        dist_probs[close_id - open_id] += probs[close_id][\n",
    "            inputs[\"input_ids\"][0][close_id]\n",
    "        ]\n",
    "\n",
    "    # get the mean of the summed probabilities\n",
    "    dist_cnts = Counter([close_id - open_id for open_id, close_id in idxs.items()])\n",
    "    dist_probs = {dist: dist_probs[dist] / n for dist, n in dist_cnts.items()}\n",
    "    return dist_probs\n",
    "\n",
    "\n",
    "def mean_dist_probs(\n",
    "    df: pd.DataFrame,\n",
    "    model: Model,\n",
    "    opening: Optional[str] = \"<{>\",\n",
    "    closing: Optional[str] = \"<}>\",\n",
    "    n: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the distance between opening and closing tokens and the mean probability of each closing token that the model should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a dataframe with the distances between opening and closing tokens and their mean probabilities\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # get the probabilities for the different distances for an entire dataframe\n",
    "    df = df.iloc[:n].copy()\n",
    "    dist_probs = df.code.apply(\n",
    "        lambda mthd: _get_dist_probs(mthd, model, opening, closing)\n",
    "    ).values\n",
    "\n",
    "    # flatten the keys of the different distances into a list\n",
    "    dist_keys = []\n",
    "    for probs in dist_probs:\n",
    "        dist_keys.extend(probs.keys())\n",
    "    # merge dictionaries across methods by taking the mean of probs with the same distance. Modified from https://stackoverflow.com/a/10461916/5768407,\n",
    "    # users georg https://stackoverflow.com/users/989121/georg and Rémy Hosseinkhan Boucher https://stackoverflow.com/users/12149730/r%c3%a9my-hosseinkhan-boucher\n",
    "    mean_dist_probs = {\n",
    "        k: np.nanmean(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    std_dist_probs = {\n",
    "        k: np.nanstd(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "\n",
    "    med_dist_probs = {\n",
    "        k: np.nanmedian(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    mad_dist_probs = {\n",
    "        k: stats.median_abs_deviation(\n",
    "            np.array([probs.get(k, np.nan) for probs in dist_probs]), nan_policy=\"omit\"\n",
    "        )\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    # TODO: convert to dictionary\n",
    "    df_dist = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"dist\": list(mean_dist_probs.keys()),\n",
    "                \"mean_prob\": list(mean_dist_probs.values()),\n",
    "                \"std_prob\": list(std_dist_probs.values()),\n",
    "                \"med_prob\": list(med_dist_probs.values()),\n",
    "                \"mad_prob\": list(mad_dist_probs.values()),\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"dist\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, gru_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, trnsfr_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "token_taxonomy = {\n",
    "  \"blocks\": {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "    \"<return>\": \"return\"\n",
    "  },\n",
    "  \"exceptions\": {\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\"\n",
    "  },\n",
    "  \"oop\": {\n",
    "    \"<class>\": \"class\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<super>\": \"super\"\n",
    "  },\n",
    "  \"tests\": {\n",
    "    \"<assert>\": \"assert\"\n",
    "  },\n",
    "  \"declarations\": {\n",
    "    \"<native>\": \"native\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<enum>\": \"enum\"\n",
    "  },\n",
    "  \"conditionals\": {\n",
    "    \"<else>\": \"else\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<default>\": \"default\"\n",
    "  },\n",
    "  \"loops\": {\n",
    "    \"<break>\": \"break\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<while>\": \"while\",\n",
    "    \"<continue>\": \"continue\"\n",
    "  },\n",
    "  \"operators\": {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\"\n",
    "  },\n",
    "  \"datatypes\": {\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<strictfp>\": \"strictfp\"\n",
    "  },\n",
    "  \"extra_tokens\": {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "    \"<n>\": \"\\n\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_wordy = [\"<n>\", \"<...>\", \"<@>\", *token_taxonomy[\"operators\"], *token_taxonomy[\"blocks\"]]\n",
    "non_wordy.remove(\"<return>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    err_cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            cnts[idx] += 1\n",
    "            if p[idx] < ERROR_THRESHOLD:\n",
    "                err_cnts[idx] += 1\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    cnts = np.array(cnts)\n",
    "    err_cnts = np.array(err_cnts)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(cnts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_errs = np.divide(err_cnts, cnts, out=nans, where=cnts != 0)\n",
    "    \n",
    "    error_taxonomy = token_taxonomy.copy()\n",
    "    \n",
    "    for cat, tokens in error_taxonomy.items():\n",
    "        errs = []\n",
    "        cnt_sum = 0\n",
    "        for token, keyword in tokens.items():\n",
    "            idx = model.tokenizer.token_to_id(token)\n",
    "            error_taxonomy[cat][token] = {\"error_rate\": mean_errs[idx], \"count\": cnts[idx]}\n",
    "            errs.append(mean_errs[idx])\n",
    "            cnt_sum += cnts[idx]\n",
    "\n",
    "        errs = np.array(errs)\n",
    "        error_taxonomy[cat][\"stats\"] = {\n",
    "            \"mean_error_rate\": np.nanmean(errs),\n",
    "            \"stdev_error_rate\": np.nanstd(errs),\n",
    "            \"median_error_rate\": np.nanmedian(errs),\n",
    "            \"mad_error_rate\": stats.median_abs_deviation(errs, nan_policy=\"omit\"),\n",
    "        }\n",
    "    \n",
    "    return error_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blocks': {'<{>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<}>': {'error_rate': 1.0, 'count': 27},\n",
       "  '<[>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<]>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<(>': {'error_rate': 1.0, 'count': 81},\n",
       "  '<)>': {'error_rate': 0.8888888888888888, 'count': 81},\n",
       "  '<;>': {'error_rate': 1.0, 'count': 53},\n",
       "  '<return>': {'error_rate': 1.0, 'count': 10},\n",
       "  'stats': {'mean_error_rate': 0.9861111111111112,\n",
       "   'stdev_error_rate': 0.03674654598700822,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'exceptions': {'<catch>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<try>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<finally>': {'error_rate': nan, 'count': 0},\n",
       "  '<throw>': {'error_rate': nan, 'count': 0},\n",
       "  '<throws>': {'error_rate': 1.0, 'count': 1},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'oop': {'<class>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<instanceof>': {'error_rate': nan, 'count': 0},\n",
       "  '<interface>': {'error_rate': nan, 'count': 0},\n",
       "  '<private>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<protected>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<public>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<abstract>': {'error_rate': nan, 'count': 0},\n",
       "  '<extends>': {'error_rate': nan, 'count': 0},\n",
       "  '<package>': {'error_rate': nan, 'count': 0},\n",
       "  '<this>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<implements>': {'error_rate': nan, 'count': 0},\n",
       "  '<import>': {'error_rate': nan, 'count': 0},\n",
       "  '<new>': {'error_rate': 1.0, 'count': 6},\n",
       "  '<super>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'tests': {'<assert>': {'error_rate': 1.0, 'count': 2},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'declarations': {'<native>': {'error_rate': nan, 'count': 0},\n",
       "  '<static>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<synchronized>': {'error_rate': nan, 'count': 0},\n",
       "  '<transient>': {'error_rate': nan, 'count': 0},\n",
       "  '<volatile>': {'error_rate': nan, 'count': 0},\n",
       "  '<void>': {'error_rate': 1.0, 'count': 7},\n",
       "  '<final>': {'error_rate': nan, 'count': 0},\n",
       "  '<enum>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'conditionals': {'<else>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<if>': {'error_rate': 1.0, 'count': 11},\n",
       "  '<switch>': {'error_rate': nan, 'count': 0},\n",
       "  '<case>': {'error_rate': nan, 'count': 0},\n",
       "  '<default>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'loops': {'<break>': {'error_rate': nan, 'count': 0},\n",
       "  '<do>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<for>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<while>': {'error_rate': nan, 'count': 0},\n",
       "  '<continue>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'operators': {'<=>': {'error_rate': 1.0, 'count': 18},\n",
       "  '<+>': {'error_rate': 1.0, 'count': 3},\n",
       "  '<->': {'error_rate': 1.0, 'count': 2},\n",
       "  '<*>': {'error_rate': 1.0, 'count': 2},\n",
       "  '</>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<%>': {'error_rate': nan, 'count': 0},\n",
       "  '<++>': {'error_rate': nan, 'count': 0},\n",
       "  '<-->': {'error_rate': nan, 'count': 0},\n",
       "  '<!>': {'error_rate': 1.0, 'count': 5},\n",
       "  '<==>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<!=>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<greater_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<lesser_equal>': {'error_rate': nan, 'count': 0},\n",
       "  '<&&>': {'error_rate': nan, 'count': 0},\n",
       "  '<||>': {'error_rate': nan, 'count': 0},\n",
       "  '<?>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<:>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<~>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_lesser>': {'error_rate': nan, 'count': 0},\n",
       "  '<double_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<triple_greater>': {'error_rate': nan, 'count': 0},\n",
       "  '<&>': {'error_rate': nan, 'count': 0},\n",
       "  '<^>': {'error_rate': nan, 'count': 0},\n",
       "  '<|>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'datatypes': {'<byte>': {'error_rate': nan, 'count': 0},\n",
       "  '<char>': {'error_rate': nan, 'count': 0},\n",
       "  '<float>': {'error_rate': nan, 'count': 0},\n",
       "  '<boolean>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<double>': {'error_rate': nan, 'count': 0},\n",
       "  '<int>': {'error_rate': 1.0, 'count': 15},\n",
       "  '<long>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<short>': {'error_rate': nan, 'count': 0},\n",
       "  '<strictfp>': {'error_rate': nan, 'count': 0},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}},\n",
       " 'extra_tokens': {'<@>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<...>': {'error_rate': nan, 'count': 0},\n",
       "  '<null>': {'error_rate': 1.0, 'count': 2},\n",
       "  '<true>': {'error_rate': 1.0, 'count': 4},\n",
       "  '<false>': {'error_rate': 1.0, 'count': 1},\n",
       "  '<n>': {'error_rate': 1.0, 'count': 98},\n",
       "  'stats': {'mean_error_rate': 1.0,\n",
       "   'stdev_error_rate': 0.0,\n",
       "   'median_error_rate': 1.0,\n",
       "   'mad_error_rate': 0.0}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :10\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/gru_layers1_vocab10000_embed256_units512\")\n",
    "err_tax = get_error_rates(df_buggy, model)\n",
    "err_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ERROR_THRESHOLD = 0.5\n",
    "\n",
    "def get_error_rates_df(df: pd.DataFrame, model: Model, bs: int = 16, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    rows = []\n",
    "    # loop through each method\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Error Rates\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        logits = model.model(inputs)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            row = {\"y_\" + k: [0] * model.tokenizer.get_vocab_size() for k in token_taxonomy.keys()}\n",
    "            # loop through each token and its probability and update the container lists\n",
    "            for j, (idx, p) in enumerate(zip(inputs[i], probs[i])):\n",
    "                if p[idx] < ERROR_THRESHOLD:\n",
    "                    tok = model.tokenizer.id_to_token(idx)\n",
    "                    for k in token_taxonomy:\n",
    "                        if tok in token_taxonomy[k]:\n",
    "                            # Check if token is wordy and could be part of variable or method\n",
    "                            if tok not in non_wordy:\n",
    "                                # Get the token version of the token behind the token under study\n",
    "                                # and check if the last character in the token contains a letter\n",
    "                                inp_tok_prev = model.tokenizer.id_to_token(inputs[i][j - 1])\n",
    "                                if re.search('[a-zA-Z]', inp_tok_prev[-1]):\n",
    "#                                     print(tok, \"is not a special token because it is preceeded by\", inp_tok_prev)\n",
    "#                                     print(model.tokenizer.decode(inputs[i], skip_special_tokens=False))\n",
    "                                    break\n",
    "                                # Check if there is a token infront of the token under study\n",
    "                                # if there is, get the token version of it\n",
    "                                # and check if the first character in the token contains a letter\n",
    "                                if j + 1 < len(inputs[i]):\n",
    "                                    inp_tok_next = model.tokenizer.id_to_token(inputs[i][j + 1])\n",
    "                                    if re.search('[a-zA-Z]', inp_tok_next[0]):\n",
    "#                                         print(tok, \"is not a special token because it is followed by\", inp_tok_next)\n",
    "#                                         print(model.tokenizer.decode(inputs[i], skip_special_tokens=False))\n",
    "                                        break\n",
    "                            row[\"y_\" + k][idx] += 1\n",
    "\n",
    "            for k in row:\n",
    "                row[k] = np.mean(row[k])\n",
    "\n",
    "            rows.append(row)\n",
    "        \n",
    "    error_df = pd.DataFrame(rows)\n",
    "    error_df[\"code\"] = df.code.values[:n]\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :1_000\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7a68301e58411fbc64d2d1b70c8f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=63.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y_blocks  y_exceptions        y_oop      y_tests  y_declarations  \\\n",
       "count  1000.000000   1000.000000  1000.000000  1000.000000     1000.000000   \n",
       "mean      0.003086      0.000058     0.000238     0.000011        0.000094   \n",
       "std       0.000785      0.000124     0.000190     0.000051        0.000102   \n",
       "min       0.000500      0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.002575      0.000000     0.000100     0.000000        0.000000   \n",
       "50%       0.003000      0.000000     0.000200     0.000000        0.000100   \n",
       "75%       0.003600      0.000100     0.000300     0.000000        0.000100   \n",
       "max       0.005400      0.001300     0.001500     0.000500        0.001300   \n",
       "\n",
       "       y_conditionals      y_loops  y_operators  y_datatypes  y_extra_tokens  \n",
       "count     1000.000000  1000.000000  1000.000000  1000.000000     1000.000000  \n",
       "mean         0.000136     0.000082     0.000446     0.000147        0.001065  \n",
       "std          0.000170     0.000156     0.000315     0.000200        0.000398  \n",
       "min          0.000000     0.000000     0.000000     0.000000        0.000100  \n",
       "25%          0.000000     0.000000     0.000200     0.000000        0.000800  \n",
       "50%          0.000100     0.000000     0.000400     0.000100        0.001000  \n",
       "75%          0.000200     0.000100     0.000600     0.000200        0.001300  \n",
       "max          0.001700     0.001500     0.002400     0.001300        0.002800  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax = get_error_rates_df(df_buggy, model)\n",
    "err_tax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91787bff0c5d4939bde7758fc1fba110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=63.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y_blocks  y_exceptions        y_oop      y_tests  y_declarations  \\\n",
       "count  1000.000000   1000.000000  1000.000000  1000.000000     1000.000000   \n",
       "mean      0.003085      0.000049     0.000223     0.000001        0.000091   \n",
       "std       0.000785      0.000101     0.000176     0.000014        0.000098   \n",
       "min       0.000500      0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.002575      0.000000     0.000100     0.000000        0.000000   \n",
       "50%       0.003000      0.000000     0.000200     0.000000        0.000100   \n",
       "75%       0.003600      0.000000     0.000300     0.000000        0.000100   \n",
       "max       0.005400      0.000700     0.001500     0.000200        0.001300   \n",
       "\n",
       "       y_conditionals      y_loops  y_operators  y_datatypes  y_extra_tokens  \n",
       "count     1000.000000  1000.000000  1000.000000  1000.000000     1000.000000  \n",
       "mean         0.000114     0.000033     0.000446     0.000090        0.001064  \n",
       "std          0.000140     0.000067     0.000315     0.000146        0.000398  \n",
       "min          0.000000     0.000000     0.000000     0.000000        0.000100  \n",
       "25%          0.000000     0.000000     0.000200     0.000000        0.000800  \n",
       "50%          0.000100     0.000000     0.000400     0.000000        0.001000  \n",
       "75%          0.000200     0.000025     0.000600     0.000100        0.001300  \n",
       "max          0.000800     0.000500     0.002400     0.000900        0.002800  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_tax = get_error_rates_df(df_buggy, model)\n",
    "err_tax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    # Need to change to sparse_categorical_crossentropy\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs[\"input_ids\"], probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.append(losses)\n",
    "\n",
    "    # flatten list of cross entropies and calculate the mean, median, std, and mad\n",
    "    cross_entropy_losses = np.concatenate(cross_entropy_losses)\n",
    "    return {\n",
    "        \"mean\": np.mean(cross_entropy_losses),\n",
    "        \"median\": np.median(cross_entropy_losses),\n",
    "        \"std\": np.std(cross_entropy_losses),\n",
    "        \"mad\": stats.median_abs_deviation(cross_entropy_losses),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = gru_model.tokenize(mthd)\n",
    "    probs = gru_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, gru_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = trnsfr_model.tokenize(mthd)\n",
    "    probs = trnsfr_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, trnsfr_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mean_cross_entropy_df(df: pd.DataFrame, model: Model, bs = 16, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Cross Entropies\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = tf.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        logits = model.model(inputs)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs, probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.extend(np.mean(losses, axis = 1))\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        zip(df.code.values[:n], cross_entropy_losses),\n",
    "        columns=[\"code\", \"y_cross_entropy\"]\n",
    "    )\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6ccb7c719842b09060b8fa22b74e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cross Entropies', max=7.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.036189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.529572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.113028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.934481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.905569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.866459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.877886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_cross_entropy\n",
       "count       100.000000\n",
       "mean          6.036189\n",
       "std           1.529572\n",
       "min           3.113028\n",
       "25%           4.934481\n",
       "50%           5.905569\n",
       "75%           6.866459\n",
       "max           9.877886"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = get_mean_cross_entropy_df(df_buggy, model)\n",
    "cross_entropy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [23] != values[1].shape = [13] [Op:Pack]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0d279113ccbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_cross_entropy_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-24d9cb9203e7>\u001b[0m in \u001b[0;36mget_mean_cross_entropy_df\u001b[0;34m(df, model, bs, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmthd\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmthd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# token the method and get the probabilities for each token from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1410\u001b[0m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6382\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6383\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6384\u001b[0;31m       return pack_eager_fallback(\n\u001b[0m\u001b[1;32m   6385\u001b[0m           values, axis=axis, name=name, ctx=_ctx)\n\u001b[1;32m   6386\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n\u001b[1;32m   6422\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6423\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6424\u001b[0;31m   _result = _execute.execute(b\"Pack\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m   6425\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   6426\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [23] != values[1].shape = [13] [Op:Pack]"
     ]
    }
   ],
   "source": [
    "cross_entropy = get_mean_cross_entropy_df(df_fake, gru_model)\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_TRANSFORMs = {\n",
    "#     \"randomized_tokens\": code_token_randomizer,\n",
    "#     \"randomized_lines\": line_randomizer,\n",
    "    \"comments_removed\": java_comment_remover,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_metrics(df, model):\n",
    "#     mean_probs = get_mean_probs(df, model)\n",
    "    error_taxonomy_df = get_error_rates_df(df, model, bs = 64)\n",
    "#     df_dist = mean_dist_probs(df, model)\n",
    "    mean_cross_entropy_df = get_mean_cross_entropy_df(df, model, bs = 64)\n",
    "\n",
    "    return {\n",
    "        \"error_taxonomy\": error_taxonomy_df,\n",
    "#         \"dist_mean\": df_dist,\n",
    "        \"mean_cross_entropy\": mean_cross_entropy_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def _long_range(bigclone_path, bugfix_path, codesearchnet_path, model, out_path, n=None):\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    long_range_results = {}\n",
    "\n",
    "    # TODO add bigclone data\n",
    "\n",
    "    df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "        :n\n",
    "    ]\n",
    "    buggy_metrics = _get_metrics(df_buggy, model)\n",
    "\n",
    "    df_fixed = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)[\n",
    "        :n\n",
    "    ]\n",
    "    fixed_metrics = _get_metrics(df_fixed, model)\n",
    "    \n",
    "    bug_fix_err_df = pd.concat(\n",
    "        [buggy_metrics[\"error_taxonomy\"], fixed_metrics[\"error_taxonomy\"]]\n",
    "    ).sort_index().reset_index(drop=True)\n",
    "    bug_fix_err_df[\"x_treatment\"] = [False, True] * len(buggy_metrics[\"error_taxonomy\"])\n",
    "    bug_fix_err_df.to_json(out_path / \"bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "    bug_fix_cross_df = pd.concat(\n",
    "        [buggy_metrics[\"mean_cross_entropy\"], fixed_metrics[\"mean_cross_entropy\"]]\n",
    "    ).sort_index().reset_index(drop=True)\n",
    "    bug_fix_cross_df[\"x_treatment\"] = [False, True] * len(buggy_metrics[\"mean_cross_entropy\"])\n",
    "    bug_fix_cross_df.to_json(out_path / \"bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     df_codesearchnet = pd.read_json(\n",
    "#         codesearchnet_path / \"codesearchnet_java\" / \"test.jsonl\",\n",
    "#         orient=\"records\",\n",
    "#         lines=True,\n",
    "#     )[:n]\n",
    "#     long_range_results[\"codesearchnet_original\"] = _get_metrics(df_codesearchnet, model)\n",
    "\n",
    "#     for transform in _TRANSFORMs:\n",
    "#         df_transformed = transform_df(df_codesearchnet, _TRANSFORMs[transform])\n",
    "#         long_range_results[\"codesearchnet_\" + transform] = _get_metrics(\n",
    "#             df_transformed, model\n",
    "#         )\n",
    "\n",
    "    return long_range_results\n",
    "\n",
    "\n",
    "def _counterfactual(control_results, treatment_results):\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(data_path, model_path, experiment_path):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    testbed_path = data_path / \"controlled/testbeds\"\n",
    "    #     models = []\n",
    "    # These model folders will need to contain the config of the model as well\n",
    "    # to differentiate them\n",
    "    for m_path in model_path.glob(\"*/\"):\n",
    "        model = None\n",
    "        print(m_path)\n",
    "        model = RNNModel.from_path(m_path)\n",
    "#         if m_path.name == \"Transformer\":\n",
    "#             model = TransformerModel.from_path(m_path)\n",
    "#         elif \"rnn\" in m_path.name:\n",
    "#             model = RNNModel.from_path(m_path)\n",
    "#         elif m_path.name == \"RNN\":\n",
    "#             pass\n",
    "#         return model\n",
    "    \n",
    "        bigclone_path = testbed_path / \"_ts_bigclone_types\"\n",
    "        bugfix_path = testbed_path / \"_ts_bug_fix\"\n",
    "        codesearchnet_path = testbed_path / \"codesearchnet\"\n",
    "\n",
    "        # Long-Range Interactions\n",
    "#         results[m_path.name][\"long_range\"] = \n",
    "        _long_range(\n",
    "            bigclone_path, bugfix_path, codesearchnet_path,\n",
    "            model, experiment_path / m_path.name#, n=1_000\n",
    "        )\n",
    "#     return dict(results)\n",
    "\n",
    "\n",
    "\n",
    "        # Long-Range Interactions\n",
    "#         results[m_path.name][\"long_range\"] = _long_range(\n",
    "#             bigclone_path, bugfix_path, codesearchnet_path, model\n",
    "#         )\n",
    "\n",
    "#     return results\n",
    "    # Counterfactuals\n",
    "\n",
    "\n",
    "#         results[m_path][\"counterfactual\"] = _counterfactual(data_dir, model)\n",
    "# _counterfactual(control_results, treatment_results)\n",
    "\n",
    "# Save results in json format\n",
    "# Long-Range Interactions\n",
    "#     long_range_results = _long_range(data_dir, models)\n",
    "#     long_range_results\n",
    "\n",
    "#     # Counterfactuals\n",
    "#     counterfactual_results = []\n",
    "#     counterfactual_results\n",
    "#     for transform in _TRANSFORMs:\n",
    "#         pass\n",
    "# _counterfactual(control_results, treatment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35522dd17df44f35baa1e8722fd88251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=1012.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d221b5bcec76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dvc-icodegen/models/controlled/rnns/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexperiment_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dvc-icodegen/nbs_experiments/results/analyses\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-5638d8c15f95>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_path, model_path, experiment_path)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Long-Range Interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#         results[m_path.name][\"long_range\"] =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         _long_range(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mbigclone_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbugfix_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodesearchnet_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;31m#, n=1_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5638d8c15f95>\u001b[0m in \u001b[0;36m_long_range\u001b[0;34m(bigclone_path, bugfix_path, codesearchnet_path, model, out_path, n)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     ]\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbuggy_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_buggy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     df_fixed = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)[\n",
      "\u001b[0;32m<ipython-input-12-5638d8c15f95>\u001b[0m in \u001b[0;36m_get_metrics\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     mean_probs = get_mean_probs(df, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0merror_taxonomy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_error_rates_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#     df_dist = mean_dist_probs(df, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmean_cross_entropy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_cross_entropy_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ff637382eaeb>\u001b[0m in \u001b[0;36mget_error_rates_df\u001b[0;34m(df, model, bs, n)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = Path(\"/home/jovyan/work\")\n",
    "data_path = path / \"dvc-icodegen/datasets\"\n",
    "model_path = path / \"dvc-icodegen/models/controlled/rnns/\"\n",
    "experiment_path = path / \"dvc-icodegen/nbs_experiments/results/analyses\"\n",
    "results = evaluate(data_path, model_path, experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_fix_cross_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;public&gt; org.ggp.base....</td>\n",
       "      <td>7.169868</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; free&lt;(&gt;&lt;)&gt; &lt;{&gt;&lt;n&gt;    definiti...</td>\n",
       "      <td>4.914442</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>&lt;private&gt; &lt;void&gt; free&lt;(&gt;&lt;)&gt; &lt;{&gt;&lt;n&gt;    definiti...</td>\n",
       "      <td>6.653746</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;public&gt; &lt;void&gt; onRece...</td>\n",
       "      <td>6.119993</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;public&gt; &lt;void&gt; onRece...</td>\n",
       "      <td>6.141350</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   code  y_cross_entropy  \\\n",
       "1995  <@>java.lang.Override<n><public> org.ggp.base....         7.169868   \n",
       "1996  <private> <void> free<(><)> <{><n>    definiti...         4.914442   \n",
       "1997  <private> <void> free<(><)> <{><n>    definiti...         6.653746   \n",
       "1998  <@>java.lang.Override<n><public> <void> onRece...         6.119993   \n",
       "1999  <@>java.lang.Override<n><public> <void> onRece...         6.141350   \n",
       "\n",
       "      x_treatment  \n",
       "1995         True  \n",
       "1996        False  \n",
       "1997         True  \n",
       "1998        False  \n",
       "1999         True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_fix_cross_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_taxonomy_df = pd.read_json(\n",
    "    experiment_path / \"rnn_layers1_vocab10000_embed256_units1024/bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blocks</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>oop</th>\n",
       "      <th>tests</th>\n",
       "      <th>declarations</th>\n",
       "      <th>conditionals</th>\n",
       "      <th>loops</th>\n",
       "      <th>operators</th>\n",
       "      <th>datatypes</th>\n",
       "      <th>extra_tokens</th>\n",
       "      <th>code</th>\n",
       "      <th>x_treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blocks  exceptions     oop   tests  declarations  conditionals   loops  \\\n",
       "0  0.0027         0.0  0.0002  0.0000        0.0001        0.0000  0.0001   \n",
       "1  0.0027         0.0  0.0002  0.0000        0.0001        0.0000  0.0001   \n",
       "2  0.0027         0.0  0.0001  0.0000        0.0001        0.0002  0.0000   \n",
       "3  0.0027         0.0  0.0001  0.0000        0.0001        0.0002  0.0000   \n",
       "4  0.0018         0.0  0.0002  0.0000        0.0001        0.0000  0.0000   \n",
       "5  0.0018         0.0  0.0002  0.0000        0.0001        0.0000  0.0000   \n",
       "6  0.0030         0.0  0.0001  0.0000        0.0000        0.0002  0.0002   \n",
       "7  0.0029         0.0  0.0001  0.0000        0.0000        0.0002  0.0002   \n",
       "8  0.0023         0.0  0.0003  0.0002        0.0001        0.0000  0.0001   \n",
       "9  0.0023         0.0  0.0003  0.0002        0.0001        0.0000  0.0001   \n",
       "\n",
       "   operators  datatypes  extra_tokens  \\\n",
       "0     0.0003     0.0000        0.0008   \n",
       "1     0.0003     0.0000        0.0008   \n",
       "2     0.0002     0.0000        0.0012   \n",
       "3     0.0002     0.0000        0.0012   \n",
       "4     0.0006     0.0005        0.0009   \n",
       "5     0.0006     0.0005        0.0009   \n",
       "6     0.0005     0.0001        0.0013   \n",
       "7     0.0005     0.0001        0.0012   \n",
       "8     0.0001     0.0000        0.0006   \n",
       "9     0.0001     0.0000        0.0006   \n",
       "\n",
       "                                                code  x_treatment  \n",
       "0  <private> <void> success<(>io.netty.channel.Ch...        False  \n",
       "1  <private> <void> success<(>io.netty.channel.Ch...         True  \n",
       "2  <private> <void> handleConnectRequest<(>com.as...        False  \n",
       "3  <private> <void> handleConnectRequest<(>com.as...         True  \n",
       "4  <@>java.lang.Override<n><protected> <void> onS...        False  \n",
       "5  <@>java.lang.Override<n><protected> <void> onS...         True  \n",
       "6  <public> <boolean> <do>esEdgesHaveWeight<(><)>...        False  \n",
       "7  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         True  \n",
       "8  <@>org.junit.Test<n><public> <void> configures...        False  \n",
       "9  <@>org.junit.Test<n><public> <void> configures...         True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_taxonomy_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
