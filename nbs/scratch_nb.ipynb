{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch NB for prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to extract probabilities from tf huggingface model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n\nAll the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 6, 50257), dtype=float32, numpy=\narray([[[5.58242900e-03, 5.08132810e-03, 1.32375411e-04, ...,\n         5.53036102e-07, 8.81839753e-07, 1.64394279e-03],\n        [1.74138957e-04, 2.43200702e-05, 3.34589981e-06, ...,\n         2.89798720e-07, 3.58230608e-07, 4.39066382e-04],\n        [1.71553285e-04, 5.55381994e-05, 1.98571342e-06, ...,\n         4.45664591e-06, 1.00630615e-07, 5.77111359e-06],\n        [3.80778462e-02, 1.58687786e-03, 1.70949825e-05, ...,\n         1.67098495e-08, 2.56327553e-06, 1.18909200e-04],\n        [1.45475147e-04, 1.00719599e-05, 9.82764732e-07, ...,\n         6.14619966e-10, 5.48141621e-08, 1.10303176e-06],\n        [8.44225213e-02, 4.18395316e-03, 2.10049279e-06, ...,\n         5.69983949e-10, 7.84554732e-09, 1.12393645e-04]]], dtype=float32)>"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "logits = outputs[0]\n",
    "probs = tf.nn.softmax(logits); probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\nOutput:\n----------------------------------------------------------------------------------------------------\nHello, my dog is cute.\n"
    }
   ],
   "source": [
    "greedy_output = model.generate(inputs['input_ids'], max_length=inputs['input_ids'].shape[1] + 1)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirms softmax is producing a valid probability distribution since the max probability token is the same selected by the greedy decoding strategy used to generate the next token (i.e., pick token with largest probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.math.argmax(probs[0][-1], output_type=tf.dtypes.int32) == greedy_output[0][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icodegen",
   "language": "python",
   "name": "icodegen"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
