{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI\n",
    "\n",
    "> Contains all the CLI functions that your library provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data and code taken from https://github.com/github/CodeSearchNet\n",
    "\n",
    "```\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 GitHub\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```\n",
    "\n",
    "```\n",
    "@article{husain2019codesearchnet,\n",
    "  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n",
    "  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n",
    "  journal={arXiv preprint arXiv:1909.09436},\n",
    "  year={2019}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import gdown\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from fastcore.script import call_parse, Param\n",
    "from icodegen.data.core import (\n",
    "    convert_df_to_tfds,\n",
    "    java_special_tokens,\n",
    "    remove_non_ascii,\n",
    "    replace_special_tokens,\n",
    "    train_tokenizer,\n",
    ")\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.evaluation.core import (\n",
    "    get_mean_probs,\n",
    "    mean_dist_probs,\n",
    "    get_mean_cross_entropy,\n",
    ")\n",
    "from icodegen.model.core import RNNModel, TransformerModel\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "seed = 115\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Standardize naming convention to use `path` instead of `dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "URLs = {\n",
    "    \"bigclonebenchmark_lg\": \"https://drive.google.com/uc?id=1-4LPiiKGR5Zmg-TLqZEkRbRIdg7UlJQb\",\n",
    "    \"bigclonebenchmark_sm\": \"https://drive.google.com/uc?id=1FCq0lSs4oqc3jpSoucsHlRqjmbVwdRQ9\",\n",
    "    \"bug_fix_pairs\": \"https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\",\n",
    "    \"codesearchnet_java\": \"https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def download_data(\n",
    "    out_dir: Param(\"The output directory to download and extract all files to.\", str)\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for downloading all the data to reproduce our study.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    # TODO: Make individual folders to place all these files in\n",
    "    # Download bigclonebenchmark_lg and bigclonebenchmark_sm\n",
    "    logging.info(\"Downloading BigCloneBenchmark datasets.\")\n",
    "    #     gdown.download(\n",
    "    #         URLs[\"bigclonebenchmark_lg\"], str(out_dir / \"bigclonebenchmark_lg.csv\")\n",
    "    #     )\n",
    "    #     gdown.download(\n",
    "    #         URLs[\"bigclonebenchmark_sm\"], str(out_dir / \"bigclonebenchmark_sm.csv\")\n",
    "    #     )\n",
    "\n",
    "    # Download Bug Fix Pairs\n",
    "    logging.info(\"Downloading and extracting Bug Fix Pairs dataset.\")\n",
    "    #     gdown.cached_download(\n",
    "    #         URLs[\"bug_fix_pairs\"],\n",
    "    #         str(out_dir / \"bug_fix_pairs.zip\"),\n",
    "    #         postprocess=gdown.extractall,\n",
    "    #     )\n",
    "    #     with zipfile.ZipFile(\n",
    "    #         str(out_dir / \"datasets\" / \"50-100\" / \"source_code.zip\"), \"r\"\n",
    "    #     ) as zip_ref:\n",
    "    #         zip_ref.extractall(out_dir)\n",
    "\n",
    "    # from https://stackoverflow.com/a/14260592/5768407 by users\n",
    "    # yoavram (https://stackoverflow.com/users/1063612/yoavram) and\n",
    "    # kamran kausar (https://stackoverflow.com/users/3486460/kamran-kausar)\n",
    "    logging.info(\"Downloading and extracting CodeSearchNet Challenge dataset.\")\n",
    "    r = requests.get(URLs[\"codesearchnet_java\"])\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(out_dir / \"codesearchnet_java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(\"/tmp\")\n",
    "\n",
    "assert Path(\"/tmp/bigclonebenchmark_lg.csv\").exists()\n",
    "assert Path(\"/tmp/bigclonebenchmark_sm.csv\").exists()\n",
    "\n",
    "assert Path(\"/tmp/bug_fix_pairs.zip\").exists()\n",
    "assert Path(\"/tmp/50-100/buggy\").exists()\n",
    "assert Path(\"/tmp/50-100/fixed\").exists()\n",
    "\n",
    "assert Path(\"/tmp/codesearchnet_java\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _process_bigclonebenchmark(path):\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/tmp/bigclonebenchmark_lg.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.clone_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _process_bug_fix(path):\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    buggy_paths = sorted((path / \"50-100\").glob(\"buggy/*.java\"))\n",
    "    fixed_paths = sorted((path / \"50-100\").glob(\"fixed/*.java\"))\n",
    "    bugs = []\n",
    "    fixes = []\n",
    "    for bug_p, fix_p in zip(buggy_paths, fixed_paths):\n",
    "        with open(bug_p, \"r\") as f:\n",
    "            bugs.append(f.read())\n",
    "\n",
    "        with open(fix_p, \"r\") as f:\n",
    "            fixes.append(f.read())\n",
    "\n",
    "    df_buggy = pd.DataFrame(bugs, columns=[\"code\"])\n",
    "    df_buggy = remove_non_ascii(df_buggy)\n",
    "    df_buggy = replace_special_tokens(df_buggy, java_special_tokens)\n",
    "\n",
    "    df_fixed = pd.DataFrame(fixes, columns=[\"code\"])\n",
    "    df_fixed = remove_non_ascii(df_fixed)\n",
    "    df_fixed = replace_special_tokens(df_fixed, java_special_tokens)\n",
    "\n",
    "    # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "    df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "    df_fixed.to_json(path / \"fixed.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_process_bug_fix(Path(\"/tmp\"))\n",
    "\n",
    "assert Path(\"/tmp/buggy.jsonl\").exists()\n",
    "assert Path(\"/tmp/fixed.jsonl\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUGGY_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# FIXED_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.info(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# df = pd.read_json(\"/tmp/bug_fix_pairs.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# assert BUGGY_MTHD == df.buggy.values[0] and FIXED_MTHD == df.fixed.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def _process_codesearchnet(path):\n",
    "    \"\"\"\n",
    "    Grabs the different data splits and converts them into dataframes.\n",
    "    Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path / \"java\" / \"final\" / \"jsonl\" / split).glob(\"**/*.gz\"))\n",
    "        df = _jsonl_list_to_dataframe(files, [\"code\"])\n",
    "        df = remove_non_ascii(df)\n",
    "        df = replace_special_tokens(df, java_special_tokens)\n",
    "        # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "        if split == \"train\":\n",
    "            # 10% selected to match the Big Code != Big Vocab paper.\n",
    "            df_trn, df_bpe = train_test_split(df, test_size=0.1)\n",
    "            df_trn.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)\n",
    "            df_bpe.to_json(path / \"bpe.jsonl\", orient=\"records\", lines=True)\n",
    "        else:\n",
    "            df.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def process_data(\n",
    "    down_dir: Param(\n",
    "        \"The directory where all the files were downloaded and extracted to.\", str\n",
    "    )\n",
    "):\n",
    "    \"\"\"Function for processing data related to the library.\"\"\"\n",
    "    down_dir = Path(down_dir)\n",
    "\n",
    "    # Process CodeSearchNet Challenge data\n",
    "    _process_codesearchnet(down_dir / \"codesearchnet_java\")\n",
    "\n",
    "    # Process Bug Fix Pairs data\n",
    "\n",
    "\n",
    "#     _process_bug_fix(down_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(\"/tmp\")\n",
    "\n",
    "assert Path(\"/tmp/codesearchnet_java/train.jsonl\").exists()\n",
    "assert Path(\"/tmp/codesearchnet_java/bpe.jsonl\").exists()\n",
    "assert Path(\"/tmp/codesearchnet_java/valid.jsonl\").exists()\n",
    "assert Path(\"/tmp/codesearchnet_java/test.jsonl\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Experiment 0.0.0\n",
    "VANILLA_CONFIG = {\n",
    "    \"rnn_type\": \"rnn\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.0.0\n",
    "GRU_CONFIG_1 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.0\n",
    "GRU_CONFIG_2 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 2,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.1\n",
    "GRU_CONFIG_3 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 3,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.0\n",
    "GRU_CONFIG_4 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 512,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.1\n",
    "GRU_CONFIG_5 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 2_048,\n",
    "}\n",
    "\n",
    "_RNN_CONFIGs = [\n",
    "    VANILLA_CONFIG,\n",
    "    GRU_CONFIG_1,\n",
    "    GRU_CONFIG_2,\n",
    "    GRU_CONFIG_3,\n",
    "    GRU_CONFIG_4,\n",
    "    GRU_CONFIG_5,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def train(\n",
    "    data_path: Param(\"The path to where the data to train the models is located\", str),\n",
    "    out_path: Param(\"The output path to save all model chkpts to.\", str),\n",
    "    epochs: Param(\"The number of epochs to train each model for.\", int) = 64,\n",
    "    max_length: Param(\n",
    "        \"The maximum number of tokens each method can be. Truncation and padding will occur if the method is too long or short, respectively.\",\n",
    "        int,\n",
    "    ) = 300,\n",
    "    batch_size: Param(\"The batch size to use for training each model.\", int) = 64,\n",
    "):\n",
    "    \"\"\"Function for training models related to the library.\"\"\"\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    data_path = Path(data_path)\n",
    "    out_path = Path(out_path)\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Load in the datasets\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    # Check if the path where the tokenizer is to be saved is not empty\n",
    "    # if it is not empty then just load the tokenizer there.\n",
    "    if (out_path / \"tokenizer.json\").exists():\n",
    "        logging.info(f\"Loading tokenizer from {str(out_path / 'tokenizer.json')}.\")\n",
    "        tokenizer = Tokenizer.from_file(str(out_path / \"tokenizer.json\"))\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}.\"\n",
    "        )\n",
    "        df_bpe = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    "        )[:1_000]\n",
    "        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "        tokenizer.save(str(out_path / \"tokenizer.json\"), pretty=True)\n",
    "        del df_bpe\n",
    "\n",
    "    # Tokenize the dataset and convert it to tfds.\n",
    "    tfds_trn_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_trn_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_trn_path.exists():\n",
    "        ds_trn = tf.data.experimental.load(\n",
    "            str(tfds_trn_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_trn = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:1_000]\n",
    "        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)\n",
    "        tfds_trn_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_trn, str(tfds_trn_path))\n",
    "        del df_trn\n",
    "\n",
    "    tfds_val_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_val_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_val_path.exists():\n",
    "        ds_val = tf.data.experimental.load(\n",
    "            str(tfds_val_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_val = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"valid.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:1_000]\n",
    "        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)\n",
    "        tfds_val_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_val, str(tfds_val_path))\n",
    "        del df_val\n",
    "\n",
    "    logging.info(\"Starting the training of all RNN based models.\")\n",
    "    # Train RNN based models\n",
    "    for config in _RNN_CONFIGs:\n",
    "        rnn_model = RNNModel(\n",
    "            config[\"rnn_type\"],\n",
    "            config[\"n_layers\"],\n",
    "            tokenizer.get_vocab_size(),\n",
    "            config[\"embedding_dim\"],\n",
    "            config[\"rnn_units\"],\n",
    "            batch_size,\n",
    "            str(out_path),\n",
    "            tokenizer,\n",
    "        )\n",
    "        rnn_model.train(ds_trn, ds_val, epochs)\n",
    "        rnn_model.save()\n",
    "\n",
    "    logging.info(\"Starting the training of all Transformer based models.\")\n",
    "    # Train Transformer models\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer from /home/jovyan/work/dvc-icodegen/models/tokenizer.json.\n",
      "INFO:root:Starting the training of all RNN based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((8, 99), (8, 99)), types: (tf.int32, tf.int32)>\n",
      "Epoch 1/5\n",
      "  2/125 [..............................] - ETA: 13s - loss: 8.6110WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0814s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0814s vs `on_train_batch_end` time: 0.1413s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 12s 98ms/step - loss: 7.0403 - val_loss: 5.0950\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 12s 94ms/step - loss: 5.7865 - val_loss: 5.0550\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 12s 95ms/step - loss: 5.0547 - val_loss: 4.5550\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 12s 95ms/step - loss: 4.6370 - val_loss: 4.3598\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 12s 94ms/step - loss: 4.4426 - val_loss: 4.2195\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "124/125 [============================>.] - ETA: 0s - loss: 5.2352WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 9s 73ms/step - loss: 5.2270 - val_loss: 3.9797\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 8s 67ms/step - loss: 3.8152 - val_loss: 3.5610\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 3.3082 - val_loss: 3.4110\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 2.9313 - val_loss: 3.3827\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 8s 66ms/step - loss: 2.5995 - val_loss: 3.3874\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/gru_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/gru_vocab10000_embed256_units1024/assets\n",
      "INFO:root:Starting the training of all Transformer based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 21s, sys: 11.1 s, total: 3min 32s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_RNN_CONFIGs = [VANILLA_CONFIG, GRU_CONFIG_1]\n",
    "train(\n",
    "    data_path=\"/home/jovyan/work/dvc-icodegen/data\",\n",
    "    out_path=\"/home/jovyan/work/dvc-icodegen/models\",\n",
    "    epochs=5,\n",
    "    max_length=100,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer from /home/jovyan/work/dvc-icodegen/models/tokenizer.json.\n",
      "INFO:root:Starting the training of all RNN based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_LoadDataset shapes: ((8, 99), (8, 99)), types: (tf.int32, tf.int32)>\n",
      "  2/125 [..............................] - ETA: 13s - loss: 8.6074WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0813s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0813s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 12s 94ms/step - loss: 5.5849 - val_loss: 4.6702\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/125 [..............................] - ETA: 6s - loss: 8.6070WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0427s vs `on_train_batch_end` time: 0.0660s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0427s vs `on_train_batch_end` time: 0.0660s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - ETA: 0s - loss: 5.2367WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0105s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0105s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 9s 71ms/step - loss: 5.2367 - val_loss: 3.9869\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/gru_vocab10000_embed256_units1024/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/dvc-icodegen/models/gru_vocab10000_embed256_units1024/assets\n",
      "INFO:root:Starting the training of all Transformer based models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 s, sys: 2.29 s, total: 50.4 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_RNN_CONFIGs = [VANILLA_CONFIG, GRU_CONFIG_1]\n",
    "train(\n",
    "    data_path=\"/home/jovyan/work/dvc-icodegen/data\",\n",
    "    out_path=\"/home/jovyan/work/dvc-icodegen/models\",\n",
    "    epochs=1,\n",
    "    max_length=100,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\n",
    "    str(\n",
    "        Path(\"/home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024\")\n",
    "        / \"tokenizer.json\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_json(\n",
    "    Path(\"/home/jovyan/work/dvc-icodegen/data\") / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")[:1_000]\n",
    "dataset = convert_df_to_tfds(df_trn, tokenizer, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1, 99), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(1, 99), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel.from_path(\n",
    "    \"/home/jovyan/work/dvc-icodegen/models/rnn_vocab10000_embed256_units1024\"\n",
    ")\n",
    "loss = model.model.evaluate(dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = 100\n",
    "text = model.generate(NUM_TOKENS, temperature=0.1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RNN_CONFIGs = [VANILLA_CONFIG, GRU_CONFIG_1]\n",
    "train(\n",
    "    data_path=\"/tmp\",\n",
    "    out_path=\"/tmp/models\",\n",
    "    epochs=10,\n",
    "    max_length=100,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\n",
    "    str(Path(\"/tmp/models/rnn_vocab10000_embed256_units1024\") / \"tokenizer.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_json(\n",
    "    Path(\"/tmp\") / \"codesearchnet_java\" / \"train.jsonl\", orient=\"records\", lines=True\n",
    ")[:100]\n",
    "dataset = convert_df_to_tfds(df_trn, tokenizer, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel.from_path(\"/tmp/models/rnn_vocab10000_embed256_units1024\")\n",
    "loss = model.model.evaluate(dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS = 100\n",
    "text = model.generate(NUM_TOKENS, temperature=0.1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_TRANSFORMs = {\n",
    "    \"randomized_tokens\": code_token_randomizer,\n",
    "    \"randomized_lines\": line_randomizer,\n",
    "    \"comments_removed\": java_comment_remover,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_metrics(df, model):\n",
    "    mean_probs = get_mean_probs(df, model)\n",
    "    df_dist = mean_dist_probs(df, model)\n",
    "    mean_cross_entropy = get_mean_cross_entropy(df, model)\n",
    "\n",
    "    return {\n",
    "        \"mean_probs\": mean_probs,\n",
    "        \"dist_mean\": df_dist,\n",
    "        \"mean_cross_entropy\": mean_cross_entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "def _long_range(data_dir, model, n=None):\n",
    "    long_range_results = {}\n",
    "\n",
    "    df_buggy = pd.read_json(data_dir / \"buggy.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    long_range_results[\"buggy\"] = _get_metrics(df_buggy, model)\n",
    "    del df_buggy\n",
    "\n",
    "    df_fixed = pd.read_json(data_dir / \"fixed.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    long_range_results[\"fixed\"] = _get_metrics(df_fixed, model)\n",
    "    del df_fixed\n",
    "\n",
    "    df_codesearchnet = pd.read_json(\n",
    "        data_dir / \"codesearchnet_java\" / \"test.jsonl\", orient=\"records\", lines=True\n",
    "    )[:n]\n",
    "    long_range_results[\"codesearchnet_original\"] = _get_metrics(df_codesearchnet, model)\n",
    "\n",
    "    for transform in _TRANSFORMs:\n",
    "        df_transformed = transform_df(df_codesearchnet, _TRANSFORMs[transform])\n",
    "        long_range_results[\"codesearchnet_\" + transform] = _get_metrics(\n",
    "            df_transformed, model\n",
    "        )\n",
    "        del df_transformed\n",
    "\n",
    "    return long_range_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# Setting up testing data\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "# Using tiny-gpt2 for just quick tests since it is... tiny :)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "model = TransformerModel(tokenizer, trnsfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "long_range_results = _long_range(Path(\"/tmp\"), model, n=100)\n",
    "# print(long_range_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(long_range_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_range_results[\"codesearchnet_original\"][\"mean_cross_entropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    long_range_results[\"codesearchnet_randomized_tokens\"][\"mean_cross_entropy\"],\n",
    "    long_range_results[\"codesearchnet_randomized_lines\"][\"mean_cross_entropy\"],\n",
    "    long_range_results[\"codesearchnet_comments_removed\"][\"mean_cross_entropy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _counterfactual(control_results, treatment_results):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def evaluate(\n",
    "    data_dir: Param(\"The message\", str),\n",
    "    model_dir: Param(\"The message\", str),\n",
    "    out_dir: Param(\"The message\", str),\n",
    "):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    model_dir = Path(model_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    results = defaultdict(dict)\n",
    "    #     models = []\n",
    "    # These model folders will need to contain the config of the model as well\n",
    "    # to differentiate them\n",
    "    for m_path in model_dir.glob(\"*/\"):\n",
    "        model = None\n",
    "        if m_path.name == \"Transformer\":\n",
    "            model = TransformerModel.from_path(m_path)\n",
    "        elif m_path.name == \"GRU\":\n",
    "            model = RNNModel.from_path(m_path)\n",
    "        elif m_path.name == \"RNN\":\n",
    "            pass\n",
    "\n",
    "        # Long-Range Interactions\n",
    "        results[m_path.name][\"long_range\"] = _long_range(data_dir, model)\n",
    "\n",
    "        # Counterfactuals\n",
    "\n",
    "\n",
    "#         results[m_path][\"counterfactual\"] = _counterfactual(data_dir, model)\n",
    "# _counterfactual(control_results, treatment_results)\n",
    "\n",
    "# Save results in json format\n",
    "# Long-Range Interactions\n",
    "#     long_range_results = _long_range(data_dir, models)\n",
    "#     long_range_results\n",
    "\n",
    "#     # Counterfactuals\n",
    "#     counterfactual_results = []\n",
    "#     counterfactual_results\n",
    "#     for transform in _TRANSFORMs:\n",
    "#         pass\n",
    "# _counterfactual(control_results, treatment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def reproduce(\n",
    "    out_dir: Param(\n",
    "        \"The output directory to download, extract, and save all files to.\", str\n",
    "    )\n",
    "):\n",
    "    \"\"\"Function for reproducing results related to the library.\"\"\"\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    download_data(out_dir)\n",
    "    process_data(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
