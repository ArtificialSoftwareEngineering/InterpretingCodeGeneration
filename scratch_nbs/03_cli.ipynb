{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI\n",
    "\n",
    "> Contains all the CLI functions that your library provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data and code taken from https://github.com/github/CodeSearchNet\n",
    "\n",
    "```\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 GitHub\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```\n",
    "\n",
    "```\n",
    "@article{husain2019codesearchnet,\n",
    "  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n",
    "  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n",
    "  journal={arXiv preprint arXiv:1909.09436},\n",
    "  year={2019}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import gdown\n",
    "import io\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from fastcore.script import call_parse, Param\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.evaluation.core import (\n",
    "    get_mean_probs,\n",
    "    mean_dist_probs,\n",
    "    get_mean_cross_entropy,\n",
    ")\n",
    "from icodegen.model.core import GRUModel, TransformerModel\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Standardize naming convention to use `path` instead of `dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "URLs = {\n",
    "    \"bigclonebenchmark_lg\": \"https://drive.google.com/uc?id=1-4LPiiKGR5Zmg-TLqZEkRbRIdg7UlJQb\",\n",
    "    \"bigclonebenchmark_sm\": \"https://drive.google.com/uc?id=1FCq0lSs4oqc3jpSoucsHlRqjmbVwdRQ9\",\n",
    "    \"bug_fix_pairs\": \"https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\",\n",
    "    \"codesearchnet_java\": \"https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def download_data(\n",
    "    out_dir: Param(\"The output directory to download and extract all files to.\", str)\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for downloading all the data to reproduce our study.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    # Download bigclonebenchmark_lg and bigclonebenchmark_sm\n",
    "    logging.info(\"Downloading BigCloneBenchmark datasets.\")\n",
    "    gdown.download(\n",
    "        URLs[\"bigclonebenchmark_lg\"], str(out_dir / \"bigclonebenchmark_lg.csv\")\n",
    "    )\n",
    "    gdown.download(\n",
    "        URLs[\"bigclonebenchmark_sm\"], str(out_dir / \"bigclonebenchmark_sm.csv\")\n",
    "    )\n",
    "\n",
    "    # Download Bug Fix Pairs\n",
    "    logging.info(\"Downloading and extracting Bug Fix Pairs dataset.\")\n",
    "    gdown.cached_download(\n",
    "        URLs[\"bug_fix_pairs\"],\n",
    "        str(out_dir / \"bug_fix_pairs.zip\"),\n",
    "        postprocess=gdown.extractall,\n",
    "    )\n",
    "    with zipfile.ZipFile(\n",
    "        str(out_dir / \"datasets\" / \"50-100\" / \"source_code.zip\"), \"r\"\n",
    "    ) as zip_ref:\n",
    "        zip_ref.extractall(out_dir)\n",
    "\n",
    "    # from https://stackoverflow.com/a/14260592/5768407 by users\n",
    "    # yoavram (https://stackoverflow.com/users/1063612/yoavram) and\n",
    "    # kamran kausar (https://stackoverflow.com/users/3486460/kamran-kausar)\n",
    "    logging.info(\"Downloading and extracting CodeSearchNet Challenge dataset.\")\n",
    "    r = requests.get(URLs[\"codesearchnet_java\"])\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(out_dir / \"codesearchnet_java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_data(\"/tmp\")\n",
    "\n",
    "# assert Path(\"/tmp/bigclonebenchmark_lg.csv\").exists()\n",
    "# assert Path(\"/tmp/bigclonebenchmark_sm.csv\").exists()\n",
    "\n",
    "# assert Path(\"/tmp/bug_fix_pairs.zip\").exists()\n",
    "# assert Path(\"/tmp/50-100/buggy\").exists()\n",
    "# assert Path(\"/tmp/50-100/fixed\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _process_bigclonebenchmark(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/tmp/bigclonebenchmark_lg.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.clone_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "def _process_bug_fix(path):\n",
    "    buggy_paths = sorted((path / \"50-100\").glob(\"buggy/*.java\"))\n",
    "    fixed_paths = sorted((path / \"50-100\").glob(\"fixed/*.java\"))\n",
    "    bugs = []\n",
    "    fixes = []\n",
    "    for bug_p, fix_p in zip(buggy_paths, fixed_paths):\n",
    "        with open(bug_p, \"r\") as f:\n",
    "            bugs.append(f.read())\n",
    "\n",
    "        with open(fix_p, \"r\") as f:\n",
    "            fixes.append(f.read())\n",
    "\n",
    "    df_buggy = pd.DataFrame(bugs, columns=[\"code\"])\n",
    "    df_fixed = pd.DataFrame(fixes, columns=[\"code\"])\n",
    "\n",
    "    # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "    df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "    df_fixed.to_json(path / \"fixed.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "# #     df = pd.DataFrame(zip(bugs, fixes), columns=[\"buggy\", \"fixed\"])\n",
    "# #     # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "# #     df.to_json(path / \"bug_fix_pairs.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _process_bug_fix(Path(\"/tmp\"))\n",
    "\n",
    "# assert Path(\"/tmp/buggy.jsonl\").exists()\n",
    "# assert Path(\"/tmp/fixed.jsonl\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUGGY_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# FIXED_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.info(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# df = pd.read_json(\"/tmp/bug_fix_pairs.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# assert BUGGY_MTHD == df.buggy.values[0] and FIXED_MTHD == df.fixed.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def _process_codesearchnet(path):\n",
    "    \"\"\"\n",
    "    Grabs the different data splits and converts them into dataframes.\n",
    "    Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path / \"java\" / \"final\" / \"jsonl\" / split).glob(\"**/*.gz\"))\n",
    "        df = _jsonl_list_to_dataframe(files, [\"code\"])\n",
    "        # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "        df.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def process_data(\n",
    "    down_dir: Param(\n",
    "        \"The directory where all the files were downloaded and extracted to.\", str\n",
    "    )\n",
    "):\n",
    "    \"\"\"Function for processing data related to the library.\"\"\"\n",
    "    down_dir = Path(down_dir)\n",
    "\n",
    "    # Process CodeSearchNet Challenge data\n",
    "    _process_codesearchnet(down_dir / \"codesearchnet_java\")\n",
    "\n",
    "    # Process Bug Fix Pairs data\n",
    "    _process_bug_fix(down_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(\"/home/jovyan/work/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def train(param1: Param(\"The message\", str)):\n",
    "    \"\"\"Function for training models related to the library.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "_TRANSFORMs = {\n",
    "    \"randomized_tokens\": code_token_randomizer,\n",
    "    \"randomized_lines\": line_randomizer,\n",
    "    \"comments_removed\": java_comment_remover,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "def _long_range(data_dir, models):\n",
    "    long_range_results = {}\n",
    "\n",
    "    df_buggy = pd.read_json(data_dir / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "    df_fixed = pd.read_json(data_dir / \"fixed.jsonl\", orient=\"records\", lines=True)\n",
    "    for model in models:\n",
    "        mean_probs_buggy = get_mean_probs(df_buggy, model)\n",
    "        mean_probs_fixed = get_mean_probs(df_fixed, model)\n",
    "        df_dist_buggy = mean_dist_probs(df_buggy, model)\n",
    "        df_dist_fixed = mean_dist_probs(df_fixed, model)\n",
    "        mean_cross_entropy_buggy = get_mean_cross_entropy(df_buggy, model)\n",
    "        mean_cross_entropy_fixed = get_mean_cross_entropy(df_fixed, model)\n",
    "        long_range_results[\"buggy\"] = [\n",
    "            model,\n",
    "            mean_probs_buggy,\n",
    "            df_dist_buggy,\n",
    "            mean_cross_entropy_buggy,\n",
    "        ]\n",
    "        long_range_results[\"fixed\"] = [\n",
    "            model,\n",
    "            mean_probs_fixed,\n",
    "            df_dist_fixed,\n",
    "            mean_cross_entropy_fixed,\n",
    "        ]\n",
    "\n",
    "    df_codesearchnet = pd.read_json(\n",
    "        data_dir / \"codesearchnet_java\" / \"test.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    for model in models:\n",
    "        mean_probs = get_mean_probs(df_codesearchnet, model)\n",
    "        df_dist = mean_dist_probs(df_codesearchnet, model)\n",
    "        mean_cross_entropy = get_mean_cross_entropy(df_codesearchnet, model)\n",
    "        long_range_results[\"codesearchnet_original\"] = [\n",
    "            model,\n",
    "            mean_probs,\n",
    "            df_dist,\n",
    "            mean_cross_entropy,\n",
    "        ]\n",
    "    for transform in _TRANSFORMs:\n",
    "        df_transformed = transform_df(df_codesearchnet, _TRANSFORMs[transform])\n",
    "        for model in models:\n",
    "            mean_probs = get_mean_probs(df_transformed, model)\n",
    "            df_dist = mean_dist_probs(df_transformed, model)\n",
    "            mean_cross_entropy = get_mean_cross_entropy(df_transformed, model)\n",
    "            long_range_results[\"codesearchnet_\" + transform] = [\n",
    "                model,\n",
    "                mean_probs,\n",
    "                df_dist,\n",
    "                mean_cross_entropy,\n",
    "            ]\n",
    "            # how are we gonna save these results?\n",
    "            # Jsonl with each mean_prob, dist, crossent, model config per line?\n",
    "\n",
    "    return long_range_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _counterfactual(control_results, treatment_results):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def evaluate(\n",
    "    data_dir: Param(\"The message\", str),\n",
    "    model_dir: Param(\"The message\", str),\n",
    "    out_dir: Param(\"The message\", str),\n",
    "):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    model_dir = Path(model_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    models = []\n",
    "    for m_path in model_dir.glob(\"*/\"):\n",
    "        if m_path.name == \"Transformer\":\n",
    "            models.append(TransformerModel.from_path(m_path))\n",
    "        elif m_path.name == \"GRU\":\n",
    "            models.append(GRUModel.from_path(m_path))\n",
    "        elif m_path.name == \"RNN\":\n",
    "            pass\n",
    "    # Long-Range Interactions\n",
    "    long_range_results = _long_range(data_dir, models)\n",
    "    long_range_results\n",
    "\n",
    "    # Counterfactuals\n",
    "    counterfactual_results = []\n",
    "    counterfactual_results\n",
    "    for transform in _TRANSFORMs:\n",
    "        pass\n",
    "        # _counterfactual(control_results, treatment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def reproduce(\n",
    "    out_dir: Param(\n",
    "        \"The output directory to download, extract, and save all files to.\", str\n",
    "    )\n",
    "):\n",
    "    \"\"\"Function for reproducing results related to the library.\"\"\"\n",
    "    download_data(out_dir)\n",
    "    process_data(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
